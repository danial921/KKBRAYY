{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "      <th>cat_8</th>\n",
       "      <th>cat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_17</th>\n",
       "      <th>cat_18</th>\n",
       "      <th>cat_19</th>\n",
       "      <th>cat_20</th>\n",
       "      <th>cat_21</th>\n",
       "      <th>cat_22</th>\n",
       "      <th>cat_23</th>\n",
       "      <th>cat_24</th>\n",
       "      <th>cat_25</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9323</td>\n",
       "      <td>7670</td>\n",
       "      <td>3810</td>\n",
       "      <td>2313</td>\n",
       "      <td>1690</td>\n",
       "      <td>1450</td>\n",
       "      <td>1390</td>\n",
       "      <td>1744</td>\n",
       "      <td>1818</td>\n",
       "      <td>2246</td>\n",
       "      <td>...</td>\n",
       "      <td>2088</td>\n",
       "      <td>1870</td>\n",
       "      <td>2251</td>\n",
       "      <td>2286</td>\n",
       "      <td>3249</td>\n",
       "      <td>4540</td>\n",
       "      <td>6821</td>\n",
       "      <td>12954</td>\n",
       "      <td>104300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8820</td>\n",
       "      <td>7629</td>\n",
       "      <td>3669</td>\n",
       "      <td>2388</td>\n",
       "      <td>1623</td>\n",
       "      <td>1531</td>\n",
       "      <td>1343</td>\n",
       "      <td>1646</td>\n",
       "      <td>1785</td>\n",
       "      <td>2211</td>\n",
       "      <td>...</td>\n",
       "      <td>1909</td>\n",
       "      <td>1810</td>\n",
       "      <td>2111</td>\n",
       "      <td>2218</td>\n",
       "      <td>3125</td>\n",
       "      <td>4585</td>\n",
       "      <td>6945</td>\n",
       "      <td>12943</td>\n",
       "      <td>105986</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13101</td>\n",
       "      <td>6905</td>\n",
       "      <td>5009</td>\n",
       "      <td>3260</td>\n",
       "      <td>2465</td>\n",
       "      <td>1954</td>\n",
       "      <td>1690</td>\n",
       "      <td>1602</td>\n",
       "      <td>1610</td>\n",
       "      <td>1676</td>\n",
       "      <td>...</td>\n",
       "      <td>1692</td>\n",
       "      <td>1725</td>\n",
       "      <td>2019</td>\n",
       "      <td>2310</td>\n",
       "      <td>3281</td>\n",
       "      <td>4908</td>\n",
       "      <td>6899</td>\n",
       "      <td>13322</td>\n",
       "      <td>111351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12866</td>\n",
       "      <td>6717</td>\n",
       "      <td>5054</td>\n",
       "      <td>3392</td>\n",
       "      <td>2559</td>\n",
       "      <td>1972</td>\n",
       "      <td>1826</td>\n",
       "      <td>1736</td>\n",
       "      <td>1708</td>\n",
       "      <td>1882</td>\n",
       "      <td>...</td>\n",
       "      <td>1769</td>\n",
       "      <td>1856</td>\n",
       "      <td>2091</td>\n",
       "      <td>2523</td>\n",
       "      <td>3399</td>\n",
       "      <td>4871</td>\n",
       "      <td>6825</td>\n",
       "      <td>13230</td>\n",
       "      <td>109851</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8157</td>\n",
       "      <td>6292</td>\n",
       "      <td>2910</td>\n",
       "      <td>1899</td>\n",
       "      <td>1423</td>\n",
       "      <td>1301</td>\n",
       "      <td>1084</td>\n",
       "      <td>1453</td>\n",
       "      <td>1767</td>\n",
       "      <td>2447</td>\n",
       "      <td>...</td>\n",
       "      <td>2172</td>\n",
       "      <td>1586</td>\n",
       "      <td>1908</td>\n",
       "      <td>2458</td>\n",
       "      <td>3705</td>\n",
       "      <td>5406</td>\n",
       "      <td>5382</td>\n",
       "      <td>13690</td>\n",
       "      <td>95304</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat_0  cat_1  cat_2  cat_3  cat_4  cat_5  cat_6  cat_7  cat_8  cat_9  ...  \\\n",
       "0   9323   7670   3810   2313   1690   1450   1390   1744   1818   2246  ...   \n",
       "1   8820   7629   3669   2388   1623   1531   1343   1646   1785   2211  ...   \n",
       "2  13101   6905   5009   3260   2465   1954   1690   1602   1610   1676  ...   \n",
       "3  12866   6717   5054   3392   2559   1972   1826   1736   1708   1882  ...   \n",
       "4   8157   6292   2910   1899   1423   1301   1084   1453   1767   2447  ...   \n",
       "\n",
       "   cat_17  cat_18  cat_19  cat_20  cat_21  cat_22  cat_23  cat_24  cat_25  \\\n",
       "0    2088    1870    2251    2286    3249    4540    6821   12954  104300   \n",
       "1    1909    1810    2111    2218    3125    4585    6945   12943  105986   \n",
       "2    1692    1725    2019    2310    3281    4908    6899   13322  111351   \n",
       "3    1769    1856    2091    2523    3399    4871    6825   13230  109851   \n",
       "4    2172    1586    1908    2458    3705    5406    5382   13690   95304   \n",
       "\n",
       "   category  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('Fitur_LBPuniform_Ikan.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    577\n",
       "5    564\n",
       "6    544\n",
       "0    500\n",
       "4    331\n",
       "3    252\n",
       "1    240\n",
       "2    240\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check missing value ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check null =  0\n"
     ]
    }
   ],
   "source": [
    "print(\"check null = \",  df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data visualization ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Pair-wise Scatter Plots\n",
    "# import seaborn as sns\n",
    "# cols = [\"cat_0\",\"cat_1\",\"cat_2\",\"cat_3\",\"cat_4\",\"cat_5\",\"cat_6\",\"cat_7\",\"cat_8\",\"cat_9\",\"cat_10\",\"cat_11\",\"cat_12\",\"cat_13\",\"cat_14\",\"cat_15\",\"cat_16\",\"cat_17\",\"cat_18\",\"cat_19\",\"cat_20\",\"cat_21\",\"cat_22\",\"cat_23\",\"cat_24\",\"cat_25\",\"category\"]\n",
    "# pp = sns.pairplot(df[cols], size=1.8, aspect=1.8,\n",
    "#                   plot_kws=dict(edgecolor=\"k\", linewidth=0.5),\n",
    "#                   diag_kind=\"kde\", diag_kws=dict(shade=True))\n",
    "\n",
    "# fig = pp.fig \n",
    "# fig.subplots_adjust(top=0.93, wspace=0.3)\n",
    "# t = fig.suptitle('Wine Attributes Pairwise Plots', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fixing outlier ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlwAAAPkCAYAAAB88uAcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABhoklEQVR4nO39f5hdVX33/z9PfgdMItQ7Rou/6m3fHR3RggKFBJKCpsEftLZUA7WoaYB8cwdsrfSGiQr9ZOSj1VQJGuo0SCiYm0sQW/EKxNaAmWhNpdg7+TZ9Y1C/1h9oFEgiIb/n+8feI4dhksCZ2XPOzDwf13Uuzl57nTNrccG6Xmfttdeu9fT0IEmSpOqMaXYDJEmSRjoDlyRJUsUMXJIkSRUzcEmSJFXMwCVJklQxA5ckSVLFDFySJEkVG9fsBmh0i4jXAwsy89Kj1JsMfAo4BagB3wQWZ+YTEfEKYBXwPOCXwJ9m5n9V23JJGpwxrK7Oe4A/yMy3VNhkNYkzXGq2VwEnPIN6HRQ/EE4sX5OBK8tztwI3ZOYrgQ8Bt0dErYK2SlJfAx7DIuL4iLgB+CRFGNMI5AyXBl35K+19wEHg58C7gb8ATgOmUAwofwb8APhrYFpEfDYz332Er/0a8P3MPFT+jQeAV0XErwO/BfwfgMxcGxErgd8G/r2C7kka4YZyDCvP/THwY+AvAWe3RihnuDSoIuI1wEeA38vME4F/Aj4DvBD4nXIWajXwvzPzv4EPAhuOMlCRmesy88Hyb7wEeC/weeBFwI97B7HSD3lmvzgl6SmaMIaRmTdk5l8De6vplVqBM1wabGcD95QDEZn5CeATERHAJRHxcmA2sKuRL4+Ik4E7gesz866IOB3o+0DQGsUvU0l6toZ0DBuUFmtYcIZLg+0AdQEoIiZHxGLgy2XRPwI30MA6hYh4B/AVil+WHy6LfwC8oM+arRdSzHJJ0rM11GOYRgkDlwbbeuCciHhBeXwJMBf4UmauBL4F/D4wtjx/ABh/tC+NiLcA1wFvzMzP9ZZn5g+BbcDby3pzgUPA5sHojKRRZ0jHMI0etZ6evldjpIGJiD8B3l8e/oRiUemnKC5hjwPWAX8IvBj4DWAtsDkz33aE70zgeOBHdcUbM3NxuS1EF8W2EHuAizPTBfOSGjLUY1hdnXcBf5SZbx60zqhlGLgkSZIq5qJ5tYRyQepthzmdmfn2oWyPJD0bjmE6Gme4JEmSKtayM1wRMRF4PcX1c2/xl0a+scALgH/LzGG9H5HjlzQqHXEMa9nARTFYbWh2IyQNuVlAd7MbMUCOX9Lo1e8Y1sqB6ycAt956KzNmzGh2WyRV7OGHH+bCCy+E8v/9qkXElcBbgQnAp4H7gJso9mDaQvFg4UMRsZBia4ADwLJyw93JwC3AdIoNMC/KzO11X+/4JY0yRxvDWjlwHQSYMWMGJ5zgU1qkUaTyS3ARMRs4HTgDOIbiGXbLgaWZeW/5IOHzIuIbwGXA64BJQHdEfAVYRLENwNXlZpZLgcv79sHxSxqV+h3D3PhU0mg0l2Jz3DuBLwF3ASdTzHJBsa/SOcApFHsl7c3MHRSb7J4IzATu7lNXkg6rlWe4JKkqzwNeArwZeBnFA4rHZGbvbdu7gGnAVGBH3ef6K+8tk6TDMnBJGo1+AfxXZu4DMiL2AC+qOz8FeAzYWb4/UnlvmSQdlpcUJY1G3cDvRUQtIl4IHAv8S7m2C2AexV2Gm4BZETEpIqYBbRQL6jcC5/apK0mH5QyXpFGnvNPwTIpANQZYDHwP6IqICcBW4PbMPBgR11EEqjFAR2buiYiVwOqI6Ab2ARc0pSOShg0Dl6RRKTOv6Kf4rH7qdVE8HL2+bDdwfkVNkzQCeUlRkiSpYgYuSZKkihm4JEmSKmbgkiRJqpiBS0Oqvb2dWq02KK/29vZmd0fSKDKY45dj2OjjXYoaUlu2bHlG9Wq1Gj09PUevKElDxPFLA+EMlyRJUsUMXJIkSRUzcEmSJFXMwCVJklQxA5ckSVLFDFySJEkVM3BJkiRVzMAlSZJUMQOXJElSxQxckiRJFTNwSZIkVczAJUmSVDEDlyRJUsUMXJIkSRUzcEmSJFVs3EA+HBHTgfuBNwDHAF8CvlOeXpmZt0XEQuAS4ACwLDPviojJwC3AdGAXcFFmbh9IWyRJklpVw4ErIsYDfwc8URadBCzPzI/X1ZkBXAa8DpgEdEfEV4BFwObMvDoi3gEsBS5vtC2SJEmtbCAzXB8DbgCuLI9PBiIizqOY5XovcAqwMTP3AnsjYhtwIjAT+Gj5ubXABwbQDkmSpJbW0BquiHgXsD0z76kr3gS8PzPPBL4LfAiYCuyoq7MLmNanvLdMkiRpRGp00fx7gDdExL3Aa4GbgbWZeX95/k7gt4GdwJS6z00BHutT3lsmSZI0IjV0SbGcxQKgDF2XAv8YEUsycxNwNsVi+k1AZ0RMAiYCbcAWYCNwbnl+HrBhAH2QJElqaQO6S7GPRcD1EbEPeBi4ODN3RsR1FIFqDNCRmXsiYiWwOiK6gX3ABYPYDkk6qoh4gCeXNnwPWIF3WkuqyIADV2bOrjs8vZ/zXUBXn7LdwPkD/duS1Ihy1v0p41dE/BneaS2pIoM5wyVJw8VrgGMiYh3FOHgV3mktqULuNC9pNNpNsbXNXIo1qLcC/453WkuqiIFL0mj0IHBLZvZk5oPAL4C7vdNaUlUMXJJGo/cAHweIiBdSzFh9MSJOKc/X32k9KyImRcQ0nn6nNXintaRnwDVckkajVcBN5Z3SPRQBbA/eaS2pIgYuSaNOZh4uJHmntaRKeElRkiSpYgYuSZKkihm4JEmSKmbgkiRJqpiBS5IkqWIGLkmSpIoZuCRJkipm4JIkSaqYgUuSJKliBi5JkqSKGbgkSZIqZuCSJEmqmIFLkiSpYgYuSZKkihm4JEmSKmbgkiRJqpiBS5IkqWIGLkmSpIoZuCRJkipm4JIkSaqYgUuSJKliBi5JkqSKGbgkSZIqNm4gH46I6cD9wBuAA8BNQA+wBVicmYciYiFwSXl+WWbeFRGTgVuA6cAu4KLM3D6QtkiSJLWqhme4ImI88HfAE2XRcmBpZs4CasB5ETEDuAw4A5gLXBsRE4FFwOay7s3A0sa7IEmS1NoGcknxY8ANwI/L45OB+8r3a4FzgFOAjZm5NzN3ANuAE4GZwN196kqSJI1IDV1SjIh3Adsz856IuLIsrmVmT/l+FzANmArsqPtof+W9ZZI0ZCLiAZ4ch74HdOKyCEkVaXSG6z3AGyLiXuC1FJcFp9ednwI8Buws3x+pvLdMkoZEREwCyMzZ5evduCxCUoUamuHKzDN735eh61LgbyJidmbeC8wD1gObgM5ycJsItFH8ctwInFuenwdsaLwLkvSsvQY4JiLWUYyDV/H0ZRFvBA5SLosA9kZE/bKIj9bV/cAQtl3SMDSguxT7eB/QFRETgK3A7Zl5MCKuowhUY4COzNwTESuB1RHRDewDLhjEdkjS0eymWIf698ArKEKTyyIkVWbAgSszZ9cdntXP+S6gq0/ZbuD8gf5tSWrQg8C2MmA9GBG/oJjh6uWyCEmDyo1PJY1G7wE+DhARL6SYsVoXEbPL871LHTYBsyJiUkRM4+nLIurrStJhDeYlRUkaLlYBN5XLGnooAtjPcVmEpIoYuCSNOpl5uJDksghJlfCSoiRJUsUMXJIkSRUzcEmSJFXMwCVJklQxA5ckSVLFDFySJEkVM3BJkiRVzMAlSZJUMQOXJElSxQxckiRJFTNwSZIkVczAJUmSVDEDlyRJUsUMXJIkSRUzcEmSJFXMwCVJklQxA5ckSVLFDFySJEkVM3BJkiRVzMAlSZJUMQOXJElSxQxckiRJFTNwSZIkVczAJUmSVDEDlyRJUsUMXJIkSRUb18iHImIs0AUEcBB4NzAN+BLwnbLaysy8LSIWApcAB4BlmXlXREwGbgGmA7uAizJz+4B6IkmS1KIaClzAWwAy84yImA0spwhbyzPz472VImIGcBnwOmAS0B0RXwEWAZsz8+qIeAewFLi84V5I0rMUEdOB+4E3AMfgD0ZJFWoocGXmFyPirvLwJcBPgZOBiIjzKAat9wKnABszcy+wNyK2AScCM4GPlp9fC3yg4R5I0rMUEeOBvwOeKItOwh+MkirU8BquzDwQEauBFcDtwCbg/Zl5JvBd4EPAVGBH3cd2UVx6rC/vLZOkofIx4Abgx+XxycCbIuJrEbEqIqZQ94MxM3cA9T8Y7y4/txY4Z2ibLmk4GtCi+cy8CPhNivVc6zLz/vLUncBvAzuBKXUfmQI81qe8t0zD3PHHH0+tVhuUFzBo33X88cc3+d+MWklEvAvYnpn31BX7g1FSpRoKXBHxzoi4sjzcDRwCvhARp5RlZ1OsjdgEzIqISRExDWgDtgAbgXPLuvOADQ22Xy3k0Ucfpaenp+Vejz76aLP/1ai1vAd4Q0TcC7wWuBlY6w/G0c0fjKpao4vmvwB8NiK+BoynWK/138D1EbEPeBi4ODN3RsR1FIFqDNCRmXsiYiWwOiK6gX3ABQPshyQ9I+UsFgBl6LoU+MeIWJKZm3jqD8bOiJgETOTpPxg34Q/GEaP3B2Or6Q1wGv4aXTT/OPDH/Zw6vZ+6XRSXHOvLdgPnN/K3JakCi/AHo6QKNTrDJUnDXmbOrjv0B6OkyrjTvCRJUsUMXJIkSRUzcEmSJFXMwCVJklQxA5ckSVLFDFySJEkVM3BJkiRVzMAlSZJUMQOXJElSxQxckiRJFTNwSZIkVczAJUmSVDEDlyRJUsUMXJIkSRUzcEmSJFXMwCVJklQxA5ckSVLFDFySJEkVM3BJkiRVzMAlSZJUMQOXJElSxQxckiRJFTNwSZIkVczAJUmSVDEDlyRJUsUMXJIkSRUb1+wGSFIzRMR04H7gDcAB4CagB9gCLM7MQxGxELikPL8sM++KiMnALcB0YBdwUWZub0IXJA0jDQWuiBgLdAEBHATeDdRwwJI0DETEeODvgCfKouXA0sy8NyJuAM6LiG8AlwGvAyYB3RHxFWARsDkzr46IdwBLgcuHvBOShpVGLym+BSAzzwA+SDFY9Q5YsyjC13kRMYNiwDoDmAtcGxETeXLAmgXcTDFgSdJQ+RhwA/Dj8vhk4L7y/VrgHOAUYGNm7s3MHcA24ERgJnB3n7qSdEQNBa7M/CJwcXn4EuCnOGBJGgYi4l3A9sy8p664lpk95ftdwDRgKrCjrk5/5b1lknREDa/hyswDEbEa+APgj4A3O2BJGgbeA/RExDnAaylm2afXnZ8CPAbsLN8fqby3TJKOaEB3KWbmRcBvUqznmlx3ygFLUkvKzDMz86zMnA18G/hTYG1EzC6rzAM2AJuAWRExKSKmAW0U61M3Auf2qStJR9RQ4IqId0bEleXhbuAQ8C0HLEnD1PuAa8qF8hOA2zPzYeA6ivHpq0BHZu4BVgKviohuiqUV1zSpzZKGkUYvKX4B+GxEfA0YD7wX2Ap0RcSE8v3tmXkwInoHrDGUA1ZErARWlwPWPuCCAfZDkp61cpar11n9nO+imMGvL9sNnF9tyySNNA0Frsx8HPjjfk45YEmSJPXhTvOSJEkVM3BJkiRVzEf7SJJGvc2LjoWrW2+Hos2Ljm12EzRIDFySpFHv1Ssfp6en5+gVh9irazV6Pt3sVmgweElRkiSpYgYuSZKkihm4JEmSKmbgkiRJqpiBS5IkqWIGLkmSpIq5LYQGjfvYSJLUPwOXBo372EiS1D8vKUqSJFXMwCVJklQxA5ckSVLFDFySJEkVM3BJkiRVzMAlSZJUMQOXJElSxQxckiRJFTNwSZIkVczAJUmSVDEDlyRJUsUMXJIkSRUzcEmSJFVsXLMbIElDLSLGAl1AAAeBdwPTgC8B3ymrrczM2yJiIXAJcABYlpl3RcRk4BZgOrALuCgztw9xNyQNIwYuSaPRWwAy84yImA0spwhbyzPz472VImIGcBnwOmAS0B0RXwEWAZsz8+qIeAewFLh8aLsgaTgxcEkadTLzixFxV3n4EuCnwMlARMR5FLNc7wVOATZm5l5gb0RsA04EZgIfLT+/FvjAEDZf0jDkGi5Jo1JmHoiI1cAK4HZgE/D+zDwT+C7wIWAqsKPuY7soLj3Wl/eWSdJhNTTDFRHjgRuBlwITgWXAD3H9g6RhJDMvioi/Ar4JnJ6ZPypP3UkRxL4GTKn7yBTgMWBnXXlvmYa5Wq3W7CY8zXHHHdfsJmiQNDrD9SfALzJzFjAPuB44iWL9w+zydVvd+oczgLnAtRExkSfXP8wCbqZY/yBJQyIi3hkRV5aHu4FDwBci4pSy7GzgfopZr1kRMSkipgFtwBZgI3BuWXcesGHIGq9K9PT0DNprML/vkUceafK/GQ2WRtdwfZ5iCr7XAVz/IGn4+ALw2Yj4GjCeYrz6b+D6iNgHPAxcnJk7I+I6ikA1BujIzD0RsRJYHRHdwD7ggmZ0QtLw0VDgysxfAkTEFIrgtZTi0uLfZ+b9EdFBsf7h27j+QVKLyczHgT/u59Tp/dTtothCor5sN3B+Na2TNBI1vGg+Il4ErAf+ITM/B9yZmfeXp+8EfpunrnMA1z9IkqRRqKHAFRHPB9YBf5WZN5bF97j+QbVareVeLjqVJDVbo2u4rgKOAz4QEb3rr/4C+ITrH0av3sWig6FWqw3q90mS1EyNruG6nP53VXb9gyRJUh9ufCpJklQxA5ckSVLFDFySJEkVM3BJkiRVzMAlSZJUMQOXJElSxQxckiRJFTNwSZIkVczAJUmSVDEDlyRJUsUMXJIkSRUzcEmSJFXMwCVJklQxA5ckSVLFDFySJEkVM3BJkiRVzMAlSZJUMQOXJElSxQxckiRJFTNwSZIkVczAJUmSVDEDlyRJUsUMXJIkSRUb1+wGSNJQi4ixQBcQwEHg3UANuAnoAbYAizPzUEQsBC4BDgDLMvOuiJgM3AJMB3YBF2Xm9iHviKRhwxkuSaPRWwAy8wzgg8Dy8rU0M2dRhK/zImIGcBlwBjAXuDYiJgKLgM1l3ZuBpUPfBUnDiYFL0qiTmV8ELi4PXwL8FDgZuK8sWwucA5wCbMzMvZm5A9gGnAjMBO7uU1eSDsvAJWlUyswDEbEaWAHcDtQys6c8vQuYBkwFdtR9rL/y3jJJOiwDl6RRKzMvAn6TYj3X5LpTU4DHgJ3l+yOV95ZJ0mEZuCSNOhHxzoi4sjzcDRwCvhURs8uyecAGYBMwKyImRcQ0oI1iQf1G4Nw+dSXpsBq6SzEixgM3Ai8FJgLLgP/EO3wkDQ9fAD4bEV8DxgPvBbYCXRExoXx/e2YejIjrKALVGKAjM/dExEpgdUR0A/uAC5rRCUnDR6PbQvwJ8IvMfGdE/BrwAPBtijt87o2IGyju8PkGxR0+rwMmAd0R8RWevMPn6oh4B8UdPpcPsC+S9Ixk5uPAH/dz6qx+6nZRXHKsL9sNnF9N6ySNRI1eUvw88IG64wN4h48kSVK/GprhysxfAkTEFIq7e5YCH/MOH0mSpKdreNF8RLwIWA/8Q2Z+jmLRaS/v8JEkSSo1FLgi4vnAOuCvMvPGsvgB7/CRJEl6ukYXzV8FHAd8ICJ613JdDlznHT6SJElP1egarsvp/65C7/CRJEnqw41PJUmSKmbgkiRJqpiBS5IkqWIGLkmSpIoZuCRJkipm4JIkSaqYgUuSJKliBi5JkqSKGbgkSZIqZuCSJEmqmIFLkiSpYgYuSZKkihm4JEmSKmbgkiRJqpiBS5IkqWIGLkmSpIoZuCRJkipm4JIkSaqYgUuSJKliBi5JkqSKjWt2AyRpKEXEeOBG4KXARGAZ8EPgS8B3ymorM/O2iFgIXAIcAJZl5l0RMRm4BZgO7AIuysztQ9sLScONgUvSaPMnwC8y850R8WvAA8BfA8sz8+O9lSJiBnAZ8DpgEtAdEV8BFgGbM/PqiHgHsBS4fKg7IWl4MXBJGm0+D9xed3wAOBmIiDiPYpbrvcApwMbM3AvsjYhtwInATOCj5WfXAh8YonZLGsZcwyVpVMnMX2bmroiYQhG8lgKbgPdn5pnAd4EPAVOBHXUf3QVM61PeWyZJR2TgkjTqRMSLgPXAP2Tm54A7M/P+8vSdwG8DO4EpdR+bAjzWp7y3TJKOyMAlaVSJiOcD64C/yswby+J7IuKU8v3ZwP0Us16zImJSREwD2oAtwEbg3LLuPGDDkDVe0rDlGi5Jo81VwHHAByKid/3VXwCfiIh9wMPAxZm5MyKuowhUY4COzNwTESuB1RHRDewDLhj6LkgabgxckkaVzLyc/u8qPL2ful1AV5+y3cD51bRO0kg1oMAVEacCH8nM2RFxEu5jI0mS9DQNB66IuAJ4J/B4WXQS7mMjSZL0NAOZ4XoIeBvwD+Wx+9hIkiT1o+G7FDPzDmB/XZH72EiSJPVjMLeFcB8bSZKkfgxm4HIfG0mSpH4M5rYQi4Dr3cdGkiTpqQYUuDLz+8Bp5ft/x31sJEmSnsZH+0iSJFXMwCVJklQxA5ckSVLFDFySJEkVM3BJkiRVzMAlSZJUMQOXJElSxQxckiRJFTNwSZIkVczAJUmSVDEDlyRJUsUMXBpS7e3t1Gq1o76Ao9Zpb29vcm8kjSaDOX45ho0+A3p4tfRsbdmypdlNkKSGOH5pIJzhkiRJqpiBS5IkqWIGLkmSpIoZuCRJkipm4JIkSaqYgUuSJKlibgshaVSJiPHAjcBLgYnAMuA/gZuAHmALsDgzD0XEQuAS4ACwLDPviojJwC3AdGAXcFFmbh/qfkgaXpzhkjTa/Anwi8ycBcwDrgeWA0vLshpwXkTMAC4DzgDmAtdGxERgEbC5rHszsLQJfZA0zBi4JI02nwc+UHd8ADgZuK88XgucA5wCbMzMvZm5A9gGnAjMBO7uU1eSjqiVLymOBXj44Yeb3Q5JQ6Du//WxVf6dzPwlQERMAW6nmKH6WGb2lFV2AdOAqcCOuo/2V95b1pfjlzTKHG0Ma+XA9QKACy+8sNntkDS0XgA8VOUfiIgXAXcCn87Mz0XER+tOTwEeA3aW749U3lvWl+OXNHr1O4a1cuD6N2AW8BPgYJPbIql6YykGqn+r8o9ExPOBdcD/ysx/KYsfiIjZmXkvxbqu9cAmoDMiJlEsrm+jWFC/ETi3PD8P2NDPn3H8kkafI45htZ6env7KJWlEiohPAm8H/quu+HLgOmACsBVYmJkHy7sUL6ZY7/rhzLwjIo4BVlMMrPuACzLTa4eSjsjAJUmSVDHvUpQkSaqYgUstJyJOjYh7m90OSXq2HL90OK28aF6jUERcAbwTeLzZbZGkZ8PxS0fiDJdazUPA25rdCElqgOOXDsvApZaSmXcA+5vdDkl6thy/dCQGLkmSpIoZuCRJkipm4JIkSaqYG59KkiRVzBkuSZKkihm4JEmSKmbgkiRJqpiBS5IkqWIGLkmSpIoZuCRJkipm4JIkSaqYgUuSJKliBi5JkqSKGbgkSZIqZuCSJEmqmIFLkiSpYgYuSZKkihm4JEmSKmbgkiRJqpiBS5IkqWIGLkmSpIoZuCRJkipm4JIkSaqYgUuSJKliBi5JkqSKGbgkSZIqZuCSJEmqmIFLkiSpYgYuSZKkihm4JEmSKmbgkiRJqpiBS5IkqWIGLkmSpIoZuCRJkipm4JIkSaqYgUuSJKli45rdAI1uEfF6YEFmXnqUepOBTwGnADXgm8DizHyi/I5PAMcCY4GPZOYtlTZckhi0MWwO8DfAeOAJ4LLM3FRtyzXUnOFSs70KOOEZ1Oug+IFwYvmaDFwZETXgDuBDmflaYB6wPCJeUU1zJekpBjqGTQBuAxZm5muAZcA/VNRWNZEzXBp0EfEe4H3AQeDnwLuBvwBOA6ZQ/Lr7M+AHwF8D0yLis5n57iN87deA72fmofJvPEAx0E0ErsnMfwbIzB9GxHaKAfA7FXRP0gg3lGNYZu6LiF/PzP3lD8jfAH5RUdfURLWenp5mt0EjSES8Bvhn4KTM/O+IeC/FrNNO4O2ZeSgi/jdwRma+JSLeBfxRZr75WfyNlwDfAC7OzLv6nLsYWApEZj4xKJ2SNGo0awyLiOcD/w48r/w7XxzEbqkFOMOlwXY2cE9m/jdAZn4C+EREBHBJRLwcmA3sauTLI+Jk4E7g+n7C1v8GLgd+z7AlqUFNGcMy86fAr0fEScC/RMR/ZuaDA+qJWopruDTYDgC/mjaNiMkRsRj4cln0j8ANFFPyz0pEvAP4CvC/M/PDdeUTI2INMB/4ncz8jwG0X9LoNqRjWERMi4g/6K2Tmf8O/Afw6oZ7oJZk4NJgWw+cExEvKI8vAeYCX8rMlcC3gN+nuJsQisFt/NG+NCLeAlwHvDEzP9fn9C3AVOD0zPz+QDsgaVQb6jHsIHBjRJxR1nsV8FsUdzFqBHENlwZdRPwJ8P7y8CcUi0o/RXEJexywDvhD4MUUC0TXApsz821H+M4Ejgd+VFe8kSJsfR14kOJ26l5/lZn3DEZ/JI0uQzmGZebiiDgL+BhFcNsLXJmZXx3UTqnpDFySJEkVc9G8WkK5IPW2w5zOzHz7ULZHkp4NxzAdjTNckiRJFWvZGa6ImAi8nuL6+cEmN0dS9cYCLwD+LTP3NrsxA+H4JY1KRxzDGgpc5UZv7yoPJwGvBWZSPM+uB9hC8YyoQxGxkOIujwPAssy8q3ym1C3AdIq9TC7KzO19/szrgQ2NtE/SsDYL6K7yD5SB6LMUC553Aospxq6bGJwxzPFLGr36HcMGfEkxIj5FsWfIm4HlmXlvRNwA3EOxk+5XgNdRBLPu8v1iYGpmXl3uS/I7mXl5n+99ObDt1ltvZcaMGQNqo6TW9/DDD3PhhRcC/M/MfKjKvxUR/ws4MTMvLtfeXEdxd9igjGGOX9Loc7QxbECXFCPidRTPglocER8C7itPrQXeSDGVvrGcWtsbEdsoHto5E/hoXd0P9PP1BwFmzJjBCSc8k+eCShohhuIS3Cspxh4yMyOijeJywGCNYY5f0ujV7xg20DVcVwHXlO9rmdk7XbYLmEaxGeWOuvr9lfeWSdJQ+Tbw5oj4InAq8OvAzxzDJFWl4Z3mI+K5wG9l5vqy6FDd6SnAYxRrI6Ycpby3TJKGyo0U49B64C3A/Tz1V6ljmKRBNZBH+5xJ8UT1Xg9ExOzy/TyKBaObgFkRMSkipgFtFItRNwLn9qkrSUPl9UB3Zs6meJDwd3EMk1ShgVxSDIpBqtf7gK6ImABsBW7PzIMRcR3FYDQG6MjMPRGxElgdEd3APuCCAbRDkp6t7wD/T0T8JcXs1ALgOTiGSapIw4ErM/+mz/GDwFn91OsCuvqU7QbOb/RvS9JAZObPgXP6OeUYJqkSA7mkKEmSpGfAwCVJklQxA5daypo1a2hvb2fs2LG0t7ezZs2aZjdJkqQBa9lnKWr0WbNmDR0dHaxatYqZM2fS3d3NggULAJg/f36TWydJUuOc4VLL6OzsZNWqVcyZM4fx48czZ84cVq1aRWdnZ7ObJknSgBi41DK2bt3KzJkzn1I2c+ZMtm7d2qQWSdKT2tvbqdVqg/Zqb29vdpc0hAxcahltbW10dz/1Aevd3d20tbU1qUWS9KQtW7bQ09Nz1BfwjOpt2bKlyT3SUDJwqWV0dHSwYMEC1q9fz/79+1m/fj0LFiygo6Oj2U2TJGlAXDSvltG7MH7JkiVs3bqVtrY2Ojs7XTAvSRr2DFxqKfPnzzdgSZJGHC8pSpIkVczApZbixqeSpJHIS4pqGW58KkkaqZzhUsvo7OzkggsuYMmSJUyaNIklS5ZwwQUXuPGpJGnYc4ZLLeM///M/+dnPfsaxxx5LT08Pjz/+OJ/5zGf4+c9/3uymSZI0IM5wqWWMHTuWgwcPcuONN7J3715uvPFGDh48yNixY5vdNEmSBsTApZZx4MABxo8f/5Sy8ePHc+DAgSa1SJKkwWHgUks59dRTmTdvHhMmTGDevHmceuqpzW6SJEkDZuBSyzj++OP58pe/zIc//GEef/xxPvzhD/PlL3+Z448/vtlNkyRpQAxcahnHHHMMz3nOc1ixYgVTpkxhxYoVPOc5z+GYY45pdtMkSRoQA5daxo9//GNWrFjBscceC8Cxxx7LihUr+PGPf9zklkmSNDBuC6GW0dbWxgknnMCWLVt+VbZ+/Xra2tqa2CpJkgbOGS61jI6ODhYsWMD69evZv38/69evZ8GCBXR0dDS7aZIkDYgzXGoZvY/vWbJkCVu3bqWtrY3Ozk4f6yNJGvac4VJL+frXv862bds4dOgQ27Zt4+tf/3qzmyRJ0oAZuNQylixZwg033PCUbSFuuOEGlixZ0uymSZI0IA1fUoyIK4G3AhOATwMPAF8CvlNWWZmZt0XEQuAS4ACwLDPviojJwC3AdGAXcFFmbm+8GxoJurq6+MhHPsJf/MVfAPzqn1dddRUrVqxoZtMkSRqQhma4ImI2cDpwBnAW8CLgJGB5Zs4uX7dFxAzgsrLeXODaiJgILAI2Z+Ys4GZg6YB7omFv7969HHfccbS3tzN27Fja29s57rjj2Lt3b7ObJknSgDQ6wzUX2AzcCUwF3g8sACIizqOY5XovcAqwMTP3AnsjYhtwIjAT+Gj5XWuBDzTaAY0c48aN4y//8i+5/fbbmTlzJt3d3fzRH/0R48Z5b4ckaXhrdA3X84DXAecDlwK3ApuA92fmmcB3gQ9RhLEddZ/bBUzrU95bplFu6tSpPProo8yfP58JEyYwf/58Hn30UaZOndrspkmSNCCNBq5fAPdk5r7MTGAP8OXMvL88fyfw28BOYErd56YAj/Up7y3TKPfoo48C8NOf/vQp/+wtlyRpuGo0cHUDvxcRtYh4IXAs8OWIOKU8fzZwP8Ws16yImBQR04A2YAuwETi3rDsP2NBoBzRy9PT00NPTw1vf+la2b9/OW9/61l+VSZI0nDW0OKa80/BMikA1BlgMbAeuj4h9wMPAxZm5MyKuowhUY4COzNwTESuB1RHRDewDLhiEvmgEGD9+PP/3//5fnv/85/PiF7+Y8ePHs3///mY3S5KkAWl4NXJmXtFP8en91OsCuvqU7aZY/yU9xeTJkwF+Nas1efJkA5cGXUSMB1YDLwUOAgsptq65CeihmIlfnJmH3NpG0mBw41O1lF27dvGjH/2Inp4efvSjH7Fr165mN0kj07nAuMw8HfhroBNYDiwtt6upAee5tY2kwWLgUkvpu17L9VuqyIPAuIgYQ3HX9H7gZOC+8vxa4BzqtrbJzB1A/dY2d/epK0mH5QZHahm1Wo1jjjmGxx9/HID9+/dz7LHHsnv37ia3TCPQLykuJ/4XxTY3bwbOzMzehN/fFjaHK3drG0lH5QyXWkZPTw9PPPEEz3/+8wF4/vOfzxNPPOEsl6rw5xRb2/wm8BqK9VwT6s73t4XN4crd2kbSURm41FJqtdpT9uGq1WpNbpFGqEd5cobqEWA88ED52DJ4crsat7aRNCi8pKiWcvDgwSMeS4Pkb4EbI2IDxczWVcC3gK6ImABsBW7PzINubSNpMBi4JI06mflL4I/7OXVWP3Xd2kbSgHlJUS2nfg2XJEkjgYFLLafvsxQlSRruDFySJEkVM3BJkiRVzMAlSZJUMQOXJElSxQxckiRJFTNwSZIkVczAJUmSVDEDlyRJUsUMXJIkSRUzcEmSJFXMwKUh1d7eTq1W6/d1JP3Vb29vH6JWS5I0MOOa3QCNLlu2bDnsuXHjxnHw4MGnlY8dO5YDBw5U2SxJkirlDJdaxqJFi6jVaowdOxYoglatVmPRokVNbpkkSQPjDJdaxooVKwDo6uri4MGDjBs3joULF/6qXJKk4crApZayYsUKVqxYQa1WY8+ePc1ujiRJg8JLipIkSRUzcEmSJFXMwCVJklSxhtdwRcSVwFuBCcCngfuAm4AeYAuwODMPRcRC4BLgALAsM++KiMnALcB0YBdwUWZuH0hHJEmSWlVDM1wRMRs4HTgDOAt4EbAcWJqZs4AacF5EzAAuK+vNBa6NiInAImBzWfdmYOkA+yFJktSyGr2kOBfYDNwJfAm4CziZYpYLYC1wDnAKsDEz92bmDmAbcCIwE7i7T11JkqQRqdFLis8DXgK8GXgZ8E/AmMzsKc/vAqYBU4EddZ/rr7y3TJIkaURqNHD9AvivzNwHZETsobis2GsK8Biws3x/pPLeMkmSpBGp0UuK3cDvRUQtIl4IHAv8S7m2C2AesAHYBMyKiEkRMQ1oo1hQvxE4t09dSZKkEamhGa7yTsMzKQLVGGAx8D2gKyImAFuB2zPzYERcRxGoxgAdmbknIlYCqyOiG9gHXDAIfZEkSWpJDW8LkZlX9FN8Vj/1uoCuPmW7gfMb/duSJEnDiRufSpIkVczAJUmSVDEDlyRJUsUMXJIkSRUzcEmSJFWs4bsUJWm4ioh3Ae8qDycBr6V45NgngB6K/QIXZ+ahiFgIXAIcAJaV2+JMBm4BplM8LeOizNw+hF2QNMw4wyVp1MnMmzJzdmbOBu4HLgM+CCzNzFlADTgvImaU586geIbstRExEVgEbC7r3gwsbUI3JA0jBi5Jo1ZEvA54VWZ+BjgZuK88tRY4BzgF2JiZezNzB7ANOJFiNuzuPnUl6bAMXJJGs6uAa8r3tczsKd/vAqYBU4EddfX7K+8tk6TDMnBJGpUi4rnAb2Xm+rLoUN3pKcBjwM7y/ZHKe8sk6bAMXJJGqzOBf647fiAiZpfv51E8A3YTMCsiJkXENKCNYkH9RuDcPnUl6bAMXJJGqwC+W3f8PuCaiPgGMAG4PTMfBq6jCFRfBToycw+wEnhVRHQDF/PkZUlJ6pfbQkgalTLzb/ocPwic1U+9LqCrT9lu4PxKGyhpRHGGS5IkqWIGLkmSpIoZuCRJkipm4JIkSaqYgUuSJKliBi5JkqSKGbgkSZIqZuCSJEmqmIFLkiSpYgYuSZKkihm4JEmSKmbgkiRJqpiBS5IkqWIGLkmSpIqNa/SDEfEAsKM8/B6wAvgS8J2ybGVm3hYRC4FLgAPAssy8KyImA7cA04FdwEWZub3RtkiSJLWyhgJXREwCyMzZdWV/BizPzI/Xlc0ALgNeB0wCuiPiK8AiYHNmXh0R7wCWApc32glJkgbi+OOP59FHHx2076vVaoPyPccddxyPPPLIoHyXmqvRGa7XAMdExLryO64CTgYiIs6jmOV6L3AKsDEz9wJ7I2IbcCIwE/ho+V1rgQ803ANJkgbo0Ucfpaenp9nNeJrBCm5qvkbXcO0GPgbMBS4FbgX+HXh/Zp4JfBf4EDCVJy87QnH5cFqf8t4ySZKkEanRwPUgcEtm9mTmg8AvgLsz8/7y/J3AbwM7gSl1n5sCPNanvLdMkiRpRGo0cL0H+DhARLyQYsbqixFxSnn+bOB+YBMwKyImRcQ0oA3YAmwEzi3rzgM2NNgOSZKkltfoGq5VwE0R0Q30UASwPcD1EbEPeBi4ODN3RsR1FIFqDNCRmXsiYiWwuvz8PuCCgXZEkiSpVTUUuDLzcCHp9H7qdgFdfcp2A+c38rclSZKGm4b34ZL68rZqSZL6Z+DSoPG2akmS+uejfSRJkipm4JIkSaqYgUuSJKliBi5JkqSKGbgkSZIqZuCSJEmqmIFLkiSpYgYuSZKkihm4JEmSKuZO85JGpYi4EngrMAH4NHAfcBPQA2wBFmfmoYhYCFwCHACWZeZdETEZuAWYDuwCLsrM7UPfC0nDhTNckkadiJgNnA6cAZwFvAhYDizNzFlADTgvImYAl5X15gLXRsREYBGwuax7M7B0yDshaVgxcEkajeYCm4E7gS8BdwEnU8xyAawFzgFOATZm5t7M3AFsA04EZgJ396krSYflJUVJo9HzgJcAbwZeBvwTMCYze5++vguYBkwFdtR9rr/y3jJJOiwDl6TR6BfAf2XmPiAjYg/FZcVeU4DHgJ3l+yOV95ZJ0mF5SVHSaNQN/F5E1CLihcCxwL+Ua7sA5gEbgE3ArIiYFBHTgDaKBfUbgXP71JWkw3KGS9KoU95peCZFoBoDLAa+B3RFxARgK3B7Zh6MiOsoAtUYoCMz90TESmB1RHQD+4ALmtIRScOGgUuDZvOiY+Hq1lvKsnnRsc1uglpQZl7RT/FZ/dTrArr6lO0Gzq+oaZJGIAOXBs2rVz5OT0/P0SsOsVfXavR8utmtkCSNZq7hkiRJqpiBS5IkqWIGLkmSpIoZuCRJkipm4JIkSaqYgUuSJKliDW8LEREP8OSzxL4HdAI3AT0UOzEvzsxDEbEQuAQ4ACwrNxycDNwCTKd4DtlFmbm94V5IkiS1sIZmuCJiEkBmzi5f7waWA0szcxZQA86LiBnAZcAZwFzg2oiYCCwCNpd1bwaWDrwrkiRJranRGa7XAMdExLryO64CTgbuK8+vBd4IHAQ2ZuZeYG9EbANOBGYCH62r+4EG2yFJktTyGg1cu4GPAX8PvIIiNNUys3eb8V3ANGAqT152PFx5b5kkSdKI1GjgehDYVgasByPiFxQzXL2mAI8BO8v3RyrvLZMkSRqRGr1L8T3AxwEi4oUUM1brImJ2eX4esAHYBMyKiEkRMQ1oo1hQvxE4t09dSZKkEanRGa5VwE0R0U1xV+J7gJ8DXRExAdgK3J6ZByPiOopANQboyMw9EbESWF1+fh9wwUA7IklSozYvOhaubr3VLZsXHdvsJmiQNBS4MvNwIemsfup2AV19ynYD5zfytyVJGmyvXvk4PT09R684xF5dq9Hz6Wa3QoOh4X24pP7UarVmN+FpjjvuuGY3QZI0yhm4NGgG89dhrVZryV+bkiQ1wkf7SJIkVczAJUmSVDEDlyRJUsUMXJIkSRUzcEmSJFXMwCVJklQxA5ckSVLFDFySJEkVM3BJkiRVzMAlSZJUMQOXJElSxQxckiRJFTNwSZIkVczAJUmSVDEDlyRJUsUMXJIkSRUb1+wGSFIzRMQDwI7y8HtAJ3AT0ANsARZn5qGIWAhcAhwAlmXmXRExGbgFmA7sAi7KzO1D3AVJw4gzXJJGnYiYBJCZs8vXu4HlwNLMnAXUgPMiYgZwGXAGMBe4NiImAouAzWXdm4GlzeiHpOHDGS5Jo9FrgGMiYh3FOHgVcDJwX3l+LfBG4CCwMTP3AnsjYhtwIjAT+Ghd3Q8MYdslDUMGLkmj0W7gY8DfA6+gCE21zOwpz+8CpgFTefKy4+HKe8sk6bAMXJJGoweBbWXAejAifkExw9VrCvAYsLN8f6Ty3jJJOizXcEkajd4DfBwgIl5IMWO1LiJml+fnARuATcCsiJgUEdOANooF9RuBc/vUlaTDcoZL0mi0CrgpIrop7kp8D/BzoCsiJgBbgdsz82BEXEcRqMYAHZm5JyJWAqvLz+8DLmhKLyQNGwYuSaNOZh4uJJ3VT90uoKtP2W7g/GpaJ2kk8pKiJElSxQY0wxUR04H7gTcAxwBfAr5Tnl6Zmbe5aaAkSRrtGg5cETEe+DvgibLoJGB5Zn68rk7vpoGvAyYB3RHxFZ7cNPDqiHgHxaaBlzfaFkmSpFY2kBmujwE3AFeWxycDERHnUcxyvRc4BTcNlCRJo1xDa7gi4l3A9sy8p654E/D+zDwT+C7wIdw0UJI0TNRqtZZ7HXfccc3+16JB0ugM13uAnog4B3gtxbPE3pqZD5fn7wRWAF/DTQMlSS2up6fn6JWeoVqtNqjfp5GhocBVzmIBEBH3ApcC/xgRSzJzE3A2xWL6TUBn+aDYiTx908BNuGmgJEka4QZzH65FwPURsQ94GLg4M3e6aaAkSRrtBhy4MnN23eHp/Zx300BJkjSqufGpJElSxQxckiRJFTNwSZIkVczAJUmSVDEDlyRJUsUMXJIkSRUzcEmSJFXMwCVJklQxA5ckSVLFDFySJEkVM3BJkiRVzMAlSZJUMQOXJElSxQxcailz585lzJjiP8sxY8Ywd+7cJrdIkqSBM3CpZcydO5d169bR09MDQE9PD+vWrTN0SZKGPQOXhlR7ezu1Wq3f17p16/r9zLp16/qt397ePsStlySpMeOa3QCNLlu2bDnsuVqtdthzvbNekiQNR85wSZIkVczApZYzfvx4arUa48ePb3ZTJEkaFF5SVMvZv3//U/4pSdJw5wyXJElSxQxckiRJFTNwSZIkVczAJUmSVDEXzavljB07lkOHDjFmzBgOHjzY7OZohIqI6cD9wBuAA8BNQA+wBVicmYciYiFwSXl+WWbeFRGTgVuA6cAu4KLM3N6ELkgaRpzhUss5ePAgPT09hi1VJiLGA38HPFEWLQeWZuYsoAacFxEzgMuAM4C5wLURMRFYBGwu694MLB3q9ksafgY0w+UvREnD1MeAG4Ary+OTgfvK92uBNwIHgY2ZuRfYGxHbgBOBmcBH6+p+YKgaLWn4aniGy1+IkoajiHgXsD0z76krrmVm7/OjdgHTgKnAjro6/ZX3lknSEQ3kkmLvL8Qfl8d9fyGeA5xC+QsxM3cA9b8Q7+5TV5KGwnuAN0TEvcBrKX70Ta87PwV4DNhZvj9SeW+ZJB1RQ4HLX4iqSltbGxMnTgRg4sSJtLW1NblFGmky88zMPCszZwPfBv4UWBsRs8sq84ANwCZgVkRMiohpQBvFcomNwLl96krSETU6w+UvRFUiM/nwhz/M448/zoc//GEys9lN0ujwPuCaiPgGMAG4PTMfBq6jCFRfBToycw+wEnhVRHQDFwPXNKnNkoaRhhbNZ+aZve/L0HUp8DcRMTsz76X41bee4hdiZ0RMAiby9F+Im/AXokrHH388jzzyCFdccQXve9/7frU9xPHHH9/spmmEKme5ep3Vz/kuoKtP2W7g/GpbJmmkGcxtIfyFqAG5/vrrGT9+/K+2gzh48CDjx4/n+uuvb3LLJEkamAFvfOovRA2Wr3/96xw8eJAZM2bws5/9jOnTp/Ozn/2Mr3/968yfP7/ZzZMkqWFufKqW0dXVxfz58/m1X/s1AH7t136N+fPn09XVdZRPSpLU2gxcahl79+5l48aNrFixgj179rBixQo2btzI3r17m900SZIGxMClllGr1Zg3bx5z5sxh/PjxzJkzh3nz5lGr1ZrdNEmSBsTApZbymc98huXLl7N7926WL1/OZz7zmWY3SZKkARvwonlpsLzyla/kFa94BVdddRXve9/7mDhxIm95y1v4zne+0+ymSZI0IM5wqWV0dHTwH//xH6xdu5Z9+/axdu1a/uM//oOOjo5mN02SpAFxhksto3frhyVLlrB161ba2tro7Ox0SwhJ0rBn4FJLmT9/vgFLkjTieElRkiSpYgYuSZKkihm4JEmSKmbgkiRJqpiBS5IkqWIGLkmSpIoZuCRJkipm4JIkSaqYgUuSJKliBi5JkqSKGbgkSZIqZuCSJEmqmIFLkiSpYgYuSZKkihm4JEmSKmbgkiRJqpiBS5IkqWIGLkmSpIoZuCRJkio2rpEPRcRYoAsI4CDwbmAa8CXgO2W1lZl5W0QsBC4BDgDLMvOuiJgM3AJMB3YBF2Xm9gH1RJIkqUU1FLiAtwBk5hkRMRtYThG2lmfmx3srRcQM4DLgdcAkoDsivgIsAjZn5tUR8Q5gKXB5w72QJElqYQ1dUszMLwIXl4cvAX4KnAy8KSK+FhGrImIKcAqwMTP3ZuYOYBtwIjATuLv8/FrgnMa7oJFkzZo1tLe3M3bsWNrb21mzZk2zmyRJ0oA1vIYrMw9ExGpgBXA7sAl4f2aeCXwX+BAwFdhR97FdFJce68t7yzTKrVmzho6ODlasWMGePXtYsWIFHR0dhi5J0rDX6CVFADLzooj4K+CbwOmZ+aPy1J0UQexrwJS6j0wBHgN21pX3lmmU6+zsZNWqVcyZMweAOXPmsGrVKpYsWcL8+fOb3DqNJIdZh1oDbgJ6gC3A4sw85DpUSYOhoRmuiHhnRFxZHu4GDgFfiIhTyrKzgfspZr1mRcSkiJgGtFEMZBuBc8u684ANDbZfI8jWrVuZOXPmU8pmzpzJ1q1bm9QijWC/WocKfJBiHepyYGlmzqIIX+fVrUM9A5gLXBsRE3lyHeos4GaKdaiSdFiNXlL8AvDbEfE14B7gvRQD0Cci4l6KwWlZZj4MXEcRqL4KdGTmHmAl8KqI6KZYC3bNQDqhkaGtrY3u7u6nlHV3d9PW1takFmmkOsI61PvKst61pa5DlTQoGrqkmJmPA3/cz6nT+6nbRTF1X1+2Gzi/kb+tkaujo4O3v/3tHHvssfzgBz/gxS9+MY8//jif/OQnm900jUB161D/APgj4M2Z2VOe7m+96eHKXYcq6ajc+FQtqaen5+iVpAHKzIuA36T4UTi57lR/600PV+46VElHZeBSy+js7OS2227je9/7HocOHeJ73/set912G52dnc1umkaYw6xD/Va5ryA8ubbUdaiSBsWA7lKUBpOL5jWEvgB8tlyHOp5iHepWoCsiJpTvb8/MgxHRuw51DOU61IhYCawu16HuAy5oRickDR8GLrWM3kXzvdtCgIvmVY0jrEM9q5+6rkOVNGBeUlTL6OjoYMGCBaxfv579+/ezfv16FixYQEdHR7ObJknSgDjDpZbRu7npkiVL2Lp1K21tbXR2drrpqSRp2DNwqaXMnz/fgCVJGnG8pChJklQxA5ckSVLFDFySJEkVM3BJkiRVzMAlSZJUMQOXJElSxQxckiRJFTNwSZIkVczApZayZs0a2tvbGTt2LO3t7axZs6bZTZIkacDcaV4tY82aNXR0dLBq1SpmzpxJd3c3CxYsAHD3eUnSsOYMl1pGZ2cnq1atYs6cOYwfP545c+awatUqOjs7m900SZIGxMCllrF161Zmzpz5lLKZM2eydevWJrVIkp7U3t5OrVY76gt4RvXa29ub3CMNJQOXWkZbWxvd3d1PKevu7qatra1JLZKkJ23ZsoWenp5Be23ZsqXZXdIQMnCpZXR0dLBgwQLWr1/P/v37Wb9+PQsWLKCjo6PZTZMkaUBcNK+W0bswfsmSJWzdupW2tjY6OztdMC9JGvYMXGop8+fPN2BJkkYcLylKkiRVzMAlSZJUMQOXJElSxQxckiRJFWto0XxEjAW6gAAOAu8GasBNQA+wBVicmYciYiFwCXAAWJaZd0XEZOAWYDqwC7goM7cPsC+SJEktqdEZrrcAZOYZwAeB5eVraWbOoghf50XEDOAy4AxgLnBtREwEFgGby7o3A0sH1AtJkqQW1lDgyswvAheXhy8BfgqcDNxXlq0FzgFOATZm5t7M3AFsA04EZgJ396krSZI0IjW8D1dmHoiI1cAfAH8EvDkze8rTu4BpwFRgR93H+ivvLetrLMDDDz/caBMlDSN1/6+PbWY7BonjlzTKHG0MG9DGp5l5UUT8FfBNYHLdqSnAY8DO8v2RynvL+noBwIUXXjiQJkoafl4APNTsRgyQ45c0evU7hjW6aP6dwAmZeS2wGzgEfCsiZmfmvcA8YD2wCeiMiEnARKCNYkH9RuDc8vw8YEM/f+bfgFnATygW5ksa2cZSDFT/1uyGDALHL2n0OeIYVuvp6emv/Igi4ljgs8AMYDzw/wJbKe5cnFC+X5iZB8u7FC+mWC/24cy8IyKOAVaXDdsHXJCZzr1LkqQRqaHAJUmSpGfOjU/VciLi1Ii4t9ntkKRny/FLhzOgRfPSYIuIK4B3Ao83uy2S9Gw4fulInOFSq3kIeFuzGyFJDXD80mEZuNRSMvMOYH+z2yFJz5bjl47EwCVJklQxA5ckSVLFDFySJEkVcx8uSZKkijnDJUmSVDEDlyRJUsUMXJIkSRUzcEmSJFXMwCVJklQxA5ckSVLFDFySJEkVM3BJkiRVzMAlSZJUMQOXJElSxQxckiRJFTNwSZIkVczAJUmSVDEDlyRJUsUMXJIkSRUzcEmSJFXMwCVJklQxA5ckSVLFDFySJEkVM3BJkiRVzMAlSZJUMQOXJElSxQxckiRJFTNwSZIkVczAJUmSVDEDlyRJUsUMXJIkSRUzcEmSJFXMwCVJklQxA5ckSVLFDFySJEkVM3BJkiRVbFyzG6DRLSJeDyzIzEuPUm8y8CngFKAGfBNYnJlP1NV5GXA/8MbM/FZ1rZakwmCMYRHxFmA18IO6j8zKzF0VNVtNYOBSs70KOOEZ1Oug+O/1RIrB6hbgSuCDABExqSybUE0zJalfgzGGnQ58LDM/XFUj1XwGLg26iHgP8D7gIPBz4N3AXwCnAVMoBps/o/g199fAtIj4bGa++whf+zXg+5l5qPwbD1AMdL0+BdxEMahJUsOaMIadDuyPiLcDO4GOzPzaoHdMTeUaLg2qiHgN8BHg9zLzROCfgM8ALwR+JzNfSTF1/r8z878pft1tOMpARWauy8wHy7/xEuC9wOfL4z8DxmdmVzW9kjRaNGMMA34B3AC8lmLW686IeCazZhpGnOHSYDsbuKcciMjMTwCfiIgALomIlwOzgYbWJkTEycCdwPWZeVdEnARcCpw5CG2XpCEdw8q/8ba6Kt0R8XXgDcBnG+2EWo8zXBpsB4Ce3oOImBwRi4Evl0X/SPFLrvZsvzgi3gF8heKXZe9ahz8FpgJfj4hvU/wKvTUi3tpwDySNZkM6hkXEcyPiqoio/74asL/B9qtFGbg02NYD50TEC8rjS4C5wJcycyXwLeD3gbHl+QPA+KN9aXkXz3UUdyB+rrc8M9+bmb+Zma/NzNcCPwYuzMx/GqT+SBpdhnQMo5gpWwy8raz32xR3Mt494J6opdR6enqOXkt6FiLiT4D3l4c/oVhU+imKS9jjgHXAHwIvBn4DWAts7jOt3vc7Ezge+FFd8cbMXNyn3veBP3JbCEmNGuoxLCJeB6ygWJB/APjzzFw/qJ1S0xm4JEmSKuaiebWEckHqbYc5nZn59qFsjyQ9G45hOhpnuCRJkirWsjNcETEReD3F9fODTW6OpOqNBV4A/Ftm7m12YwbC8UsalY44hrVs4KIYrDY0uxGShtwsoLvZjRggxy9p9Op3DGvlwPUTgFtvvZUZM2Y0uy2SKvbwww9z4YUXQvn//jDn+CWNMkcbw1o5cB0EmDFjBiec4BMOpFFkJFyCc/ySRq9+xzA3PpUkSaqYgUuSJKliBi5JkqSKGbgkSZIqZuCSJEmqmIFLkiSpYgYuSZKkihm41FLWrFlDe3s7Y8eOpb29nTVr1jS7SZIkDVgrb3yqUWbNmjV0dHSwatUqZs6cSXd3NwsWLABg/vz5TW6dJEmNc4ZLLaOzs5NVq1YxZ84cxo8fz5w5c1i1ahWdnZ3NbpokSQNi4FLL2Lp1KzNnznxK2cyZM9m6dWuTWiRJT2pvb6dWqw3aq729vdld0hAycKlltLW10d391Aesd3d309bW1qQWSdKTtmzZQk9Pz1FfwDOqt2XLlib3SEPJwKWW0dHRwYIFC1i/fj379+9n/fr1LFiwgI6OjmY3TZKkAXlGi+Yj4lTgI5k5OyJeC9wAHAAeBP4sMw9FxELgkrJ8WWbeFRGTgVuA6cAu4KLM3B4RpwGfLOuuy8xrBrtjGn56F8YvWbKErVu30tbWRmdnpwvmJUnD3lFnuCLiCuDvgUll0YeAv87MmcBE4E0RMQO4DDgDmAtcGxETgUXA5sycBdwMLC2/4wbgAmAmcGpEnDR4XdJwNn/+fLZs2cLBgwfZsmWLYUuSNCI8k0uKDwFvqzt+ADg+ImrAFGA/cAqwMTP3ZuYOYBtwIkWgurv83FrgnIiYCkzMzIcyswe4Bzh7UHojSZLUgo4auDLzDopQ1es7wHXAVuD5wL3AVGBHXZ1dwLQ+5fVlO/upK0mSNCI1smj+k8CszPwtisuEH6cIUFPq6kwBHutT3l9ZfbkkSdKI1EjgeoQnZ6h+DBwHbAJmRcSkiJgGtAFbgI3AuWXdecCGzNwJ7IuIl5eXJecCGwbQB0mSpJbWyKN9/gz4PxFxANgHLMzMhyPiOorgNAboyMw9EbESWB0R3WXdC8rvuBS4FRhLcZfiNwfaEUnq547qFcBBYC/wp5n508G4ozoiPgS8qSx/b2ZuGtKOShp2nlHgyszvA6eV77sp7kbsW6cL6OpTths4v5+6/9r7fZI0GMo7qt8JPF4WfRJYkpnfjohLgL+KiI9S3FH9Ooo7r7sj4is8eUf11RHxDoo7qi+nuKP6D4HvAl+uu6P6LOBU4EXAHcDrh6KPkoYvNz6VNFL0vaP6HZn57fL9OGAPg3NH9UyK2a6ezPwBMC4i/kfFfZM0zBm4JI0Ife+ozsyfAETE6cD/Av6Wwbmj+nDfIUmHZeCSNGJFxNspLgu+KTO3Mzh3VHuntaRnzcAlaUSKiD+hmNmanZnfLYsH447qjcDciBgTES8GxmTmz4euZ5KGo0buUpSklhYRYyk2aP4B8IWIALgvMz80GHdUR8QG4Bvldyweup5JGq4MXJJGjPo7qoHjD1NnwHdUZ+bVwNUDaqykUcVLipIkSRUzcGlItbe3U6vVBuXV3t7e7O5IkvSMeElRQ2rLli3PqF6tVqOnp6fi1kiSNDSc4ZIkSaqYgUuSJKliBi5JkqSKGbgkSZIqZuCSJEmqmIFLkiSpYgYuSZKkihm4JEmSKmbgkiRJqpiBS5IkqWIGLkmSpIoZuCRJkipm4JIkSaqYgUuSJKliBi5JkqSKGbgkSZIqZuCSJEmqmIFLkiSpYgYuSZKkihm4JEmSKmbgkiRJqpiBS5IkqWIGLkmSpIoZuCRJkio2rtkNkKTBFBGnAh/JzNnl8R8A52fmBeXxacAngQPAusy8piz/EPCmsvy9mbkpIp4HfA6YDPwYeHdm7o6ItwAfLOvemJldQ9lHScOPM1ySRoyIuAL4e2BSefxJ4FqeOtbdAFwAzAROjYiTIuIk4CzgVOAdwKfKuh8EPpeZs4AHgEsiYjzwt8Aby89cHBEzqu6bpOHNwCVpJHkIeFvd8deBRb0HETEVmJiZD2VmD3APcDZF+FqXmT2Z+QNgXET8j7L87vLja4FzgDZgW2Y+mpn7gG5gVsX9kjTMGbgkjRiZeQewv+74NqCnrspUYGfd8S5gWlm+4yjlR6srSYf1jNZw1a+JiIjpQBdwHDAW+NPMfCgiFgKXUKxpWJaZd0XEZOAWYDrFoHRRZm4/3BoKSarYTmBK3fEU4DFg32HKe+s/0U9Z37qSdFhHneHquyYC+Chwa2aeCSwFfqtcv3AZcAYwF7g2IiZSTOVvLtc/3FzWh37WUAxelySpf5m5E9gXES+PiBrFeLUB2AjMjYgxEfFiYExm/rwsP7f8+Lyy7lbgFRFxfERMAM4EvjHUfZE0vDyTS4p910ScAZwQEf8MXAjcC5wCbMzMvZm5A9gGnEg/6x+OsIZCkobCpcCtwCbggcz8ZmbeTxGmvgHcASwu6y4D3hERG4HfAa7PzP3AX1CMXd+guEvxR0PcB0nDzFEvKWbmHRHx0rqilwKPZuY5EfFB4K+AB3l26x/6rqH4jQbbL0lPkZnfB06rO76X4odh7/G/1p+vK78auLpP2U+B3+un7peALw1KgyWNCo0smv8F8E/l+y8Br+Pwaxrqy13/IEmSRqVGAlc3T65pOBP4/1JMzc+KiEkRMY3itukt9LP+4QhrKCRJkkakRgLX+4A/jYivU0y1fzgzHwauowhOXwU6MnMPsBJ4VUR0AxcDvXcjPm0NxcC6IUmS1Lqe0bYQ9WsiMvP/B7yhnzpdFNtF1JftBs7vp26/aygkSZJGIjc+lSRJqpiBS5IkqWIGLkmSpIoZuCRJkipm4JIkSaqYgUuSJKliBi5JkqSKGbgkSZIqZuCSJEmqmIFLkiSpYgYuSZKkij2jZylKkjSSHX/88Tz66KOD9n21Wm1Qvue4447jkUceGZTvUnMZuCRJo96jjz5KT09Ps5vxNIMV3NR8XlKUJEmqmIFLkiSpYgYuSZKkihm4JEmSKmbgkiRJqpiBS5IkqWIGLkmSpIoZuCRJkipm4JIkSaqYgUuSJKliPtpH0ogREacCH8nM2RHxP4GbgB5gC7A4Mw9FxELgEuAAsCwz74qIycAtwHRgF3BRZm6PiNOAT5Z112XmNeXf+RDwprL8vZm5aUg7KmnYcYZL0ogQEVcAfw9MKouWA0szcxZQA86LiBnAZcAZwFzg2oiYCCwCNpd1bwaWlt9xA3ABMBM4NSJOioiTgLOAU4F3AJ8aiv5JGt4MXJJGioeAt9UdnwzcV75fC5wDnAJszMy9mbkD2AacSBGo7q6vGxFTgYmZ+VBm9gD3AGeXdddlZk9m/gAYFxH/o+K+SRrmDFySRoTMvAPYX1dUK4MSFJcJpwFTgR11dforry/beZS69eWSdFiu4ZI0Uh2qez8FeIwiQE05SvnR6u47TLkkHZYzXJJGqgciYnb5fh6wAdgEzIqISRExDWijWFC/ETi3vm5m7gT2RcTLI6JGseZrQ1l3bkSMiYgXA2My8+dD1itJw5IzXJJGqvcBXRExAdgK3J6ZByPiOorgNAboyMw9EbESWB0R3RQzWBeU33EpcCswlmLd1jcBImID8I3yOxYPZackDU8GLkkjRmZ+HzitfP8gxd2Efet0AV19ynYD5/dT9197v69P+dXA1YPQZEmjhJcUJUmSKmbgkiRJqpiBS5IkqWIGLkmSpIoZuCRJkir2jO5SrH8gbF3ZBcCSzPyd8njAD4SVJKkZNi86Fq5uvQcGbF50bLOboEFy1MBVPhD2ncDjdWWvBRZQPBCWugfCvo7iwbHdEfEVnnwg7NUR8Q6KB8JeTvFA2D8Evgt8OSJOysx/H8R+SZL0jL165eP09PQcveIQe3WtRs+nm90KDYZncknxKQ+EjYhfA/5f4L11dQbjgbCSJEkj0lEDV/0DYSNiLLAK+HOKS4S9BuOBsJIkSSPSs91p/mTgFcBKikuHr4yITwBfZeAPhNUwd/zxx/Poo48O2vfVarVB+Z7jjjuORx55ZFC+S5KkRjyrwJWZm4BXAUTES4H/k5nvLddwdUbEJGAiT38g7CbqHggbEfsi4uUUa7jmAi6aHwEeffTRllwDMVjBTZKkRg3KthCZ+TDQ+0DYr1I+EJZiJuxV5QNhL+bJYNX7QNhNwAO9D4SVJEkaiZ7RDFf9A2EPVzYYD4SVJEkaidz4VJIkqWIGLkmSpIoZuCRJkipm4JIkSaqYgUuSJKliBi5JkqSKGbgkSZIqZuCSJEmqmIFLkiSpYgYuSZKkihm4JEmSKmbgkiRJqpiBS5IkqWIGLkmSpIqNa3YDJKkqETER+CzwG8BOYDHQA9xU/nMLsDgzD0XEQuAS4ACwLDPviojJwC3AdGAXcFFmbo+I04BPlnXXZeY1Q9szScONM1ySRrKFwC8z8zRgCXA9sBxYmpmzgBpwXkTMAC4DzgDmAteWYW0RsLmsezOwtPzeG4ALgJnAqRFx0hD2SdIw5AyXBs3mRcfC1dOa3Yyn2bzo2GY3Qc3zSmAtQGZmRLQBY4H7yvNrgTcCB4GNmbkX2BsR24ATKQLVR+vqfiAipgITM/MhgIi4Bzgb+Peh6ZKk4cjApUHz6pWP09PT0+xmPM2razV6Pt3sVqhJvg28OSK+CJwK/Drws8zs/Q91FzANmArsqPtcf+X1ZTv71P2NapovaaTwkqKkkexGinC0HngLcD/FbFavKcBjZZ0pRyk/Wl1JOiwDl6SR7PVAd2bOBu4Evgs8EBGzy/PzgA3AJmBWREyKiGlAG8WC+o3AufV1M3MnsC8iXh4RNYo1XxuGqD+ShikvKUoayb4D/D8R8ZcUs1ALgOcAXRExAdgK3J6ZByPiOorgNAboyMw9EbESWB0R3cA+ioXyAJcCt1KsB1uXmd8cyk5JGn4MXJJGrMz8OXBOP6fO6qduF9DVp2w3cH4/df8VOG2QmilpFPCSoiRJUsUMXJIkSRUzcEmSJFXMwCVJklQxA5ckSVLFDFySJEkVM3BJkiRVzMAlSZJUMQOXJElSxdxpXpIkoFarNbsJT3Pcccc1uwkaJAYuSdKo19PTM2jfVavVBvX7NDJ4SVGSJKliBi5JkqSKGbgkSZIq9ozWcEXEqcBHMnN2RLwWWAEcBPYCf5qZP42IhcAlwAFgWWbeFRGTgVuA6cAu4KLM3B4RpwGfLOuuy8xrBrtjkiRJreKoM1wRcQXw98CksuiTwJLMnA18AfiriJgBXAacAcwFro2IicAiYHNmzgJuBpaW33EDcAEwEzg1Ik4atB5JkiS1mGdySfEh4G11x+/IzG+X78cBe4BTgI2ZuTczdwDbgBMpAtXdZd21wDkRMRWYmJkPZWYPcA9w9oB7IkmS1KKOGrgy8w5gf93xTwAi4nTgfwF/C0wFdtR9bBcwrU95fdnOfupKkiSNSA0tmo+It1NcFnxTZm6nCFBT6qpMAR7rU95fWX25JEnSiPSsA1dE/AnFzNbszPxuWbwJmBURkyJiGtAGbAE2AueWdeYBGzJzJ7AvIl4eETWKNV8bBtgPSZKklvWsdpqPiLHAdcAPgC9EBMB9mfmhiLiOIjiNAToyc09ErARWR0Q3sI9ioTzApcCtwFiKuxS/OSi9kSRJakHPKHBl5veB08rD4w9Tpwvo6lO2Gzi/n7r/Wvd9kiRJI5obn0qSJFXMwCVJklQxA5ckSVLFDFySJEkVM3BJkiRV7FltCyEdTa1Wa3YTnua4445rdhPUJBExHlgNvBQ4CCwEDgA3AT0U+wUuzsxDEbEQuKQ8vywz74qIycAtwHSKp2JclJnbI+I0iufKHqDY2uaaIe2YpGHHGS4Nmp6enkF7Deb3PfLII03+N6MmOhcYl5mnA38NdALLgaWZOQuoAedFxAzgMuAMis2Yr42IicAiYHNZ92Zgafm9N1DsKzgTODUiThrCPkkahgxckkayB4FxETGG4jmu+4GTgfvK82uBc4BTgI2ZuTczdwDbgBMpAtXd9XUjYiowMTMfyswe4B7g7KHqkKThyUuKkkayX1JcTvwv4HnAm4Ezy6AExWXCaRRhbEfd5/orry/b2afub1TTfEkjhTNckkayPwfuyczfBF5DsZ5rQt35KcBjFAFqylHKj1ZXkg7LwCVpJHuUJ2eoHgHGAw9ExOyybB7FM2A3AbMiYlJETAPaKBbUb6RYB/arupm5E9gXES+PiBrFmq8NQ9EZScOXlxQljWR/C9wYERsoZrauAr4FdEXEBGArcHtmHoyI6yiC0xigIzP3RMRKYHVEdAP7KBbKA1wK3AqMpbhL8ZtD2itJw46BS9KIlZm/BP64n1Nn9VO3C+jqU7YbOL+fuv8KnDZIzZQ0CnhJUZIkqWIGLkmSpIoZuCRJkipm4JIkSaqYgUuSJKliBi5JkqSKGbgkSZIqZuCSJEmqmIFLkiSpYgYuSZKkihm4JEmSKmbgkiRJqpiBS5IkqWIGLkmSpIoZuCRJkipm4JIkSaqYgUuSJKliBi5JkqSKGbgkSZIqZuCSJEmqmIFLkiSpYgYuSZKkihm4JEmSKjbumVSKiFOBj2Tm7Ij4n8BNQA+wBVicmYciYiFwCXAAWJaZd0XEZOAWYDqwC7goM7dHxGnAJ8u66zLzmsHumCRJUqs46gxXRFwB/D0wqSxaDizNzFlADTgvImYAlwFnAHOBayNiIrAI2FzWvRlYWn7HDcAFwEzg1Ig4afC6JEmS1FqeySXFh4C31R2fDNxXvl8LnAOcAmzMzL2ZuQPYBpxIEajurq8bEVOBiZn5UGb2APcAZw+4J5IkSS3qqIErM+8A9tcV1cqgBMVlwmnAVGBHXZ3+yuvLdvZTV5IkaUR6Rmu4+jhU934K8BhFgJpylPKj1ZWkQRUR7wLeVR5OAl5LMfP+CVyHKmkINXKX4gMRMbt8Pw/YAGwCZkXEpIiYBrRRDGQbgXPr62bmTmBfRLw8ImoUa742DKAPktSvzLwpM2dn5mzgfoq1ph/EdaiShlgjget9wDUR8Q1gAnB7Zj4MXEcRnL4KdGTmHmAl8KqI6AYuBnp/BV4K3EoR1B7IzG8OrBuSdHgR8TrgVZn5GVyHKqkJntElxcz8PnBa+f5B4Kx+6nQBXX3KdgPn91P3X3u/T5KGwFU8+YOvinWov1FNsyWNFG58KmlEi4jnAr+VmevLItehShpyBi5JI92ZwD/XHbsOVdKQa+QuRUkaTgL4bt3x+4CuiJgAbKVYh3owInrXoY6hXIcaESuB1eU61H0UC+XhyXWoYynuUnQdqqQjMnBJGtEy82/6HLsOVdKQ85KiJElSxQxckiRJFTNwSZIkVczAJUmSVDEDlyRJUsUMXJIkSRUzcEmSJFXMwCVJklQxA5ckSVLFDFySJEkVM3BJkiRVzMAlSZJUMQOXJElSxQxckiRJFTNwSZIkVczAJUmSVDEDlyRJUsUMXJIkSRUzcEmSJFXMwCVJklQxA5ckSVLFDFySJEkVM3BJkiRVzMAlSZJUMQOXJElSxQxckiRJFTNwSZIkVWxcsxsgSVWKiCuBtwITgE8D9wE3AT3AFmBxZh6KiIXAJcABYFlm3hURk4FbgOnALuCizNweEacBnyzrrsvMa4a4W5KGGWe4JI1YETEbOB04AzgLeBGwHFiambOAGnBeRMwALivrzQWujYiJwCJgc1n3ZmBp+dU3ABcAM4FTI+KkIeuUpGHJwCVpJJsLbAbuBL4E3AWcTDHLBbAWOAc4BdiYmXszcwewDTiRIlDdXV83IqYCEzPzoczsAe4Bzh6i/kgaprykKGkkex7wEuDNwMuAfwLGlEEJisuE04CpwI66z/VXXl+2s0/d36io/ZJGCGe4JI1kvwDuycx9mZnAHorQ1GsK8BhFgJpylPKj1dUI197eTq1WO+oLeEb12tvbm9wjDaWGZrgiYjywGngpcBBYSLF49CYGsBB1YF2RpKfpBi6PiOXAC4BjgX+JiNmZeS8wD1gPbAI6I2ISMBFooxjHNgLnlufnARsyc2dE7IuIlwPfpbhs6aL5UWDLli3NboKGsUZnuM4FxmXm6cBfA50MzkJUSRo0mXkX8ABFYPoSsBh4H3BNRHyD4s7F2zPzYeA6YAPwVaAjM/cAK4FXRUQ3cDFPBqtLgVvL730gM785dL1Sq1qzZg3t7e2MHTuW9vZ21qxZ0+wmqYU0uobrQWBcRIyhWM+wHziNpy5EfSPF7NfGzNwL7I2I+oWoH62r+4EG26ERZu7cuXzlK18BYMyYMbzhDW/gnnvuaXKrNJxl5hX9FJ/VT70uoKtP2W7g/H7q/ivFmCcBRdi6/PLLOfbYYwF4/PHHufzyywGYP39+M5umFtHoDNcvKS4n/hfFAHUdUBvgQlSNcnPnzmXdunU897nPBeC5z30u69atY+7cuc1tmCQdxRVXXMG4ceO48cYb2bNnDzfeeCPjxo3jiiv6y/sajRoNXH9OsRD1N4HXUKznmlB3vpGFqBrl1q1bx3Oe8xzuuOMOAO644w6e85znsG7duia3TJKO7Ic//CGrV69mzpw5jB8/njlz5rB69Wp++MMfNrtpahGNBq5HeXKG6hFgPPBAuckglItLKdY3zIqISRExjacvRK2vq1HgSHf5APzyl7/kd3/3dwH43d/9XX75y18C/d/x4x0+kqThotHA9bfASRHRu8D0KorFqANdiKoRbsuWLfT09PT7ApgyZQpf/epX2bdvH1/96leZMqWYCO2vvncMSWoVJ5xwAueffz4ve9nLGDNmDC972cs4//zzOeGEE5rdNLWIhhbNZ+YvgT/u59SAFqJqdKvVauzatYvPf/7znHTSSXz+859n165dv5r9kqRW9fu///t8+tOfZtKkSQA88cQT7Nq1i3e+851NbplahRufqqVMnDiRlStX8tznPpeVK1cyceLEZjdJko5q/fr1XHnllTzvec+jVqvxvOc9jyuvvJL169c3u2lqET7aRy3jla98Jb//+7/PF7/4RbZu3UpbW9uvjiWplW3dupUHHniAZcuW/aps//79XHvttU1slVqJM1xqGR0dHXzuc59jxYoV7NmzhxUrVvC5z32Ojo6OZjdNko6ora2N7u7up5R1d3fT1tbWpBap1TjDpZbRuzngkiVLfjXD1dnZ6aaBklpeR0cHCxYsYNWqVcycOZPu7m4WLFhAZ2dns5umFmHgUkuZP3++AUvSsOMPRh2NgUuSpEHgD0YdiWu4JEmSKmbgkiRJqpiBS5IkqWIGLkmSpIoZuCRJkipm4JIkSaqYgUuSJKliBi5JkqSKGbgkSZIqZuBSS1mzZg3t7e2MHTuW9vZ21qxZ0+wmSZI0YD7aRy1jzZo1dHR0PO3hr4CPy5AkDWvOcKlldHZ2smrVKubMmcP48eOZM2cOq1atorOzs9lNkyRpQAxcahlbt25l5syZTymbOXMmW7dubVKLJEkaHAYutYy2tja6u7ufUtbd3U1bW1uTWiRJ0uAwcKlldHR0sGDBAtavX8/+/ftZv349CxYsoKOjo9lNkyRpQAxcahnz58/nTW96E/PmzWPChAnMmzePN73pTS6YlzQseJe1jsS7FNUy1qxZw5e//GXWrl37lLsUTz/9dEOXGhYRDwA7ysPvAZ3ATUAPsAVYnJmHImIhcAlwAFiWmXdFxGTgFmA6sAu4KDO3R8RpwCfLuusy85qh7JNaj3dZ62ic4VLL8C5FDbaImASQmbPL17uB5cDSzJwF1IDzImIGcBlwBjAXuDYiJgKLgM1l3ZuBpeVX3wBcAMwETo2Ik4ayX2o9jl86Gme41DK8S1EVeA1wTESsoxjvrgJOBu4rz68F3ggcBDZm5l5gb0RsA06kCFQfrav7gYiYCkzMzIcAIuIe4Gzg34emS2pFjl86Gme41DK8S1EV2A18jGLW6lLgVqCWmT3l+V3ANGAqT152PFx5fdnOfupqFHP80tEYuNQyvEtRFXgQuCUzezLzQeAXwPPrzk8BHqMIUFOOUn60uhrFHL90NF5SVMvoXVi6ZMkStm7dSltbG52dnS441UC8B3g18P+JiBdSzE6ti4jZmXkvMA9YD2wCOss1XxOBNooF9RuBc8vz84ANmbkzIvZFxMuB71LMnrlofpRz/NLRGLjUUubPn+8ApcG0CrgpIrop7kp8D/BzoCsiJgBbgdsz82BEXAdsoJj578jMPRGxElhdfn4fxUJ5ePLy5FiKuxS/OaS9Ukty/NKRGLgkjViZWR+S6p3VT90uoKtP2W7g/H7q/itw2iA1U9Io4BouSZKkihm4JEmSKmbgkiRJqpiBS5IkqWIGLkmSpIq18l2KYwEefvjhZrdD0hCo+399bDPbMUgcv6RR5mhjWCsHrhcAXHjhhc1uh6Sh9QLgoWY3YoAcv6TRq98xrJUD178Bs4CfUDxYVtLINpZioPq3ZjdkEDh+SaPPEcewWk9PT3/lkiRJGiQumpckSaqYgUuSJKliBi61nIg4NSLubXY7JOnZcvzS4bTyonmNQhFxBfBO4PFmt0WSng3HLx2JM1xqNQ8Bb2t2IySpAY5fOiwDl1pKZt4B7G92OyTp2XL80pEYuCRJkipm4JIkSaqYgUuSJKli7jQvSZJUMWe4JEmSKmbgkiRJqpiBS5IkqWIGLkmSpIoZuCRJkipm4JIkSaqYgUuSJKli/3+vrk5t8tiEswAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 720x2160 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = [\"cat_0\",\"cat_1\",\"cat_2\",\"cat_3\",\"cat_4\",\"cat_5\",\"cat_6\",\"cat_7\",\"cat_8\",\"cat_9\",\"cat_10\",\"cat_11\",\"cat_12\",\"cat_13\",\"cat_14\",\"cat_15\",\"cat_16\",\"cat_17\",\"cat_18\",\"cat_19\",\"cat_20\",\"cat_21\",\"cat_22\",\"cat_23\",\"cat_24\",\"cat_25\",\"category\"]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (10, 30))\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "for x in range (20,26):\n",
    "    index = x%10\n",
    "    ax=plt.subplot(5,2,index+1) \n",
    "    plt.boxplot(df[cols[x]])\n",
    "    ax.set_title(cols[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFixOutliers = df\n",
    "\n",
    "outlierframe =['cat_9','cat_10','cat_11','cat_12','cat_12','cat_13','cat_17','cat_18','cat_19','cat_22','cat_24','cat_25']\n",
    "for cols in outlierframe :\n",
    "    Q1 = dfFixOutliers[cols].quantile(0.25)\n",
    "    Q3 = dfFixOutliers[cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1     \n",
    "\n",
    "    filter = (dfFixOutliers[cols] >= Q1 - 1.5 * IQR) & (dfFixOutliers[cols] <= Q3 + 1.5 *IQR)\n",
    "    dfFixOutliers=dfFixOutliers.loc[filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x2160 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (10, 30))\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "# for x in range (0,10): \n",
    "for x in range (10,12): \n",
    "    index = x%10\n",
    "    ax=plt.subplot(5,2,index+1)\n",
    "    plt.boxplot(dfFixOutliers[outlierframe[x]])\n",
    "    ax.set_title(outlierframe[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "      <th>cat_8</th>\n",
       "      <th>cat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_17</th>\n",
       "      <th>cat_18</th>\n",
       "      <th>cat_19</th>\n",
       "      <th>cat_20</th>\n",
       "      <th>cat_21</th>\n",
       "      <th>cat_22</th>\n",
       "      <th>cat_23</th>\n",
       "      <th>cat_24</th>\n",
       "      <th>cat_25</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.980142</td>\n",
       "      <td>1.557762</td>\n",
       "      <td>-1.134152</td>\n",
       "      <td>-1.482144</td>\n",
       "      <td>-1.273575</td>\n",
       "      <td>-1.194822</td>\n",
       "      <td>-0.996246</td>\n",
       "      <td>-0.673919</td>\n",
       "      <td>-0.730094</td>\n",
       "      <td>-0.576136</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.558060</td>\n",
       "      <td>-0.601064</td>\n",
       "      <td>-0.431706</td>\n",
       "      <td>-0.982956</td>\n",
       "      <td>-1.157560</td>\n",
       "      <td>-1.799805</td>\n",
       "      <td>1.076145</td>\n",
       "      <td>0.039481</td>\n",
       "      <td>0.679935</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.248937</td>\n",
       "      <td>1.522809</td>\n",
       "      <td>-1.345525</td>\n",
       "      <td>-1.393379</td>\n",
       "      <td>-1.349129</td>\n",
       "      <td>-1.095365</td>\n",
       "      <td>-1.055516</td>\n",
       "      <td>-0.800330</td>\n",
       "      <td>-0.770404</td>\n",
       "      <td>-0.613393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.824485</td>\n",
       "      <td>-0.690527</td>\n",
       "      <td>-0.640220</td>\n",
       "      <td>-1.080771</td>\n",
       "      <td>-1.363526</td>\n",
       "      <td>-1.673845</td>\n",
       "      <td>1.192847</td>\n",
       "      <td>0.032540</td>\n",
       "      <td>0.817749</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.038764</td>\n",
       "      <td>0.905593</td>\n",
       "      <td>0.663269</td>\n",
       "      <td>-0.361342</td>\n",
       "      <td>-0.399633</td>\n",
       "      <td>-0.575981</td>\n",
       "      <td>-0.617926</td>\n",
       "      <td>-0.857086</td>\n",
       "      <td>-0.984167</td>\n",
       "      <td>-1.182897</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.147469</td>\n",
       "      <td>-0.817265</td>\n",
       "      <td>-0.777244</td>\n",
       "      <td>-0.948433</td>\n",
       "      <td>-1.104407</td>\n",
       "      <td>-0.769732</td>\n",
       "      <td>1.149554</td>\n",
       "      <td>0.271705</td>\n",
       "      <td>1.256284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.913184</td>\n",
       "      <td>0.745321</td>\n",
       "      <td>0.730728</td>\n",
       "      <td>-0.205116</td>\n",
       "      <td>-0.293632</td>\n",
       "      <td>-0.553879</td>\n",
       "      <td>-0.446421</td>\n",
       "      <td>-0.684238</td>\n",
       "      <td>-0.864460</td>\n",
       "      <td>-0.963612</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.032862</td>\n",
       "      <td>-0.621939</td>\n",
       "      <td>-0.670008</td>\n",
       "      <td>-0.642041</td>\n",
       "      <td>-0.908407</td>\n",
       "      <td>-0.873299</td>\n",
       "      <td>1.079910</td>\n",
       "      <td>0.213649</td>\n",
       "      <td>1.133674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.603234</td>\n",
       "      <td>0.383004</td>\n",
       "      <td>-2.483341</td>\n",
       "      <td>-1.972125</td>\n",
       "      <td>-1.574663</td>\n",
       "      <td>-1.377773</td>\n",
       "      <td>-1.382133</td>\n",
       "      <td>-1.049283</td>\n",
       "      <td>-0.792391</td>\n",
       "      <td>-0.362173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.433034</td>\n",
       "      <td>-1.024520</td>\n",
       "      <td>-0.942566</td>\n",
       "      <td>-0.735541</td>\n",
       "      <td>-0.400136</td>\n",
       "      <td>0.624224</td>\n",
       "      <td>-0.278157</td>\n",
       "      <td>0.503928</td>\n",
       "      <td>-0.055398</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3243</th>\n",
       "      <td>-0.114973</td>\n",
       "      <td>0.770044</td>\n",
       "      <td>-0.506029</td>\n",
       "      <td>-0.414601</td>\n",
       "      <td>-0.551868</td>\n",
       "      <td>-0.616500</td>\n",
       "      <td>-0.728900</td>\n",
       "      <td>-0.808070</td>\n",
       "      <td>-0.901105</td>\n",
       "      <td>-0.924225</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.663737</td>\n",
       "      <td>-0.902255</td>\n",
       "      <td>-0.716179</td>\n",
       "      <td>-0.876510</td>\n",
       "      <td>-0.278882</td>\n",
       "      <td>0.923729</td>\n",
       "      <td>0.838037</td>\n",
       "      <td>1.911783</td>\n",
       "      <td>0.786115</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3244</th>\n",
       "      <td>-0.046037</td>\n",
       "      <td>0.910708</td>\n",
       "      <td>0.170065</td>\n",
       "      <td>-0.154225</td>\n",
       "      <td>-0.506761</td>\n",
       "      <td>-0.741742</td>\n",
       "      <td>-0.836090</td>\n",
       "      <td>-0.920292</td>\n",
       "      <td>-0.925535</td>\n",
       "      <td>-0.960418</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.802159</td>\n",
       "      <td>-0.841122</td>\n",
       "      <td>-0.826394</td>\n",
       "      <td>-0.585941</td>\n",
       "      <td>-0.212441</td>\n",
       "      <td>1.575922</td>\n",
       "      <td>1.038500</td>\n",
       "      <td>1.403794</td>\n",
       "      <td>0.785380</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3245</th>\n",
       "      <td>-0.769594</td>\n",
       "      <td>-0.278543</td>\n",
       "      <td>-0.798353</td>\n",
       "      <td>-0.383829</td>\n",
       "      <td>-0.144780</td>\n",
       "      <td>-0.238319</td>\n",
       "      <td>-0.134937</td>\n",
       "      <td>-0.045733</td>\n",
       "      <td>0.043118</td>\n",
       "      <td>0.031690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117677</td>\n",
       "      <td>-0.358024</td>\n",
       "      <td>-0.239575</td>\n",
       "      <td>-0.102619</td>\n",
       "      <td>-0.117763</td>\n",
       "      <td>0.002822</td>\n",
       "      <td>-0.263098</td>\n",
       "      <td>-0.129638</td>\n",
       "      <td>0.175027</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246</th>\n",
       "      <td>-0.145967</td>\n",
       "      <td>0.429040</td>\n",
       "      <td>-0.612465</td>\n",
       "      <td>-0.461942</td>\n",
       "      <td>-0.613890</td>\n",
       "      <td>-0.573525</td>\n",
       "      <td>-0.543523</td>\n",
       "      <td>-0.676499</td>\n",
       "      <td>-0.610387</td>\n",
       "      <td>-0.698553</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.628015</td>\n",
       "      <td>-0.875416</td>\n",
       "      <td>-0.877033</td>\n",
       "      <td>-0.591695</td>\n",
       "      <td>-0.278882</td>\n",
       "      <td>0.680207</td>\n",
       "      <td>0.573576</td>\n",
       "      <td>1.216375</td>\n",
       "      <td>0.544737</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>-0.423313</td>\n",
       "      <td>0.163057</td>\n",
       "      <td>-0.434072</td>\n",
       "      <td>-0.334121</td>\n",
       "      <td>-0.342122</td>\n",
       "      <td>-0.519499</td>\n",
       "      <td>-0.194207</td>\n",
       "      <td>-0.250829</td>\n",
       "      <td>-0.135222</td>\n",
       "      <td>-0.124791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279728</td>\n",
       "      <td>-0.709910</td>\n",
       "      <td>-0.558304</td>\n",
       "      <td>-0.676564</td>\n",
       "      <td>-0.511424</td>\n",
       "      <td>-0.456232</td>\n",
       "      <td>-0.016519</td>\n",
       "      <td>0.673048</td>\n",
       "      <td>0.231019</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3057 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cat_0     cat_1     cat_2     cat_3     cat_4     cat_5     cat_6  \\\n",
       "0    -0.980142  1.557762 -1.134152 -1.482144 -1.273575 -1.194822 -0.996246   \n",
       "1    -1.248937  1.522809 -1.345525 -1.393379 -1.349129 -1.095365 -1.055516   \n",
       "2     1.038764  0.905593  0.663269 -0.361342 -0.399633 -0.575981 -0.617926   \n",
       "3     0.913184  0.745321  0.730728 -0.205116 -0.293632 -0.553879 -0.446421   \n",
       "4    -1.603234  0.383004 -2.483341 -1.972125 -1.574663 -1.377773 -1.382133   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3243 -0.114973  0.770044 -0.506029 -0.414601 -0.551868 -0.616500 -0.728900   \n",
       "3244 -0.046037  0.910708  0.170065 -0.154225 -0.506761 -0.741742 -0.836090   \n",
       "3245 -0.769594 -0.278543 -0.798353 -0.383829 -0.144780 -0.238319 -0.134937   \n",
       "3246 -0.145967  0.429040 -0.612465 -0.461942 -0.613890 -0.573525 -0.543523   \n",
       "3247 -0.423313  0.163057 -0.434072 -0.334121 -0.342122 -0.519499 -0.194207   \n",
       "\n",
       "         cat_7     cat_8     cat_9  ...    cat_17    cat_18    cat_19  \\\n",
       "0    -0.673919 -0.730094 -0.576136  ... -0.558060 -0.601064 -0.431706   \n",
       "1    -0.800330 -0.770404 -0.613393  ... -0.824485 -0.690527 -0.640220   \n",
       "2    -0.857086 -0.984167 -1.182897  ... -1.147469 -0.817265 -0.777244   \n",
       "3    -0.684238 -0.864460 -0.963612  ... -1.032862 -0.621939 -0.670008   \n",
       "4    -1.049283 -0.792391 -0.362173  ... -0.433034 -1.024520 -0.942566   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3243 -0.808070 -0.901105 -0.924225  ... -0.663737 -0.902255 -0.716179   \n",
       "3244 -0.920292 -0.925535 -0.960418  ... -0.802159 -0.841122 -0.826394   \n",
       "3245 -0.045733  0.043118  0.031690  ...  0.117677 -0.358024 -0.239575   \n",
       "3246 -0.676499 -0.610387 -0.698553  ... -0.628015 -0.875416 -0.877033   \n",
       "3247 -0.250829 -0.135222 -0.124791  ... -0.279728 -0.709910 -0.558304   \n",
       "\n",
       "        cat_20    cat_21    cat_22    cat_23    cat_24    cat_25  category  \n",
       "0    -0.982956 -1.157560 -1.799805  1.076145  0.039481  0.679935         0  \n",
       "1    -1.080771 -1.363526 -1.673845  1.192847  0.032540  0.817749         0  \n",
       "2    -0.948433 -1.104407 -0.769732  1.149554  0.271705  1.256284         0  \n",
       "3    -0.642041 -0.908407 -0.873299  1.079910  0.213649  1.133674         0  \n",
       "4    -0.735541 -0.400136  0.624224 -0.278157  0.503928 -0.055398         0  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3243 -0.876510 -0.278882  0.923729  0.838037  1.911783  0.786115         7  \n",
       "3244 -0.585941 -0.212441  1.575922  1.038500  1.403794  0.785380         7  \n",
       "3245 -0.102619 -0.117763  0.002822 -0.263098 -0.129638  0.175027         7  \n",
       "3246 -0.591695 -0.278882  0.680207  0.573576  1.216375  0.544737         7  \n",
       "3247 -0.676564 -0.511424 -0.456232 -0.016519  0.673048  0.231019         7  \n",
       "\n",
       "[3057 rows x 27 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "d_pre = dfFixOutliers\n",
    "variable_cat2 = [\"cat_0\",\"cat_1\",\"cat_2\",\"cat_3\",\"cat_4\",\"cat_5\",\"cat_6\",\"cat_7\",\"cat_8\",\"cat_9\",\"cat_10\",\"cat_11\",\"cat_12\",\"cat_13\",\"cat_14\",\"cat_15\",\"cat_16\",\"cat_17\",\"cat_18\",\"cat_19\",\"cat_20\",\"cat_21\",\"cat_22\",\"cat_23\",\"cat_24\",\"cat_25\"]\n",
    "d_pre[variable_cat2] = StandardScaler().fit_transform(d_pre[variable_cat2])\n",
    "d_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x y partition ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "3243    7\n",
       "3244    7\n",
       "3245    7\n",
       "3246    7\n",
       "3247    7\n",
       "Name: category, Length: 3248, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=df.drop(['category'],axis=1)\n",
    "\n",
    "y=df['category']\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting DataSet ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Training: (3248, 26)\n",
      "Input Training: (2598, 26)\n",
      "Input Test: (650, 26)\n",
      "Output Training: (2598,)\n",
      "Output Test: (650,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,train_size=0.8,random_state=1)\n",
    "print(\"Input Training:\",x.shape)\n",
    "print(\"Input Training:\",x_train.shape)\n",
    "print(\"Input Test:\",x_test.shape)\n",
    "print(\"Output Training:\",y_train.shape)\n",
    "print(\"Output Test:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over Fiting ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After oversampling:  Counter({6: 457, 7: 457, 0: 457, 5: 457, 3: 457, 4: 457, 1: 457, 2: 457})\n",
      "Input Training: (3057, 26)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "# define oversampling strategy\n",
    "\n",
    "x=d_pre.drop(['category'],axis=1)\n",
    "\n",
    "y=d_pre['category']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,train_size=0.8,random_state=1)\n",
    "SMOTE = SMOTE()\n",
    "\n",
    "# fit and apply the transform\n",
    "X_train_SMOTE, y_train_SMOTE = SMOTE.fit_resample(x_train, y_train)\n",
    "\n",
    "# summarize class distribution\n",
    "print(\"After oversampling: \",Counter(y_train_SMOTE))\n",
    "print(\"Input Training:\",x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Selection ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hasil data Tanpa normalisasi \n",
    "- hasil data dengan Normalisasi\n",
    "- hasil dataNormalisasi + imbalanced yg nambah\n",
    "\n",
    "menggunakan model : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNB ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.18      0.26        84\n",
      "           1       0.17      0.51      0.25        41\n",
      "           2       0.18      0.70      0.29        46\n",
      "           3       0.12      0.28      0.16        40\n",
      "           4       0.22      0.41      0.29        68\n",
      "           5       0.08      0.01      0.02       117\n",
      "           6       0.40      0.02      0.03       115\n",
      "           7       0.19      0.08      0.11       101\n",
      "\n",
      "    accuracy                           0.19       612\n",
      "   macro avg       0.23      0.27      0.18       612\n",
      "weighted avg       0.25      0.19      0.14       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = GaussianNB()\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(x_train, y_train)\n",
    "y_pred = nbtrain.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.18      0.27        84\n",
      "           1       0.16      0.56      0.25        41\n",
      "           2       0.19      0.67      0.30        46\n",
      "           3       0.18      0.40      0.25        40\n",
      "           4       0.20      0.46      0.28        68\n",
      "           5       0.08      0.01      0.02       117\n",
      "           6       0.50      0.02      0.03       115\n",
      "           7       0.22      0.06      0.09       101\n",
      "\n",
      "    accuracy                           0.20       612\n",
      "   macro avg       0.26      0.29      0.19       612\n",
      "weighted avg       0.28      0.20      0.15       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = GaussianNB()\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "y_pred = nbtrain.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.52      0.66        84\n",
      "           1       0.71      0.83      0.76        41\n",
      "           2       0.45      0.72      0.55        46\n",
      "           3       0.40      0.20      0.27        40\n",
      "           4       0.41      0.35      0.38        68\n",
      "           5       0.73      0.81      0.77       117\n",
      "           6       0.52      0.52      0.52       115\n",
      "           7       0.41      0.48      0.44       101\n",
      "\n",
      "    accuracy                           0.57       612\n",
      "   macro avg       0.56      0.55      0.54       612\n",
      "weighted avg       0.58      0.57      0.56       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# normal\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = svm.SVC()\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(x_train, y_train)\n",
    "y_pred = nbtrain.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.61      0.72        84\n",
      "           1       0.66      0.85      0.74        41\n",
      "           2       0.39      0.80      0.53        46\n",
      "           3       0.35      0.70      0.47        40\n",
      "           4       0.34      0.59      0.43        68\n",
      "           5       0.86      0.67      0.75       117\n",
      "           6       0.61      0.47      0.53       115\n",
      "           7       0.65      0.22      0.33       101\n",
      "\n",
      "    accuracy                           0.56       612\n",
      "   macro avg       0.60      0.61      0.56       612\n",
      "weighted avg       0.64      0.56      0.57       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# overfit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = svm.SVC()\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "y_pred = nbtrain.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.63      0.66        84\n",
      "           1       0.60      0.66      0.63        41\n",
      "           2       0.51      0.57      0.54        46\n",
      "           3       0.25      0.30      0.27        40\n",
      "           4       0.36      0.31      0.33        68\n",
      "           5       0.66      0.65      0.66       117\n",
      "           6       0.54      0.57      0.55       115\n",
      "           7       0.39      0.39      0.39       101\n",
      "\n",
      "    accuracy                           0.52       612\n",
      "   macro avg       0.50      0.51      0.50       612\n",
      "weighted avg       0.52      0.52      0.52       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# normal\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = DecisionTreeClassifier(random_state=0)\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(x_train, y_train)\n",
    "y_pred = nbtrain.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.55      0.59        84\n",
      "           1       0.49      0.68      0.57        41\n",
      "           2       0.46      0.54      0.50        46\n",
      "           3       0.24      0.33      0.28        40\n",
      "           4       0.33      0.41      0.36        68\n",
      "           5       0.68      0.49      0.57       117\n",
      "           6       0.58      0.63      0.61       115\n",
      "           7       0.41      0.32      0.36       101\n",
      "\n",
      "    accuracy                           0.49       612\n",
      "   macro avg       0.48      0.49      0.48       612\n",
      "weighted avg       0.51      0.49      0.50       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# overfit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = DecisionTreeClassifier(random_state=0)\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "y_pred = nbtrain.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knn ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 neighbors accuracy : 0.619281045751634\n",
      "2 neighbors accuracy : 0.6062091503267973\n",
      "3 neighbors accuracy : 0.6029411764705882\n",
      "4 neighbors accuracy : 0.6241830065359477\n",
      "5 neighbors accuracy : 0.6241830065359477\n",
      "6 neighbors accuracy : 0.6356209150326797\n",
      "7 neighbors accuracy : 0.6209150326797386\n",
      "8 neighbors accuracy : 0.619281045751634\n",
      "9 neighbors accuracy : 0.6258169934640523\n",
      "10 neighbors accuracy : 0.6176470588235294\n",
      "11 neighbors accuracy : 0.6127450980392157\n",
      "12 neighbors accuracy : 0.619281045751634\n",
      "13 neighbors accuracy : 0.6176470588235294\n",
      "14 neighbors accuracy : 0.6143790849673203\n",
      "15 neighbors accuracy : 0.619281045751634\n",
      "16 neighbors accuracy : 0.6143790849673203\n",
      "17 neighbors accuracy : 0.6127450980392157\n",
      "18 neighbors accuracy : 0.5980392156862745\n",
      "19 neighbors accuracy : 0.5964052287581699\n"
     ]
    }
   ],
   "source": [
    "# normal\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score  \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "for i in range (1,20) :\n",
    "    # Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "    modelnb = KNeighborsClassifier(n_neighbors=i)\n",
    "    # Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "    nbtrain = modelnb.fit(x_train, y_train)\n",
    "    y_pred = nbtrain.predict(x_test)\n",
    "    print(str(i) + \" neighbors accuracy : \" + str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 neighbors accuracy : 0.6356209150326797\n"
     ]
    }
   ],
   "source": [
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = KNeighborsClassifier(n_neighbors=6)\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(x_train, y_train)\n",
    "y_pred = nbtrain.predict(x_test)\n",
    "print(str(i) + \" neighbors accuracy : \" + str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.73      0.76        84\n",
      "           1       0.72      0.83      0.77        41\n",
      "           2       0.54      0.87      0.67        46\n",
      "           3       0.24      0.55      0.34        40\n",
      "           4       0.39      0.59      0.47        68\n",
      "           5       0.84      0.65      0.73       117\n",
      "           6       0.67      0.52      0.59       115\n",
      "           7       0.60      0.24      0.34       101\n",
      "\n",
      "    accuracy                           0.58       612\n",
      "   macro avg       0.60      0.62      0.58       612\n",
      "weighted avg       0.64      0.58      0.59       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# overfit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = KNeighborsClassifier(n_neighbors=6)\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "y_pred = nbtrain.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "49/49 [==============================] - 1s 3ms/step - loss: 0.5567 - accuracy: 0.1027\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -0.0487 - accuracy: 0.1121\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -1.5492 - accuracy: 0.1080\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -3.8946 - accuracy: 0.0822\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -6.1870 - accuracy: 0.0806\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: -8.0621 - accuracy: 0.0806\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 5ms/step - loss: -9.6526 - accuracy: 0.0806\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -11.0874 - accuracy: 0.0806\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: -12.4210 - accuracy: 0.0806\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: -13.6943 - accuracy: 0.0806\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -14.9274 - accuracy: 0.0806\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -16.1278 - accuracy: 0.0806\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -17.3062 - accuracy: 0.0806\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -18.4644 - accuracy: 0.0806\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -19.6123 - accuracy: 0.0806\n",
      "Epoch 16/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -20.7456 - accuracy: 0.0806\n",
      "Epoch 17/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -21.8701 - accuracy: 0.0806\n",
      "Epoch 18/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: -22.9904 - accuracy: 0.0806\n",
      "Epoch 19/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -24.1852 - accuracy: 0.0916\n",
      "Epoch 20/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -25.3525 - accuracy: 0.0957\n",
      "Epoch 21/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -26.4882 - accuracy: 0.0998\n",
      "Epoch 22/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -27.6197 - accuracy: 0.1039\n",
      "Epoch 23/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: -28.7412 - accuracy: 0.1047\n",
      "Epoch 24/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: -29.8568 - accuracy: 0.1076\n",
      "Epoch 25/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -30.9593 - accuracy: 0.1063\n",
      "Epoch 26/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -32.0439 - accuracy: 0.1051\n",
      "Epoch 27/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -33.1597 - accuracy: 0.1063\n",
      "Epoch 28/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -34.2598 - accuracy: 0.1088\n",
      "Epoch 29/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -35.3508 - accuracy: 0.1100\n",
      "Epoch 30/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: -36.4468 - accuracy: 0.1084\n",
      "Epoch 31/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -37.5691 - accuracy: 0.1121\n",
      "Epoch 32/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -38.6513 - accuracy: 0.1129\n",
      "Epoch 33/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -39.7397 - accuracy: 0.1145\n",
      "Epoch 34/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -40.8058 - accuracy: 0.1129\n",
      "Epoch 35/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -41.8955 - accuracy: 0.1141\n",
      "Epoch 36/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -42.9990 - accuracy: 0.1157\n",
      "Epoch 37/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -44.0189 - accuracy: 0.1104\n",
      "Epoch 38/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -45.1597 - accuracy: 0.1162\n",
      "Epoch 39/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -46.1813 - accuracy: 0.1125\n",
      "Epoch 40/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -47.2394 - accuracy: 0.1112\n",
      "Epoch 41/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -48.3682 - accuracy: 0.1137\n",
      "Epoch 42/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -49.4656 - accuracy: 0.1182\n",
      "Epoch 43/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -50.5178 - accuracy: 0.1141\n",
      "Epoch 44/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -51.6459 - accuracy: 0.1194\n",
      "Epoch 45/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -52.6170 - accuracy: 0.1133\n",
      "Epoch 46/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -53.7233 - accuracy: 0.1166\n",
      "Epoch 47/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -54.8264 - accuracy: 0.1198\n",
      "Epoch 48/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -55.8079 - accuracy: 0.1108\n",
      "Epoch 49/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -56.9250 - accuracy: 0.1157\n",
      "Epoch 50/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: -57.9819 - accuracy: 0.1145\n",
      "Epoch 51/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -59.1317 - accuracy: 0.1194\n",
      "Epoch 52/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -59.9794 - accuracy: 0.1121\n",
      "Epoch 53/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -61.1397 - accuracy: 0.1117\n",
      "Epoch 54/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -62.2612 - accuracy: 0.1166\n",
      "Epoch 55/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -63.3763 - accuracy: 0.1194\n",
      "Epoch 56/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -64.4730 - accuracy: 0.1202\n",
      "Epoch 57/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -65.4842 - accuracy: 0.1182\n",
      "Epoch 58/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -66.5502 - accuracy: 0.1186\n",
      "Epoch 59/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -67.6276 - accuracy: 0.1190\n",
      "Epoch 60/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -68.7181 - accuracy: 0.1202\n",
      "Epoch 61/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -69.8093 - accuracy: 0.1207\n",
      "Epoch 62/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -70.8666 - accuracy: 0.1223\n",
      "Epoch 63/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -71.9489 - accuracy: 0.1215\n",
      "Epoch 64/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -72.8804 - accuracy: 0.1145\n",
      "Epoch 65/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -74.0729 - accuracy: 0.1223\n",
      "Epoch 66/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -75.1276 - accuracy: 0.1207\n",
      "Epoch 67/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -76.1853 - accuracy: 0.1223\n",
      "Epoch 68/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -77.2561 - accuracy: 0.1211\n",
      "Epoch 69/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -78.3395 - accuracy: 0.1227\n",
      "Epoch 70/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -79.3880 - accuracy: 0.1235\n",
      "Epoch 71/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -80.2669 - accuracy: 0.1129\n",
      "Epoch 72/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -81.4339 - accuracy: 0.1194\n",
      "Epoch 73/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -82.5117 - accuracy: 0.1190\n",
      "Epoch 74/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -83.6260 - accuracy: 0.1215\n",
      "Epoch 75/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -84.6906 - accuracy: 0.1231\n",
      "Epoch 76/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -85.8027 - accuracy: 0.1235\n",
      "Epoch 77/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -86.8548 - accuracy: 0.1227\n",
      "Epoch 78/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -87.8740 - accuracy: 0.1243\n",
      "Epoch 79/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -88.9667 - accuracy: 0.1231\n",
      "Epoch 80/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -90.0151 - accuracy: 0.1223\n",
      "Epoch 81/100\n",
      "49/49 [==============================] - 0s 4ms/step - loss: -90.8207 - accuracy: 0.1170\n",
      "Epoch 82/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -91.8085 - accuracy: 0.1104\n",
      "Epoch 83/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -92.9145 - accuracy: 0.1121\n",
      "Epoch 84/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -94.0071 - accuracy: 0.1137\n",
      "Epoch 85/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -95.0779 - accuracy: 0.1145\n",
      "Epoch 86/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -96.3117 - accuracy: 0.1215\n",
      "Epoch 87/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -97.4766 - accuracy: 0.1252\n",
      "Epoch 88/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -98.4724 - accuracy: 0.1219\n",
      "Epoch 89/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -99.4576 - accuracy: 0.1211\n",
      "Epoch 90/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -100.6397 - accuracy: 0.1239\n",
      "Epoch 91/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -101.7114 - accuracy: 0.1256\n",
      "Epoch 92/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -102.6625 - accuracy: 0.1211\n",
      "Epoch 93/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -103.8000 - accuracy: 0.1239\n",
      "Epoch 94/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -104.8698 - accuracy: 0.1256\n",
      "Epoch 95/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -105.6271 - accuracy: 0.1133\n",
      "Epoch 96/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -106.6919 - accuracy: 0.1145\n",
      "Epoch 97/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -107.8854 - accuracy: 0.1182\n",
      "Epoch 98/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -109.0462 - accuracy: 0.1235\n",
      "Epoch 99/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -110.1288 - accuracy: 0.1227\n",
      "Epoch 100/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -111.1925 - accuracy: 0.1243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x161c75a6d60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normal\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units=10,kernel_initializer='uniform' , activation='tanh' , input_dim=26))\n",
    "classifier.add(Dense(units=6,kernel_initializer='uniform' , activation='tanh'))\n",
    "classifier.add(Dense(units=1 , kernel_initializer='uniform' , activation='sigmoid'))\n",
    "classifier.compile(optimizer='adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "classifier.fit(x_train, y_train, batch_size = 50, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.30      0.46        84\n",
      "           1       0.07      1.00      0.13        41\n",
      "           2       0.00      0.00      0.00        46\n",
      "           3       0.00      0.00      0.00        40\n",
      "           4       0.00      0.00      0.00        68\n",
      "           5       0.00      0.00      0.00       117\n",
      "           6       0.00      0.00      0.00       115\n",
      "           7       0.00      0.00      0.00       101\n",
      "\n",
      "    accuracy                           0.11       612\n",
      "   macro avg       0.13      0.16      0.07       612\n",
      "weighted avg       0.14      0.11      0.07       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = classifier.predict(x_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Parameters: batch_size: 15 - epochs: 25 Accuracy: 0.11329243332147598\n",
      "2 Parameters: batch_size: 15 - epochs: 50 Accuracy: 0.11492842435836792\n",
      "3 Parameters: batch_size: 15 - epochs: 100 Accuracy: 0.11411043256521225\n",
      "4 Parameters: batch_size: 20 - epochs: 25 Accuracy: 0.11738241463899612\n",
      "5 Parameters: batch_size: 20 - epochs: 50 Accuracy: 0.12392637878656387\n",
      "6 Parameters: batch_size: 20 - epochs: 100 Accuracy: 0.11492842435836792\n",
      "7 Parameters: batch_size: 25 - epochs: 25 Accuracy: 0.11533742398023605\n",
      "8 Parameters: batch_size: 25 - epochs: 50 Accuracy: 0.1231083869934082\n",
      "9 Parameters: batch_size: 25 - epochs: 100 Accuracy: 0.11002045124769211\n",
      "10 Parameters: batch_size: 50 - epochs: 25 Accuracy: 0.10715746134519577\n",
      "11 Parameters: batch_size: 50 - epochs: 50 Accuracy: 0.11901840567588806\n",
      "12 Parameters: batch_size: 50 - epochs: 100 Accuracy: 0.12597137689590454\n"
     ]
    }
   ],
   "source": [
    "# Defining a function for finding best hyperparameters\n",
    "def FunctionFindBestParams(X_train, y_train):\n",
    "    \n",
    "    # Defining the list of hyper parameters to try\n",
    "    TrialNumber=0\n",
    "    batch_size_list=[15, 20, 25, 50]\n",
    "    epoch_list=[25, 50, 100]\n",
    "    \n",
    "    for batch_size_trial in batch_size_list:\n",
    "        for epochs_trial in epoch_list:\n",
    "            TrialNumber+=1\n",
    " \n",
    "            # Creating the classifier ANN model\n",
    "            classifier = Sequential()\n",
    "            classifier.add(Dense(units=10,kernel_initializer='uniform' , activation='tanh' , input_dim=26))\n",
    "            classifier.add(Dense(units=6,kernel_initializer='uniform' , activation='tanh'))\n",
    "            classifier.add(Dense(units=1 , kernel_initializer='uniform' , activation='sigmoid'))\n",
    "            classifier.compile(optimizer='adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "            survivalANN_Model=classifier.fit(X_train,y_train, batch_size=batch_size_trial , epochs=epochs_trial, verbose=0)\n",
    "            Accuracy = survivalANN_Model.history['accuracy'][-1]\n",
    "            \n",
    "            # printing the results of the current iteration\n",
    "            print(TrialNumber, 'Parameters:','batch_size:', batch_size_trial,'-', 'epochs:',epochs_trial, 'Accuracy:', Accuracy)\n",
    " \n",
    "# Calling the function\n",
    "ResultsData = FunctionFindBestParams(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "74/74 [==============================] - 1s 2ms/step - loss: 0.4649 - accuracy: 0.1433\n",
      "Epoch 2/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -0.8309 - accuracy: 0.1313\n",
      "Epoch 3/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -3.6571 - accuracy: 0.1269\n",
      "Epoch 4/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -6.3535 - accuracy: 0.1250\n",
      "Epoch 5/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -8.3862 - accuracy: 0.1250\n",
      "Epoch 6/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -10.1197 - accuracy: 0.1250\n",
      "Epoch 7/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -11.7128 - accuracy: 0.1250\n",
      "Epoch 8/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -13.2165 - accuracy: 0.1250\n",
      "Epoch 9/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -14.6647 - accuracy: 0.1250\n",
      "Epoch 10/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -16.0893 - accuracy: 0.1250\n",
      "Epoch 11/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -17.5345 - accuracy: 0.1310\n",
      "Epoch 12/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -18.9889 - accuracy: 0.1398\n",
      "Epoch 13/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -20.4032 - accuracy: 0.1420\n",
      "Epoch 14/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -21.8058 - accuracy: 0.1452\n",
      "Epoch 15/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -23.2203 - accuracy: 0.1502\n",
      "Epoch 16/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -24.5556 - accuracy: 0.1461\n",
      "Epoch 17/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -25.9498 - accuracy: 0.1515\n",
      "Epoch 18/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -27.3047 - accuracy: 0.1534\n",
      "Epoch 19/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -28.6533 - accuracy: 0.1518\n",
      "Epoch 20/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -30.0120 - accuracy: 0.1556\n",
      "Epoch 21/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -31.3449 - accuracy: 0.1570\n",
      "Epoch 22/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -32.6828 - accuracy: 0.1554\n",
      "Epoch 23/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -34.0331 - accuracy: 0.1584\n",
      "Epoch 24/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -35.2948 - accuracy: 0.1529\n",
      "Epoch 25/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -36.6864 - accuracy: 0.1592\n",
      "Epoch 26/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -38.0446 - accuracy: 0.1581\n",
      "Epoch 27/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -39.3487 - accuracy: 0.1595\n",
      "Epoch 28/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -40.6386 - accuracy: 0.1554\n",
      "Epoch 29/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -42.0399 - accuracy: 0.1595\n",
      "Epoch 30/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -43.3720 - accuracy: 0.1619\n",
      "Epoch 31/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -44.6921 - accuracy: 0.1617\n",
      "Epoch 32/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -46.0186 - accuracy: 0.1608\n",
      "Epoch 33/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -47.3315 - accuracy: 0.1619\n",
      "Epoch 34/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -48.6581 - accuracy: 0.1625\n",
      "Epoch 35/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -50.0110 - accuracy: 0.1619\n",
      "Epoch 36/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -51.2222 - accuracy: 0.1581\n",
      "Epoch 37/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -52.6187 - accuracy: 0.1614\n",
      "Epoch 38/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -53.9599 - accuracy: 0.1630\n",
      "Epoch 39/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -55.2375 - accuracy: 0.1595\n",
      "Epoch 40/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -56.5623 - accuracy: 0.1622\n",
      "Epoch 41/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -57.9327 - accuracy: 0.1638\n",
      "Epoch 42/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -59.2156 - accuracy: 0.1649\n",
      "Epoch 43/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -60.5804 - accuracy: 0.1647\n",
      "Epoch 44/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -61.8272 - accuracy: 0.1617\n",
      "Epoch 45/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -63.2276 - accuracy: 0.1644\n",
      "Epoch 46/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -64.4932 - accuracy: 0.1644\n",
      "Epoch 47/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -65.8453 - accuracy: 0.1649\n",
      "Epoch 48/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -67.0598 - accuracy: 0.1636\n",
      "Epoch 49/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -68.4346 - accuracy: 0.1649\n",
      "Epoch 50/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -69.7782 - accuracy: 0.1655\n",
      "Epoch 51/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -71.0591 - accuracy: 0.1644\n",
      "Epoch 52/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -72.3884 - accuracy: 0.1666\n",
      "Epoch 53/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -73.6737 - accuracy: 0.1641\n",
      "Epoch 54/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -75.0418 - accuracy: 0.1660\n",
      "Epoch 55/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -76.3286 - accuracy: 0.1638\n",
      "Epoch 56/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -77.6635 - accuracy: 0.1671\n",
      "Epoch 57/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -78.8870 - accuracy: 0.1636\n",
      "Epoch 58/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -80.2610 - accuracy: 0.1660\n",
      "Epoch 59/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -81.5299 - accuracy: 0.1638\n",
      "Epoch 60/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -82.8208 - accuracy: 0.1671\n",
      "Epoch 61/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -84.2009 - accuracy: 0.1674\n",
      "Epoch 62/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -85.4796 - accuracy: 0.1674\n",
      "Epoch 63/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -86.6788 - accuracy: 0.1625\n",
      "Epoch 64/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -87.8899 - accuracy: 0.1600\n",
      "Epoch 65/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -89.4340 - accuracy: 0.1663\n",
      "Epoch 66/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -90.7200 - accuracy: 0.1671\n",
      "Epoch 67/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -92.0036 - accuracy: 0.1677\n",
      "Epoch 68/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -93.2911 - accuracy: 0.1674\n",
      "Epoch 69/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -94.6688 - accuracy: 0.1671\n",
      "Epoch 70/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -95.9947 - accuracy: 0.1674\n",
      "Epoch 71/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -97.2421 - accuracy: 0.1679\n",
      "Epoch 72/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -98.5883 - accuracy: 0.1663\n",
      "Epoch 73/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -99.8705 - accuracy: 0.1663\n",
      "Epoch 74/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -101.2582 - accuracy: 0.1685\n",
      "Epoch 75/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -102.4925 - accuracy: 0.1655\n",
      "Epoch 76/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -103.5775 - accuracy: 0.1630\n",
      "Epoch 77/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -105.1510 - accuracy: 0.1671\n",
      "Epoch 78/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -106.5597 - accuracy: 0.1699\n",
      "Epoch 79/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -107.6221 - accuracy: 0.1658\n",
      "Epoch 80/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -108.5671 - accuracy: 0.1543\n",
      "Epoch 81/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -109.9404 - accuracy: 0.1551\n",
      "Epoch 82/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -111.5794 - accuracy: 0.1663\n",
      "Epoch 83/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -112.6756 - accuracy: 0.1603\n",
      "Epoch 84/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -113.7639 - accuracy: 0.1545\n",
      "Epoch 85/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -115.1595 - accuracy: 0.1573\n",
      "Epoch 86/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -116.8502 - accuracy: 0.1658\n",
      "Epoch 87/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -118.2655 - accuracy: 0.1685\n",
      "Epoch 88/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -119.5755 - accuracy: 0.1696\n",
      "Epoch 89/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -120.7732 - accuracy: 0.1660\n",
      "Epoch 90/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -122.1176 - accuracy: 0.1688\n",
      "Epoch 91/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -123.4405 - accuracy: 0.1674\n",
      "Epoch 92/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -124.7882 - accuracy: 0.1677\n",
      "Epoch 93/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -126.1231 - accuracy: 0.1699\n",
      "Epoch 94/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -127.3878 - accuracy: 0.1690\n",
      "Epoch 95/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -128.6776 - accuracy: 0.1663\n",
      "Epoch 96/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -130.0164 - accuracy: 0.1682\n",
      "Epoch 97/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -131.3926 - accuracy: 0.1690\n",
      "Epoch 98/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -132.7141 - accuracy: 0.1688\n",
      "Epoch 99/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -133.9287 - accuracy: 0.1674\n",
      "Epoch 100/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -134.6722 - accuracy: 0.1592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x161cedceeb0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overfit\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units=10,kernel_initializer='uniform' , activation='tanh' , input_dim=26))\n",
    "classifier.add(Dense(units=6,kernel_initializer='uniform' , activation='tanh'))\n",
    "classifier.add(Dense(units=1 , kernel_initializer='uniform' , activation='sigmoid'))\n",
    "classifier.compile(optimizer='adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "classifier.fit(X_train_SMOTE, y_train_SMOTE, batch_size = 50, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.21      0.35        84\n",
      "           1       0.07      1.00      0.13        41\n",
      "           2       0.00      0.00      0.00        46\n",
      "           3       0.00      0.00      0.00        40\n",
      "           4       0.00      0.00      0.00        68\n",
      "           5       0.00      0.00      0.00       117\n",
      "           6       0.00      0.00      0.00       115\n",
      "           7       0.00      0.00      0.00       101\n",
      "\n",
      "    accuracy                           0.10       612\n",
      "   macro avg       0.13      0.15      0.06       612\n",
      "weighted avg       0.14      0.10      0.06       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = classifier.predict(x_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GaussianNB\n",
    "- normal:   0.19\n",
    "- overfit:  0.21 \n",
    "\n",
    "SVM\n",
    "- normal:   0.57\n",
    "- overfit:  0.55\n",
    "\n",
    "Decision Tree\n",
    "- normal:   0.52\n",
    "- overfit:  0.52\n",
    "\n",
    "KNN\n",
    "- normal:   0.6356209150326797\n",
    "- overfit:  0.59\n",
    "\n",
    "ANN\n",
    "- normal:   0.11 \n",
    "- overfit:  0.10\n",
    "\n",
    "In this case, we can see that it looks like Support Vector Machines (SVM) has the largest estimated accuracy score at about 0.98 or 98%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7288e82646d3164eca24130947288f8779d11454649f2c02a5dfc42af7f324c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

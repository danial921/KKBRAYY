{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "      <th>cat_8</th>\n",
       "      <th>cat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_17</th>\n",
       "      <th>cat_18</th>\n",
       "      <th>cat_19</th>\n",
       "      <th>cat_20</th>\n",
       "      <th>cat_21</th>\n",
       "      <th>cat_22</th>\n",
       "      <th>cat_23</th>\n",
       "      <th>cat_24</th>\n",
       "      <th>cat_25</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9323</td>\n",
       "      <td>7670</td>\n",
       "      <td>3810</td>\n",
       "      <td>2313</td>\n",
       "      <td>1690</td>\n",
       "      <td>1450</td>\n",
       "      <td>1390</td>\n",
       "      <td>1744</td>\n",
       "      <td>1818</td>\n",
       "      <td>2246</td>\n",
       "      <td>...</td>\n",
       "      <td>2088</td>\n",
       "      <td>1870</td>\n",
       "      <td>2251</td>\n",
       "      <td>2286</td>\n",
       "      <td>3249</td>\n",
       "      <td>4540</td>\n",
       "      <td>6821</td>\n",
       "      <td>12954</td>\n",
       "      <td>104300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8820</td>\n",
       "      <td>7629</td>\n",
       "      <td>3669</td>\n",
       "      <td>2388</td>\n",
       "      <td>1623</td>\n",
       "      <td>1531</td>\n",
       "      <td>1343</td>\n",
       "      <td>1646</td>\n",
       "      <td>1785</td>\n",
       "      <td>2211</td>\n",
       "      <td>...</td>\n",
       "      <td>1909</td>\n",
       "      <td>1810</td>\n",
       "      <td>2111</td>\n",
       "      <td>2218</td>\n",
       "      <td>3125</td>\n",
       "      <td>4585</td>\n",
       "      <td>6945</td>\n",
       "      <td>12943</td>\n",
       "      <td>105986</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13101</td>\n",
       "      <td>6905</td>\n",
       "      <td>5009</td>\n",
       "      <td>3260</td>\n",
       "      <td>2465</td>\n",
       "      <td>1954</td>\n",
       "      <td>1690</td>\n",
       "      <td>1602</td>\n",
       "      <td>1610</td>\n",
       "      <td>1676</td>\n",
       "      <td>...</td>\n",
       "      <td>1692</td>\n",
       "      <td>1725</td>\n",
       "      <td>2019</td>\n",
       "      <td>2310</td>\n",
       "      <td>3281</td>\n",
       "      <td>4908</td>\n",
       "      <td>6899</td>\n",
       "      <td>13322</td>\n",
       "      <td>111351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12866</td>\n",
       "      <td>6717</td>\n",
       "      <td>5054</td>\n",
       "      <td>3392</td>\n",
       "      <td>2559</td>\n",
       "      <td>1972</td>\n",
       "      <td>1826</td>\n",
       "      <td>1736</td>\n",
       "      <td>1708</td>\n",
       "      <td>1882</td>\n",
       "      <td>...</td>\n",
       "      <td>1769</td>\n",
       "      <td>1856</td>\n",
       "      <td>2091</td>\n",
       "      <td>2523</td>\n",
       "      <td>3399</td>\n",
       "      <td>4871</td>\n",
       "      <td>6825</td>\n",
       "      <td>13230</td>\n",
       "      <td>109851</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8157</td>\n",
       "      <td>6292</td>\n",
       "      <td>2910</td>\n",
       "      <td>1899</td>\n",
       "      <td>1423</td>\n",
       "      <td>1301</td>\n",
       "      <td>1084</td>\n",
       "      <td>1453</td>\n",
       "      <td>1767</td>\n",
       "      <td>2447</td>\n",
       "      <td>...</td>\n",
       "      <td>2172</td>\n",
       "      <td>1586</td>\n",
       "      <td>1908</td>\n",
       "      <td>2458</td>\n",
       "      <td>3705</td>\n",
       "      <td>5406</td>\n",
       "      <td>5382</td>\n",
       "      <td>13690</td>\n",
       "      <td>95304</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat_0  cat_1  cat_2  cat_3  cat_4  cat_5  cat_6  cat_7  cat_8  cat_9  ...  \\\n",
       "0   9323   7670   3810   2313   1690   1450   1390   1744   1818   2246  ...   \n",
       "1   8820   7629   3669   2388   1623   1531   1343   1646   1785   2211  ...   \n",
       "2  13101   6905   5009   3260   2465   1954   1690   1602   1610   1676  ...   \n",
       "3  12866   6717   5054   3392   2559   1972   1826   1736   1708   1882  ...   \n",
       "4   8157   6292   2910   1899   1423   1301   1084   1453   1767   2447  ...   \n",
       "\n",
       "   cat_17  cat_18  cat_19  cat_20  cat_21  cat_22  cat_23  cat_24  cat_25  \\\n",
       "0    2088    1870    2251    2286    3249    4540    6821   12954  104300   \n",
       "1    1909    1810    2111    2218    3125    4585    6945   12943  105986   \n",
       "2    1692    1725    2019    2310    3281    4908    6899   13322  111351   \n",
       "3    1769    1856    2091    2523    3399    4871    6825   13230  109851   \n",
       "4    2172    1586    1908    2458    3705    5406    5382   13690   95304   \n",
       "\n",
       "   category  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('Fitur_LBPuniform_Ikan.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cat_0', 'cat_1', 'cat_2', 'cat_3', 'cat_4', 'cat_5', 'cat_6', 'cat_7',\n",
       "       'cat_8', 'cat_9', 'cat_10', 'cat_11', 'cat_12', 'cat_13', 'cat_14',\n",
       "       'cat_15', 'cat_16', 'cat_17', 'cat_18', 'cat_19', 'cat_20', 'cat_21',\n",
       "       'cat_22', 'cat_23', 'cat_24', 'cat_25', 'category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    577\n",
       "5    564\n",
       "6    544\n",
       "0    500\n",
       "4    331\n",
       "3    252\n",
       "1    240\n",
       "2    240\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Adaboost\n",
    "*   Adaboost + Normalisasi\n",
    "*   Adaboost + Normalisasi + imbalanced Overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x y partition ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "3243    7\n",
       "3244    7\n",
       "3245    7\n",
       "3246    7\n",
       "3247    7\n",
       "Name: category, Length: 3248, dtype: int64"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df.drop(['category'],axis=1)\n",
    "y = df['category']\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting DataSet ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Training: (3248, 26)\n",
      "Input Training: (2598, 26)\n",
      "Input Test: (650, 26)\n",
      "Output Training: (2598,)\n",
      "Output Test: (650,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_ada, x_test_ada, y_train_ada, y_test_ada = train_test_split(x, y, train_size=0.8,random_state=1)\n",
    "print(\"Input Training:\",x.shape)\n",
    "print(\"Input Training:\",x_train_ada.shape)\n",
    "print(\"Input Test:\",x_test_ada.shape)\n",
    "print(\"Output Training:\",y_train_ada.shape)\n",
    "print(\"Output Test:\",y_test_ada.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15975355, 0.07241733, 0.00958981, ..., 0.14675932, 0.14670291,\n",
       "        0.15010377],\n",
       "       [0.15691719, 0.14460951, 0.13513871, ..., 0.14074908, 0.14242612,\n",
       "        0.14362121],\n",
       "       [0.13442079, 0.146122  , 0.13631884, ..., 0.12884309, 0.132377  ,\n",
       "        0.13785988],\n",
       "       ...,\n",
       "       [0.14636597, 0.06027012, 0.00744168, ..., 0.14717055, 0.14481282,\n",
       "        0.14544028],\n",
       "       [0.12323081, 0.09096467, 0.0875205 , ..., 0.13785853, 0.13790885,\n",
       "        0.13935031],\n",
       "       [0.13003815, 0.13996243, 0.14634144, ..., 0.13315268, 0.13518736,\n",
       "        0.13548248]])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_mod = AdaBoostClassifier(n_estimators=300, learning_rate=0.2)\n",
    "\n",
    "ada_mod.fit(x_train_ada, y_train_ada)\n",
    "\n",
    "x_adaptiveresult = ada_mod.predict_proba(x_test_ada)\n",
    "x_adaptiveresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.159754</td>\n",
       "      <td>0.072417</td>\n",
       "      <td>0.009590</td>\n",
       "      <td>0.144885</td>\n",
       "      <td>0.169788</td>\n",
       "      <td>0.146759</td>\n",
       "      <td>0.146703</td>\n",
       "      <td>0.150104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.156917</td>\n",
       "      <td>0.144610</td>\n",
       "      <td>0.135139</td>\n",
       "      <td>0.118532</td>\n",
       "      <td>0.018006</td>\n",
       "      <td>0.140749</td>\n",
       "      <td>0.142426</td>\n",
       "      <td>0.143621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.134421</td>\n",
       "      <td>0.146122</td>\n",
       "      <td>0.136319</td>\n",
       "      <td>0.071697</td>\n",
       "      <td>0.112361</td>\n",
       "      <td>0.128843</td>\n",
       "      <td>0.132377</td>\n",
       "      <td>0.137860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.163001</td>\n",
       "      <td>0.054822</td>\n",
       "      <td>0.052293</td>\n",
       "      <td>0.175421</td>\n",
       "      <td>0.141631</td>\n",
       "      <td>0.082407</td>\n",
       "      <td>0.164777</td>\n",
       "      <td>0.165648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.130812</td>\n",
       "      <td>0.090072</td>\n",
       "      <td>0.110969</td>\n",
       "      <td>0.124330</td>\n",
       "      <td>0.125790</td>\n",
       "      <td>0.137270</td>\n",
       "      <td>0.140710</td>\n",
       "      <td>0.140047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>0.136384</td>\n",
       "      <td>0.179467</td>\n",
       "      <td>0.163702</td>\n",
       "      <td>0.092131</td>\n",
       "      <td>0.025626</td>\n",
       "      <td>0.132467</td>\n",
       "      <td>0.133694</td>\n",
       "      <td>0.136527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>0.144047</td>\n",
       "      <td>0.138025</td>\n",
       "      <td>0.141977</td>\n",
       "      <td>0.068065</td>\n",
       "      <td>0.096695</td>\n",
       "      <td>0.130520</td>\n",
       "      <td>0.139547</td>\n",
       "      <td>0.141124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>0.146366</td>\n",
       "      <td>0.060270</td>\n",
       "      <td>0.007442</td>\n",
       "      <td>0.176805</td>\n",
       "      <td>0.171694</td>\n",
       "      <td>0.147171</td>\n",
       "      <td>0.144813</td>\n",
       "      <td>0.145440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>0.123231</td>\n",
       "      <td>0.090965</td>\n",
       "      <td>0.087520</td>\n",
       "      <td>0.144399</td>\n",
       "      <td>0.138768</td>\n",
       "      <td>0.137859</td>\n",
       "      <td>0.137909</td>\n",
       "      <td>0.139350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>0.130038</td>\n",
       "      <td>0.139962</td>\n",
       "      <td>0.146341</td>\n",
       "      <td>0.085228</td>\n",
       "      <td>0.094607</td>\n",
       "      <td>0.133153</td>\n",
       "      <td>0.135187</td>\n",
       "      <td>0.135482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>650 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat_0     cat_1     cat_2     cat_3     cat_4     cat_5     cat_6  \\\n",
       "0    0.159754  0.072417  0.009590  0.144885  0.169788  0.146759  0.146703   \n",
       "1    0.156917  0.144610  0.135139  0.118532  0.018006  0.140749  0.142426   \n",
       "2    0.134421  0.146122  0.136319  0.071697  0.112361  0.128843  0.132377   \n",
       "3    0.163001  0.054822  0.052293  0.175421  0.141631  0.082407  0.164777   \n",
       "4    0.130812  0.090072  0.110969  0.124330  0.125790  0.137270  0.140710   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "645  0.136384  0.179467  0.163702  0.092131  0.025626  0.132467  0.133694   \n",
       "646  0.144047  0.138025  0.141977  0.068065  0.096695  0.130520  0.139547   \n",
       "647  0.146366  0.060270  0.007442  0.176805  0.171694  0.147171  0.144813   \n",
       "648  0.123231  0.090965  0.087520  0.144399  0.138768  0.137859  0.137909   \n",
       "649  0.130038  0.139962  0.146341  0.085228  0.094607  0.133153  0.135187   \n",
       "\n",
       "        cat_7  \n",
       "0    0.150104  \n",
       "1    0.143621  \n",
       "2    0.137860  \n",
       "3    0.165648  \n",
       "4    0.140047  \n",
       "..        ...  \n",
       "645  0.136527  \n",
       "646  0.141124  \n",
       "647  0.145440  \n",
       "648  0.139350  \n",
       "649  0.135482  \n",
       "\n",
       "[650 rows x 8 columns]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_adaptivetest = np.array(ada_mod.predict_proba(x_test_ada))\n",
    "dataset_test = pd.DataFrame(x_adaptivetest, columns=x.columns[:8])\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.109725</td>\n",
       "      <td>0.090432</td>\n",
       "      <td>0.114522</td>\n",
       "      <td>0.138600</td>\n",
       "      <td>0.129700</td>\n",
       "      <td>0.136681</td>\n",
       "      <td>0.141701</td>\n",
       "      <td>0.138639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.139010</td>\n",
       "      <td>0.110770</td>\n",
       "      <td>0.133712</td>\n",
       "      <td>0.073902</td>\n",
       "      <td>0.106780</td>\n",
       "      <td>0.142641</td>\n",
       "      <td>0.146225</td>\n",
       "      <td>0.146960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.153692</td>\n",
       "      <td>0.068315</td>\n",
       "      <td>0.007921</td>\n",
       "      <td>0.157719</td>\n",
       "      <td>0.176270</td>\n",
       "      <td>0.146247</td>\n",
       "      <td>0.143749</td>\n",
       "      <td>0.146089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.130683</td>\n",
       "      <td>0.146666</td>\n",
       "      <td>0.168009</td>\n",
       "      <td>0.143703</td>\n",
       "      <td>0.018940</td>\n",
       "      <td>0.096666</td>\n",
       "      <td>0.146555</td>\n",
       "      <td>0.148778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.138894</td>\n",
       "      <td>0.148492</td>\n",
       "      <td>0.010369</td>\n",
       "      <td>0.128042</td>\n",
       "      <td>0.156890</td>\n",
       "      <td>0.138639</td>\n",
       "      <td>0.137410</td>\n",
       "      <td>0.141263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>0.132811</td>\n",
       "      <td>0.150818</td>\n",
       "      <td>0.171065</td>\n",
       "      <td>0.142008</td>\n",
       "      <td>0.021867</td>\n",
       "      <td>0.078832</td>\n",
       "      <td>0.150211</td>\n",
       "      <td>0.152389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>0.117397</td>\n",
       "      <td>0.100526</td>\n",
       "      <td>0.135026</td>\n",
       "      <td>0.137285</td>\n",
       "      <td>0.093730</td>\n",
       "      <td>0.137095</td>\n",
       "      <td>0.141192</td>\n",
       "      <td>0.137749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>0.146882</td>\n",
       "      <td>0.061214</td>\n",
       "      <td>0.007406</td>\n",
       "      <td>0.176998</td>\n",
       "      <td>0.171839</td>\n",
       "      <td>0.146589</td>\n",
       "      <td>0.144119</td>\n",
       "      <td>0.144954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2596</th>\n",
       "      <td>0.139311</td>\n",
       "      <td>0.050502</td>\n",
       "      <td>0.050160</td>\n",
       "      <td>0.171799</td>\n",
       "      <td>0.149022</td>\n",
       "      <td>0.143797</td>\n",
       "      <td>0.148739</td>\n",
       "      <td>0.146670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597</th>\n",
       "      <td>0.143401</td>\n",
       "      <td>0.059196</td>\n",
       "      <td>0.006879</td>\n",
       "      <td>0.188559</td>\n",
       "      <td>0.175326</td>\n",
       "      <td>0.144440</td>\n",
       "      <td>0.140556</td>\n",
       "      <td>0.141645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2598 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cat_0     cat_1     cat_2     cat_3     cat_4     cat_5     cat_6  \\\n",
       "0     0.109725  0.090432  0.114522  0.138600  0.129700  0.136681  0.141701   \n",
       "1     0.139010  0.110770  0.133712  0.073902  0.106780  0.142641  0.146225   \n",
       "2     0.153692  0.068315  0.007921  0.157719  0.176270  0.146247  0.143749   \n",
       "3     0.130683  0.146666  0.168009  0.143703  0.018940  0.096666  0.146555   \n",
       "4     0.138894  0.148492  0.010369  0.128042  0.156890  0.138639  0.137410   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2593  0.132811  0.150818  0.171065  0.142008  0.021867  0.078832  0.150211   \n",
       "2594  0.117397  0.100526  0.135026  0.137285  0.093730  0.137095  0.141192   \n",
       "2595  0.146882  0.061214  0.007406  0.176998  0.171839  0.146589  0.144119   \n",
       "2596  0.139311  0.050502  0.050160  0.171799  0.149022  0.143797  0.148739   \n",
       "2597  0.143401  0.059196  0.006879  0.188559  0.175326  0.144440  0.140556   \n",
       "\n",
       "         cat_7  \n",
       "0     0.138639  \n",
       "1     0.146960  \n",
       "2     0.146089  \n",
       "3     0.148778  \n",
       "4     0.141263  \n",
       "...        ...  \n",
       "2593  0.152389  \n",
       "2594  0.137749  \n",
       "2595  0.144954  \n",
       "2596  0.146670  \n",
       "2597  0.141645  \n",
       "\n",
       "[2598 rows x 8 columns]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_adaptivetrain = np.array(ada_mod.predict_proba(x_train_ada))\n",
    "dataset_train = pd.DataFrame(x_adaptivetrain, columns=x.columns[:8])\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.31      0.33       107\n",
      "           1       0.66      0.55      0.60        49\n",
      "           2       0.33      0.79      0.47        53\n",
      "           3       0.11      0.21      0.14        57\n",
      "           4       0.18      0.48      0.26        66\n",
      "           5       0.51      0.17      0.26       109\n",
      "           6       0.26      0.05      0.09        97\n",
      "           7       0.21      0.08      0.12       112\n",
      "\n",
      "    accuracy                           0.28       650\n",
      "   macro avg       0.33      0.33      0.28       650\n",
      "weighted avg       0.32      0.28      0.25       650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = GaussianNB()\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(dataset_train, y_train_ada)\n",
    "y_pred = nbtrain.predict(dataset_test)\n",
    "print(classification_report(y_test_ada, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.56      0.65       107\n",
      "           1       0.48      0.67      0.56        49\n",
      "           2       0.32      0.47      0.38        53\n",
      "           3       0.00      0.00      0.00        57\n",
      "           4       0.27      0.38      0.31        66\n",
      "           5       0.35      0.36      0.35       109\n",
      "           6       0.25      0.49      0.33        97\n",
      "           7       0.20      0.05      0.08       112\n",
      "\n",
      "    accuracy                           0.36       650\n",
      "   macro avg       0.33      0.37      0.33       650\n",
      "weighted avg       0.35      0.36      0.34       650\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = svm.SVC()\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(dataset_train, y_train_ada)\n",
    "y_pred = nbtrain.predict(dataset_test)\n",
    "print(classification_report(y_test_ada, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.61      0.58       107\n",
      "           1       0.61      0.61      0.61        49\n",
      "           2       0.56      0.58      0.57        53\n",
      "           3       0.26      0.28      0.27        57\n",
      "           4       0.27      0.20      0.23        66\n",
      "           5       0.38      0.38      0.38       109\n",
      "           6       0.41      0.52      0.46        97\n",
      "           7       0.28      0.22      0.25       112\n",
      "\n",
      "    accuracy                           0.42       650\n",
      "   macro avg       0.42      0.42      0.42       650\n",
      "weighted avg       0.41      0.42      0.41       650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = DecisionTreeClassifier(random_state=0)\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(dataset_train, y_train_ada)\n",
    "y_pred = nbtrain.predict(dataset_test)\n",
    "print(classification_report(y_test_ada, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 neighbors accuracy : 0.4307692307692308\n",
      "2 neighbors accuracy : 0.41384615384615386\n",
      "3 neighbors accuracy : 0.4276923076923077\n",
      "4 neighbors accuracy : 0.45076923076923076\n",
      "5 neighbors accuracy : 0.43846153846153846\n",
      "6 neighbors accuracy : 0.44769230769230767\n",
      "7 neighbors accuracy : 0.4276923076923077\n",
      "8 neighbors accuracy : 0.4307692307692308\n",
      "9 neighbors accuracy : 0.4107692307692308\n",
      "10 neighbors accuracy : 0.42923076923076925\n",
      "11 neighbors accuracy : 0.4169230769230769\n",
      "12 neighbors accuracy : 0.4169230769230769\n",
      "13 neighbors accuracy : 0.4246153846153846\n",
      "14 neighbors accuracy : 0.4169230769230769\n",
      "15 neighbors accuracy : 0.42153846153846153\n",
      "16 neighbors accuracy : 0.42\n",
      "17 neighbors accuracy : 0.42153846153846153\n",
      "18 neighbors accuracy : 0.4153846153846154\n",
      "19 neighbors accuracy : 0.4307692307692308\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score  \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "for i in range (1,20) :\n",
    "    # Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "    modelnb = KNeighborsClassifier(n_neighbors=i)\n",
    "    # Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "    nbtrain = modelnb.fit(dataset_train, y_train_ada)\n",
    "    y_pred = nbtrain.predict(dataset_test)\n",
    "    print(str(i) + \" neighbors accuracy : \" + str(accuracy_score(y_test_ada, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 neighbors accuracy : 0.45076923076923076\n"
     ]
    }
   ],
   "source": [
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = KNeighborsClassifier(n_neighbors=4)\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(dataset_train, y_train_ada)\n",
    "y_pred = nbtrain.predict(dataset_test)\n",
    "print(str(i) + \" neighbors accuracy : \" + str(accuracy_score(y_test_ada, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 1s 2ms/step - loss: 0.5340 - accuracy: 0.0747\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -0.4484 - accuracy: 0.0735\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: -2.8480 - accuracy: 0.0735\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -5.3006 - accuracy: 0.0735\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: -7.1661 - accuracy: 0.0735\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: -8.7215 - accuracy: 0.0735\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -10.1265 - accuracy: 0.0735\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -11.4490 - accuracy: 0.0735\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -12.7167 - accuracy: 0.0735\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -13.9547 - accuracy: 0.0735\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -15.1642 - accuracy: 0.0735\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -16.3559 - accuracy: 0.0735\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -17.5354 - accuracy: 0.0735\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -18.7018 - accuracy: 0.0735\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -19.8568 - accuracy: 0.0735\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -21.0106 - accuracy: 0.0735\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: -22.1526 - accuracy: 0.0735\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: -23.2923 - accuracy: 0.0735\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: -24.4267 - accuracy: 0.0735\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -25.5555 - accuracy: 0.0735\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -26.6846 - accuracy: 0.0735\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -27.8064 - accuracy: 0.0735\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -28.9286 - accuracy: 0.0735\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -30.0468 - accuracy: 0.0735\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -31.1653 - accuracy: 0.0735\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -32.2772 - accuracy: 0.0735\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: -33.3903 - accuracy: 0.0735\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -34.5051 - accuracy: 0.0735\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -35.6111 - accuracy: 0.0735\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -36.7196 - accuracy: 0.0735\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -37.8295 - accuracy: 0.0735\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -38.9336 - accuracy: 0.0735\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -40.0381 - accuracy: 0.0735\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -41.1423 - accuracy: 0.0735\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: -42.2473 - accuracy: 0.0735\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -43.3478 - accuracy: 0.0735\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -44.4484 - accuracy: 0.0735\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -45.5517 - accuracy: 0.0735\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -46.6499 - accuracy: 0.0735\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -47.7497 - accuracy: 0.0735\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -48.8486 - accuracy: 0.0735\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -49.9471 - accuracy: 0.0735\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -51.0441 - accuracy: 0.0735\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -52.1401 - accuracy: 0.0735\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: -53.2377 - accuracy: 0.0735\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -54.3333 - accuracy: 0.0735\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -55.4302 - accuracy: 0.0735\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -56.5258 - accuracy: 0.0735\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -57.6181 - accuracy: 0.0735\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -58.7152 - accuracy: 0.0735\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -59.8092 - accuracy: 0.0735\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -60.9031 - accuracy: 0.0735\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -61.9982 - accuracy: 0.0735\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -63.0907 - accuracy: 0.0735\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -64.1850 - accuracy: 0.0735\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -65.2786 - accuracy: 0.0735\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -66.3720 - accuracy: 0.0735\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -67.4630 - accuracy: 0.0735\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -68.5561 - accuracy: 0.0735\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -69.6517 - accuracy: 0.0735\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -70.7426 - accuracy: 0.0735\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -71.8338 - accuracy: 0.0735\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -72.9244 - accuracy: 0.0735\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -74.0191 - accuracy: 0.0735\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -75.1082 - accuracy: 0.0735\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -76.2008 - accuracy: 0.0735\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -77.2918 - accuracy: 0.0735\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -78.3840 - accuracy: 0.0735\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -79.4744 - accuracy: 0.0735\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -80.5638 - accuracy: 0.0735\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -81.6570 - accuracy: 0.0735\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -82.7464 - accuracy: 0.0735\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -83.8358 - accuracy: 0.0735\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -84.9277 - accuracy: 0.0735\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -86.0179 - accuracy: 0.0735\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -87.1097 - accuracy: 0.0735\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -88.1979 - accuracy: 0.0735\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -89.2898 - accuracy: 0.0735\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -90.3796 - accuracy: 0.0735\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -91.4701 - accuracy: 0.0735\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -92.5603 - accuracy: 0.0735\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -93.6523 - accuracy: 0.0735\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -94.7402 - accuracy: 0.0735\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -95.8321 - accuracy: 0.0735\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -96.9225 - accuracy: 0.0735\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -98.0125 - accuracy: 0.0735\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -99.1011 - accuracy: 0.0735\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: -100.1916 - accuracy: 0.0735\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -101.2831 - accuracy: 0.0735\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -102.3708 - accuracy: 0.0735\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -103.4610 - accuracy: 0.0735\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: -104.5517 - accuracy: 0.0735\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: -105.6393 - accuracy: 0.0735\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -106.7308 - accuracy: 0.0735\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -107.8187 - accuracy: 0.0735\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: -108.9115 - accuracy: 0.0735\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: -109.9977 - accuracy: 0.0735\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -111.0896 - accuracy: 0.0735\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -112.1785 - accuracy: 0.0735\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: -113.2686 - accuracy: 0.0735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a4782367f0>"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units=10,kernel_initializer='uniform' , activation='tanh' , input_dim=8))\n",
    "classifier.add(Dense(units=6,kernel_initializer='uniform' , activation='tanh'))\n",
    "classifier.add(Dense(units=1 , kernel_initializer='uniform' , activation='sigmoid'))\n",
    "classifier.compile(optimizer='adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "classifier.fit(dataset_train, y_train_ada, batch_size = 50, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Parameters: batch_size: 15 - epochs: 25 Accuracy: 0.07351808995008469\n",
      "2 Parameters: batch_size: 15 - epochs: 50 Accuracy: 0.07351808995008469\n",
      "3 Parameters: batch_size: 15 - epochs: 100 Accuracy: 0.07351808995008469\n",
      "4 Parameters: batch_size: 20 - epochs: 25 Accuracy: 0.07351808995008469\n",
      "5 Parameters: batch_size: 20 - epochs: 50 Accuracy: 0.07351808995008469\n",
      "6 Parameters: batch_size: 20 - epochs: 100 Accuracy: 0.07351808995008469\n",
      "7 Parameters: batch_size: 25 - epochs: 25 Accuracy: 0.07351808995008469\n",
      "8 Parameters: batch_size: 25 - epochs: 50 Accuracy: 0.07351808995008469\n",
      "9 Parameters: batch_size: 25 - epochs: 100 Accuracy: 0.07351808995008469\n",
      "10 Parameters: batch_size: 50 - epochs: 25 Accuracy: 0.07351808995008469\n",
      "11 Parameters: batch_size: 50 - epochs: 50 Accuracy: 0.07351808995008469\n",
      "12 Parameters: batch_size: 50 - epochs: 100 Accuracy: 0.07351808995008469\n"
     ]
    }
   ],
   "source": [
    "# Defining a function for finding best hyperparameters\n",
    "def FunctionFindBestParams(dataset_train, y_train):\n",
    "    \n",
    "    # Defining the list of hyper parameters to try\n",
    "    TrialNumber=0\n",
    "    batch_size_list=[15, 20, 25, 50]\n",
    "    epoch_list=[25, 50, 100]\n",
    "    \n",
    "    for batch_size_trial in batch_size_list:\n",
    "        for epochs_trial in epoch_list:\n",
    "            TrialNumber+=1\n",
    " \n",
    "            # Creating the classifier ANN model\n",
    "            classifier = Sequential()\n",
    "            classifier.add(Dense(units=10,kernel_initializer='uniform' , activation='tanh' , input_dim=8))\n",
    "            classifier.add(Dense(units=6,kernel_initializer='uniform' , activation='tanh'))\n",
    "            classifier.add(Dense(units=1 , kernel_initializer='uniform' , activation='sigmoid'))\n",
    "            classifier.compile(optimizer='adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "            survivalANN_Model=classifier.fit(dataset_train,y_train, batch_size=batch_size_trial , epochs=epochs_trial, verbose=0)\n",
    "            Accuracy = survivalANN_Model.history['accuracy'][-1]\n",
    "            \n",
    "            # printing the results of the current iteration\n",
    "            print(TrialNumber, 'Parameters:','batch_size:', batch_size_trial,'-', 'epochs:',epochs_trial, 'Accuracy:', Accuracy)\n",
    " \n",
    "# Calling the function\n",
    "ResultsData = FunctionFindBestParams(dataset_train, y_train_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       107\n",
      "           1       0.08      1.00      0.14        49\n",
      "           2       0.00      0.00      0.00        53\n",
      "           3       0.00      0.00      0.00        57\n",
      "           4       0.00      0.00      0.00        66\n",
      "           5       0.00      0.00      0.00       109\n",
      "           6       0.00      0.00      0.00        97\n",
      "           7       0.00      0.00      0.00       112\n",
      "\n",
      "    accuracy                           0.08       650\n",
      "   macro avg       0.01      0.12      0.02       650\n",
      "weighted avg       0.01      0.08      0.01       650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = classifier.predict(dataset_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "print(classification_report(y_test_ada, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Adaboost + Normalisasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check missing value ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check null =  0\n"
     ]
    }
   ],
   "source": [
    "print(\"check null = \",  df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data visualization ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Pair-wise Scatter Plots\n",
    "# import seaborn as sns\n",
    "# cols = [\"cat_0\",\"cat_1\",\"cat_2\",\"cat_3\",\"cat_4\",\"cat_5\",\"cat_6\",\"cat_7\",\"cat_8\",\"cat_9\",\"cat_10\",\"cat_11\",\"cat_12\",\"cat_13\",\"cat_14\",\"cat_15\",\"cat_16\",\"cat_17\",\"cat_18\",\"cat_19\",\"cat_20\",\"cat_21\",\"cat_22\",\"cat_23\",\"cat_24\",\"cat_25\",\"category\"]\n",
    "# pp = sns.pairplot(df[cols], size=1.8, aspect=1.8,\n",
    "#                   plot_kws=dict(edgecolor=\"k\", linewidth=0.5),\n",
    "#                   diag_kind=\"kde\", diag_kws=dict(shade=True))\n",
    "\n",
    "# fig = pp.fig \n",
    "# fig.subplots_adjust(top=0.93, wspace=0.3)\n",
    "# t = fig.suptitle('Wine Attributes Pairwise Plots', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fixing outlier ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x2160 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = [\"cat_0\",\"cat_1\",\"cat_2\",\"cat_3\",\"cat_4\",\"cat_5\",\"cat_6\",\"cat_7\",\"cat_8\",\"cat_9\",\"cat_10\",\"cat_11\",\"cat_12\",\"cat_13\",\"cat_14\",\"cat_15\",\"cat_16\",\"cat_17\",\"cat_18\",\"cat_19\",\"cat_20\",\"cat_21\",\"cat_22\",\"cat_23\",\"cat_24\",\"cat_25\",\"category\"]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (10, 30))\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "for x in range (20,26):\n",
    "    index = x%10\n",
    "    ax=plt.subplot(5,2,index+1) \n",
    "    plt.boxplot(df[cols[x]])\n",
    "    ax.set_title(cols[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFixOutliers = df\n",
    "\n",
    "outlierframe =['cat_9','cat_10','cat_11','cat_12','cat_12','cat_13','cat_17','cat_18','cat_19','cat_22','cat_24','cat_25']\n",
    "for cols in outlierframe :\n",
    "    Q1 = dfFixOutliers[cols].quantile(0.25)\n",
    "    Q3 = dfFixOutliers[cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1     \n",
    "\n",
    "    filter = (dfFixOutliers[cols] >= Q1 - 1.5 * IQR) & (dfFixOutliers[cols] <= Q3 + 1.5 *IQR)\n",
    "    dfFixOutliers=dfFixOutliers.loc[filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x2160 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (10, 30))\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "# for x in range (0,10): \n",
    "for x in range (10,12): \n",
    "    index = x%10\n",
    "    ax=plt.subplot(5,2,index+1)\n",
    "    plt.boxplot(dfFixOutliers[outlierframe[x]])\n",
    "    ax.set_title(outlierframe[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "      <th>cat_8</th>\n",
       "      <th>cat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_17</th>\n",
       "      <th>cat_18</th>\n",
       "      <th>cat_19</th>\n",
       "      <th>cat_20</th>\n",
       "      <th>cat_21</th>\n",
       "      <th>cat_22</th>\n",
       "      <th>cat_23</th>\n",
       "      <th>cat_24</th>\n",
       "      <th>cat_25</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.980142</td>\n",
       "      <td>1.557762</td>\n",
       "      <td>-1.134152</td>\n",
       "      <td>-1.482144</td>\n",
       "      <td>-1.273575</td>\n",
       "      <td>-1.194822</td>\n",
       "      <td>-0.996246</td>\n",
       "      <td>-0.673919</td>\n",
       "      <td>-0.730094</td>\n",
       "      <td>-0.576136</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.558060</td>\n",
       "      <td>-0.601064</td>\n",
       "      <td>-0.431706</td>\n",
       "      <td>-0.982956</td>\n",
       "      <td>-1.157560</td>\n",
       "      <td>-1.799805</td>\n",
       "      <td>1.076145</td>\n",
       "      <td>0.039481</td>\n",
       "      <td>0.679935</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.248937</td>\n",
       "      <td>1.522809</td>\n",
       "      <td>-1.345525</td>\n",
       "      <td>-1.393379</td>\n",
       "      <td>-1.349129</td>\n",
       "      <td>-1.095365</td>\n",
       "      <td>-1.055516</td>\n",
       "      <td>-0.800330</td>\n",
       "      <td>-0.770404</td>\n",
       "      <td>-0.613393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.824485</td>\n",
       "      <td>-0.690527</td>\n",
       "      <td>-0.640220</td>\n",
       "      <td>-1.080771</td>\n",
       "      <td>-1.363526</td>\n",
       "      <td>-1.673845</td>\n",
       "      <td>1.192847</td>\n",
       "      <td>0.032540</td>\n",
       "      <td>0.817749</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.038764</td>\n",
       "      <td>0.905593</td>\n",
       "      <td>0.663269</td>\n",
       "      <td>-0.361342</td>\n",
       "      <td>-0.399633</td>\n",
       "      <td>-0.575981</td>\n",
       "      <td>-0.617926</td>\n",
       "      <td>-0.857086</td>\n",
       "      <td>-0.984167</td>\n",
       "      <td>-1.182897</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.147469</td>\n",
       "      <td>-0.817265</td>\n",
       "      <td>-0.777244</td>\n",
       "      <td>-0.948433</td>\n",
       "      <td>-1.104407</td>\n",
       "      <td>-0.769732</td>\n",
       "      <td>1.149554</td>\n",
       "      <td>0.271705</td>\n",
       "      <td>1.256284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.913184</td>\n",
       "      <td>0.745321</td>\n",
       "      <td>0.730728</td>\n",
       "      <td>-0.205116</td>\n",
       "      <td>-0.293632</td>\n",
       "      <td>-0.553879</td>\n",
       "      <td>-0.446421</td>\n",
       "      <td>-0.684238</td>\n",
       "      <td>-0.864460</td>\n",
       "      <td>-0.963612</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.032862</td>\n",
       "      <td>-0.621939</td>\n",
       "      <td>-0.670008</td>\n",
       "      <td>-0.642041</td>\n",
       "      <td>-0.908407</td>\n",
       "      <td>-0.873299</td>\n",
       "      <td>1.079910</td>\n",
       "      <td>0.213649</td>\n",
       "      <td>1.133674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.603234</td>\n",
       "      <td>0.383004</td>\n",
       "      <td>-2.483341</td>\n",
       "      <td>-1.972125</td>\n",
       "      <td>-1.574663</td>\n",
       "      <td>-1.377773</td>\n",
       "      <td>-1.382133</td>\n",
       "      <td>-1.049283</td>\n",
       "      <td>-0.792391</td>\n",
       "      <td>-0.362173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.433034</td>\n",
       "      <td>-1.024520</td>\n",
       "      <td>-0.942566</td>\n",
       "      <td>-0.735541</td>\n",
       "      <td>-0.400136</td>\n",
       "      <td>0.624224</td>\n",
       "      <td>-0.278157</td>\n",
       "      <td>0.503928</td>\n",
       "      <td>-0.055398</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3243</th>\n",
       "      <td>-0.114973</td>\n",
       "      <td>0.770044</td>\n",
       "      <td>-0.506029</td>\n",
       "      <td>-0.414601</td>\n",
       "      <td>-0.551868</td>\n",
       "      <td>-0.616500</td>\n",
       "      <td>-0.728900</td>\n",
       "      <td>-0.808070</td>\n",
       "      <td>-0.901105</td>\n",
       "      <td>-0.924225</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.663737</td>\n",
       "      <td>-0.902255</td>\n",
       "      <td>-0.716179</td>\n",
       "      <td>-0.876510</td>\n",
       "      <td>-0.278882</td>\n",
       "      <td>0.923729</td>\n",
       "      <td>0.838037</td>\n",
       "      <td>1.911783</td>\n",
       "      <td>0.786115</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3244</th>\n",
       "      <td>-0.046037</td>\n",
       "      <td>0.910708</td>\n",
       "      <td>0.170065</td>\n",
       "      <td>-0.154225</td>\n",
       "      <td>-0.506761</td>\n",
       "      <td>-0.741742</td>\n",
       "      <td>-0.836090</td>\n",
       "      <td>-0.920292</td>\n",
       "      <td>-0.925535</td>\n",
       "      <td>-0.960418</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.802159</td>\n",
       "      <td>-0.841122</td>\n",
       "      <td>-0.826394</td>\n",
       "      <td>-0.585941</td>\n",
       "      <td>-0.212441</td>\n",
       "      <td>1.575922</td>\n",
       "      <td>1.038500</td>\n",
       "      <td>1.403794</td>\n",
       "      <td>0.785380</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3245</th>\n",
       "      <td>-0.769594</td>\n",
       "      <td>-0.278543</td>\n",
       "      <td>-0.798353</td>\n",
       "      <td>-0.383829</td>\n",
       "      <td>-0.144780</td>\n",
       "      <td>-0.238319</td>\n",
       "      <td>-0.134937</td>\n",
       "      <td>-0.045733</td>\n",
       "      <td>0.043118</td>\n",
       "      <td>0.031690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117677</td>\n",
       "      <td>-0.358024</td>\n",
       "      <td>-0.239575</td>\n",
       "      <td>-0.102619</td>\n",
       "      <td>-0.117763</td>\n",
       "      <td>0.002822</td>\n",
       "      <td>-0.263098</td>\n",
       "      <td>-0.129638</td>\n",
       "      <td>0.175027</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246</th>\n",
       "      <td>-0.145967</td>\n",
       "      <td>0.429040</td>\n",
       "      <td>-0.612465</td>\n",
       "      <td>-0.461942</td>\n",
       "      <td>-0.613890</td>\n",
       "      <td>-0.573525</td>\n",
       "      <td>-0.543523</td>\n",
       "      <td>-0.676499</td>\n",
       "      <td>-0.610387</td>\n",
       "      <td>-0.698553</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.628015</td>\n",
       "      <td>-0.875416</td>\n",
       "      <td>-0.877033</td>\n",
       "      <td>-0.591695</td>\n",
       "      <td>-0.278882</td>\n",
       "      <td>0.680207</td>\n",
       "      <td>0.573576</td>\n",
       "      <td>1.216375</td>\n",
       "      <td>0.544737</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>-0.423313</td>\n",
       "      <td>0.163057</td>\n",
       "      <td>-0.434072</td>\n",
       "      <td>-0.334121</td>\n",
       "      <td>-0.342122</td>\n",
       "      <td>-0.519499</td>\n",
       "      <td>-0.194207</td>\n",
       "      <td>-0.250829</td>\n",
       "      <td>-0.135222</td>\n",
       "      <td>-0.124791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279728</td>\n",
       "      <td>-0.709910</td>\n",
       "      <td>-0.558304</td>\n",
       "      <td>-0.676564</td>\n",
       "      <td>-0.511424</td>\n",
       "      <td>-0.456232</td>\n",
       "      <td>-0.016519</td>\n",
       "      <td>0.673048</td>\n",
       "      <td>0.231019</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3057 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cat_0     cat_1     cat_2     cat_3     cat_4     cat_5     cat_6  \\\n",
       "0    -0.980142  1.557762 -1.134152 -1.482144 -1.273575 -1.194822 -0.996246   \n",
       "1    -1.248937  1.522809 -1.345525 -1.393379 -1.349129 -1.095365 -1.055516   \n",
       "2     1.038764  0.905593  0.663269 -0.361342 -0.399633 -0.575981 -0.617926   \n",
       "3     0.913184  0.745321  0.730728 -0.205116 -0.293632 -0.553879 -0.446421   \n",
       "4    -1.603234  0.383004 -2.483341 -1.972125 -1.574663 -1.377773 -1.382133   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3243 -0.114973  0.770044 -0.506029 -0.414601 -0.551868 -0.616500 -0.728900   \n",
       "3244 -0.046037  0.910708  0.170065 -0.154225 -0.506761 -0.741742 -0.836090   \n",
       "3245 -0.769594 -0.278543 -0.798353 -0.383829 -0.144780 -0.238319 -0.134937   \n",
       "3246 -0.145967  0.429040 -0.612465 -0.461942 -0.613890 -0.573525 -0.543523   \n",
       "3247 -0.423313  0.163057 -0.434072 -0.334121 -0.342122 -0.519499 -0.194207   \n",
       "\n",
       "         cat_7     cat_8     cat_9  ...    cat_17    cat_18    cat_19  \\\n",
       "0    -0.673919 -0.730094 -0.576136  ... -0.558060 -0.601064 -0.431706   \n",
       "1    -0.800330 -0.770404 -0.613393  ... -0.824485 -0.690527 -0.640220   \n",
       "2    -0.857086 -0.984167 -1.182897  ... -1.147469 -0.817265 -0.777244   \n",
       "3    -0.684238 -0.864460 -0.963612  ... -1.032862 -0.621939 -0.670008   \n",
       "4    -1.049283 -0.792391 -0.362173  ... -0.433034 -1.024520 -0.942566   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3243 -0.808070 -0.901105 -0.924225  ... -0.663737 -0.902255 -0.716179   \n",
       "3244 -0.920292 -0.925535 -0.960418  ... -0.802159 -0.841122 -0.826394   \n",
       "3245 -0.045733  0.043118  0.031690  ...  0.117677 -0.358024 -0.239575   \n",
       "3246 -0.676499 -0.610387 -0.698553  ... -0.628015 -0.875416 -0.877033   \n",
       "3247 -0.250829 -0.135222 -0.124791  ... -0.279728 -0.709910 -0.558304   \n",
       "\n",
       "        cat_20    cat_21    cat_22    cat_23    cat_24    cat_25  category  \n",
       "0    -0.982956 -1.157560 -1.799805  1.076145  0.039481  0.679935         0  \n",
       "1    -1.080771 -1.363526 -1.673845  1.192847  0.032540  0.817749         0  \n",
       "2    -0.948433 -1.104407 -0.769732  1.149554  0.271705  1.256284         0  \n",
       "3    -0.642041 -0.908407 -0.873299  1.079910  0.213649  1.133674         0  \n",
       "4    -0.735541 -0.400136  0.624224 -0.278157  0.503928 -0.055398         0  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3243 -0.876510 -0.278882  0.923729  0.838037  1.911783  0.786115         7  \n",
       "3244 -0.585941 -0.212441  1.575922  1.038500  1.403794  0.785380         7  \n",
       "3245 -0.102619 -0.117763  0.002822 -0.263098 -0.129638  0.175027         7  \n",
       "3246 -0.591695 -0.278882  0.680207  0.573576  1.216375  0.544737         7  \n",
       "3247 -0.676564 -0.511424 -0.456232 -0.016519  0.673048  0.231019         7  \n",
       "\n",
       "[3057 rows x 27 columns]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "d_pre = dfFixOutliers\n",
    "variable_cat2 = [\"cat_0\",\"cat_1\",\"cat_2\",\"cat_3\",\"cat_4\",\"cat_5\",\"cat_6\",\"cat_7\",\"cat_8\",\"cat_9\",\"cat_10\",\"cat_11\",\"cat_12\",\"cat_13\",\"cat_14\",\"cat_15\",\"cat_16\",\"cat_17\",\"cat_18\",\"cat_19\",\"cat_20\",\"cat_21\",\"cat_22\",\"cat_23\",\"cat_24\",\"cat_25\"]\n",
    "d_pre[variable_cat2] = StandardScaler().fit_transform(d_pre[variable_cat2])\n",
    "d_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X Y Partition ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "3243    7\n",
       "3244    7\n",
       "3245    7\n",
       "3246    7\n",
       "3247    7\n",
       "Name: category, Length: 3057, dtype: int64"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_norm=d_pre.drop(['category'],axis=1)\n",
    "\n",
    "y_norm=d_pre['category']\n",
    "y_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting DataSet ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Training: (3057, 26)\n",
      "Input Training: (2445, 26)\n",
      "Input Test: (612, 26)\n",
      "Output Training: (2445,)\n",
      "Output Test: (612,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_norm, x_test_norm, y_train_norm, y_test_norm = train_test_split(x_norm, y_norm,train_size=0.8,random_state=1)\n",
    "print(\"Input Training:\",x_norm.shape)\n",
    "print(\"Input Training:\",x_train_norm.shape)\n",
    "print(\"Input Test:\",x_test_norm.shape)\n",
    "print(\"Output Training:\",y_train_norm.shape)\n",
    "print(\"Output Test:\",y_test_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13894881, 0.13554041, 0.14451803, ..., 0.1257676 , 0.14052584,\n",
       "        0.1420485 ],\n",
       "       [0.13970541, 0.15870533, 0.13896371, ..., 0.12100018, 0.14606553,\n",
       "        0.14626277],\n",
       "       [0.1866264 , 0.09315282, 0.14212265, ..., 0.17632458, 0.13669617,\n",
       "        0.14564282],\n",
       "       ...,\n",
       "       [0.12406234, 0.11590304, 0.09810103, ..., 0.14030798, 0.1387998 ,\n",
       "        0.14043417],\n",
       "       [0.12953965, 0.10415269, 0.06502889, ..., 0.13878207, 0.13954496,\n",
       "        0.13969166],\n",
       "       [0.14063617, 0.11885668, 0.00667858, ..., 0.14991912, 0.14177795,\n",
       "        0.14387573]])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_mod = AdaBoostClassifier(n_estimators=300, learning_rate=0.2)\n",
    "\n",
    "ada_mod.fit(x_train_norm, y_train_norm)\n",
    "\n",
    "x_adaptiveresult_norm = ada_mod.predict_proba(x_test_norm)\n",
    "x_adaptiveresult_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.138949</td>\n",
       "      <td>0.135540</td>\n",
       "      <td>0.144518</td>\n",
       "      <td>0.065245</td>\n",
       "      <td>0.107406</td>\n",
       "      <td>0.125768</td>\n",
       "      <td>0.140526</td>\n",
       "      <td>0.142049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.139705</td>\n",
       "      <td>0.158705</td>\n",
       "      <td>0.138964</td>\n",
       "      <td>0.138982</td>\n",
       "      <td>0.010315</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>0.146066</td>\n",
       "      <td>0.146263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.186626</td>\n",
       "      <td>0.093153</td>\n",
       "      <td>0.142123</td>\n",
       "      <td>0.057279</td>\n",
       "      <td>0.062155</td>\n",
       "      <td>0.176325</td>\n",
       "      <td>0.136696</td>\n",
       "      <td>0.145643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.138909</td>\n",
       "      <td>0.142066</td>\n",
       "      <td>0.142803</td>\n",
       "      <td>0.064979</td>\n",
       "      <td>0.105890</td>\n",
       "      <td>0.126273</td>\n",
       "      <td>0.137946</td>\n",
       "      <td>0.141135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.180807</td>\n",
       "      <td>0.089096</td>\n",
       "      <td>0.143604</td>\n",
       "      <td>0.054959</td>\n",
       "      <td>0.081390</td>\n",
       "      <td>0.171820</td>\n",
       "      <td>0.135867</td>\n",
       "      <td>0.142457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>0.136943</td>\n",
       "      <td>0.131604</td>\n",
       "      <td>0.007320</td>\n",
       "      <td>0.144416</td>\n",
       "      <td>0.152650</td>\n",
       "      <td>0.148261</td>\n",
       "      <td>0.137977</td>\n",
       "      <td>0.140828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>0.140794</td>\n",
       "      <td>0.119536</td>\n",
       "      <td>0.006724</td>\n",
       "      <td>0.145989</td>\n",
       "      <td>0.153062</td>\n",
       "      <td>0.150111</td>\n",
       "      <td>0.140604</td>\n",
       "      <td>0.143179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>0.124062</td>\n",
       "      <td>0.115903</td>\n",
       "      <td>0.098101</td>\n",
       "      <td>0.099789</td>\n",
       "      <td>0.142603</td>\n",
       "      <td>0.140308</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>0.140434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>0.129540</td>\n",
       "      <td>0.104153</td>\n",
       "      <td>0.065029</td>\n",
       "      <td>0.140609</td>\n",
       "      <td>0.142652</td>\n",
       "      <td>0.138782</td>\n",
       "      <td>0.139545</td>\n",
       "      <td>0.139692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>0.140636</td>\n",
       "      <td>0.118857</td>\n",
       "      <td>0.006679</td>\n",
       "      <td>0.145768</td>\n",
       "      <td>0.152488</td>\n",
       "      <td>0.149919</td>\n",
       "      <td>0.141778</td>\n",
       "      <td>0.143876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat_0     cat_1     cat_2     cat_3     cat_4     cat_5     cat_6  \\\n",
       "0    0.138949  0.135540  0.144518  0.065245  0.107406  0.125768  0.140526   \n",
       "1    0.139705  0.158705  0.138964  0.138982  0.010315  0.121000  0.146066   \n",
       "2    0.186626  0.093153  0.142123  0.057279  0.062155  0.176325  0.136696   \n",
       "3    0.138909  0.142066  0.142803  0.064979  0.105890  0.126273  0.137946   \n",
       "4    0.180807  0.089096  0.143604  0.054959  0.081390  0.171820  0.135867   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "607  0.136943  0.131604  0.007320  0.144416  0.152650  0.148261  0.137977   \n",
       "608  0.140794  0.119536  0.006724  0.145989  0.153062  0.150111  0.140604   \n",
       "609  0.124062  0.115903  0.098101  0.099789  0.142603  0.140308  0.138800   \n",
       "610  0.129540  0.104153  0.065029  0.140609  0.142652  0.138782  0.139545   \n",
       "611  0.140636  0.118857  0.006679  0.145768  0.152488  0.149919  0.141778   \n",
       "\n",
       "        cat_7  \n",
       "0    0.142049  \n",
       "1    0.146263  \n",
       "2    0.145643  \n",
       "3    0.141135  \n",
       "4    0.142457  \n",
       "..        ...  \n",
       "607  0.140828  \n",
       "608  0.143179  \n",
       "609  0.140434  \n",
       "610  0.139692  \n",
       "611  0.143876  \n",
       "\n",
       "[612 rows x 8 columns]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_adaptivetest = np.array(ada_mod.predict_proba(x_test_norm))\n",
    "dataset_test_norm = pd.DataFrame(x_adaptivetest, columns=x_norm.columns[:8])\n",
    "dataset_test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.139984</td>\n",
       "      <td>0.139079</td>\n",
       "      <td>0.007996</td>\n",
       "      <td>0.133946</td>\n",
       "      <td>0.156154</td>\n",
       "      <td>0.143755</td>\n",
       "      <td>0.137996</td>\n",
       "      <td>0.141089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.128749</td>\n",
       "      <td>0.104035</td>\n",
       "      <td>0.066691</td>\n",
       "      <td>0.140033</td>\n",
       "      <td>0.142260</td>\n",
       "      <td>0.138970</td>\n",
       "      <td>0.139720</td>\n",
       "      <td>0.139544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.140292</td>\n",
       "      <td>0.157408</td>\n",
       "      <td>0.123776</td>\n",
       "      <td>0.145038</td>\n",
       "      <td>0.010716</td>\n",
       "      <td>0.123567</td>\n",
       "      <td>0.149583</td>\n",
       "      <td>0.149620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.138251</td>\n",
       "      <td>0.140309</td>\n",
       "      <td>0.008849</td>\n",
       "      <td>0.134354</td>\n",
       "      <td>0.155794</td>\n",
       "      <td>0.144642</td>\n",
       "      <td>0.137699</td>\n",
       "      <td>0.140102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.138173</td>\n",
       "      <td>0.130556</td>\n",
       "      <td>0.007671</td>\n",
       "      <td>0.142072</td>\n",
       "      <td>0.154883</td>\n",
       "      <td>0.146613</td>\n",
       "      <td>0.139057</td>\n",
       "      <td>0.140975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>0.137072</td>\n",
       "      <td>0.127302</td>\n",
       "      <td>0.007566</td>\n",
       "      <td>0.144543</td>\n",
       "      <td>0.153423</td>\n",
       "      <td>0.148356</td>\n",
       "      <td>0.140132</td>\n",
       "      <td>0.141606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>0.137689</td>\n",
       "      <td>0.129132</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.142704</td>\n",
       "      <td>0.155394</td>\n",
       "      <td>0.146922</td>\n",
       "      <td>0.139421</td>\n",
       "      <td>0.141056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>0.143431</td>\n",
       "      <td>0.149271</td>\n",
       "      <td>0.009686</td>\n",
       "      <td>0.124668</td>\n",
       "      <td>0.147560</td>\n",
       "      <td>0.139515</td>\n",
       "      <td>0.141249</td>\n",
       "      <td>0.144619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>0.136255</td>\n",
       "      <td>0.132062</td>\n",
       "      <td>0.008559</td>\n",
       "      <td>0.142017</td>\n",
       "      <td>0.154552</td>\n",
       "      <td>0.146966</td>\n",
       "      <td>0.139180</td>\n",
       "      <td>0.140409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>0.139784</td>\n",
       "      <td>0.118020</td>\n",
       "      <td>0.006522</td>\n",
       "      <td>0.147130</td>\n",
       "      <td>0.154383</td>\n",
       "      <td>0.149867</td>\n",
       "      <td>0.141058</td>\n",
       "      <td>0.143236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2445 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cat_0     cat_1     cat_2     cat_3     cat_4     cat_5     cat_6  \\\n",
       "0     0.139984  0.139079  0.007996  0.133946  0.156154  0.143755  0.137996   \n",
       "1     0.128749  0.104035  0.066691  0.140033  0.142260  0.138970  0.139720   \n",
       "2     0.140292  0.157408  0.123776  0.145038  0.010716  0.123567  0.149583   \n",
       "3     0.138251  0.140309  0.008849  0.134354  0.155794  0.144642  0.137699   \n",
       "4     0.138173  0.130556  0.007671  0.142072  0.154883  0.146613  0.139057   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2440  0.137072  0.127302  0.007566  0.144543  0.153423  0.148356  0.140132   \n",
       "2441  0.137689  0.129132  0.007682  0.142704  0.155394  0.146922  0.139421   \n",
       "2442  0.143431  0.149271  0.009686  0.124668  0.147560  0.139515  0.141249   \n",
       "2443  0.136255  0.132062  0.008559  0.142017  0.154552  0.146966  0.139180   \n",
       "2444  0.139784  0.118020  0.006522  0.147130  0.154383  0.149867  0.141058   \n",
       "\n",
       "         cat_7  \n",
       "0     0.141089  \n",
       "1     0.139544  \n",
       "2     0.149620  \n",
       "3     0.140102  \n",
       "4     0.140975  \n",
       "...        ...  \n",
       "2440  0.141606  \n",
       "2441  0.141056  \n",
       "2442  0.144619  \n",
       "2443  0.140409  \n",
       "2444  0.143236  \n",
       "\n",
       "[2445 rows x 8 columns]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_adaptivetrain = np.array(ada_mod.predict_proba(x_train_norm))\n",
    "dataset_train_norm = pd.DataFrame(x_adaptivetrain, columns=x_norm.columns[:8])\n",
    "dataset_train_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.30      0.36        84\n",
      "           1       0.62      0.37      0.46        41\n",
      "           2       0.28      0.76      0.41        46\n",
      "           3       0.13      0.23      0.17        40\n",
      "           4       0.17      0.51      0.26        68\n",
      "           5       0.60      0.21      0.31       117\n",
      "           6       0.36      0.07      0.12       115\n",
      "           7       0.21      0.15      0.17       101\n",
      "\n",
      "    accuracy                           0.27       612\n",
      "   macro avg       0.36      0.32      0.28       612\n",
      "weighted avg       0.37      0.27      0.26       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = GaussianNB()\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(dataset_train_norm, y_train_norm)\n",
    "y_pred = nbtrain.predict(dataset_test_norm)\n",
    "print(classification_report(y_test_norm, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.40      0.55        84\n",
      "           1       0.68      0.56      0.61        41\n",
      "           2       0.32      0.76      0.45        46\n",
      "           3       0.00      0.00      0.00        40\n",
      "           4       0.22      0.16      0.19        68\n",
      "           5       0.59      0.29      0.39       117\n",
      "           6       0.26      0.13      0.17       115\n",
      "           7       0.21      0.55      0.31       101\n",
      "\n",
      "    accuracy                           0.34       612\n",
      "   macro avg       0.39      0.36      0.33       612\n",
      "weighted avg       0.41      0.34      0.33       612\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = svm.SVC()\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(dataset_train_norm, y_train_norm)\n",
    "y_pred = nbtrain.predict(dataset_test_norm)\n",
    "print(classification_report(y_test_norm, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.44      0.45        84\n",
      "           1       0.55      0.59      0.56        41\n",
      "           2       0.37      0.48      0.42        46\n",
      "           3       0.14      0.15      0.14        40\n",
      "           4       0.37      0.31      0.34        68\n",
      "           5       0.41      0.39      0.40       117\n",
      "           6       0.40      0.41      0.41       115\n",
      "           7       0.32      0.33      0.33       101\n",
      "\n",
      "    accuracy                           0.39       612\n",
      "   macro avg       0.38      0.39      0.38       612\n",
      "weighted avg       0.39      0.39      0.39       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = DecisionTreeClassifier(random_state=0)\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(dataset_train_norm, y_train_norm)\n",
    "y_pred = nbtrain.predict(dataset_test_norm)\n",
    "print(classification_report(y_test_norm, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 neighbors accuracy : 0.4199346405228758\n",
      "2 neighbors accuracy : 0.39869281045751637\n",
      "3 neighbors accuracy : 0.4264705882352941\n",
      "4 neighbors accuracy : 0.43137254901960786\n",
      "5 neighbors accuracy : 0.44281045751633985\n",
      "6 neighbors accuracy : 0.4493464052287582\n",
      "7 neighbors accuracy : 0.4444444444444444\n",
      "8 neighbors accuracy : 0.4477124183006536\n",
      "9 neighbors accuracy : 0.42320261437908496\n",
      "10 neighbors accuracy : 0.43137254901960786\n",
      "11 neighbors accuracy : 0.4297385620915033\n",
      "12 neighbors accuracy : 0.4297385620915033\n",
      "13 neighbors accuracy : 0.4526143790849673\n",
      "14 neighbors accuracy : 0.4411764705882353\n",
      "15 neighbors accuracy : 0.434640522875817\n",
      "16 neighbors accuracy : 0.43790849673202614\n",
      "17 neighbors accuracy : 0.4215686274509804\n",
      "18 neighbors accuracy : 0.44281045751633985\n",
      "19 neighbors accuracy : 0.4444444444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score  \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "for i in range (1,20) :\n",
    "    # Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "    modelnb = KNeighborsClassifier(n_neighbors=i)\n",
    "    # Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "    nbtrain = modelnb.fit(dataset_train_norm, y_train_norm)\n",
    "    y_pred = nbtrain.predict(dataset_test_norm)\n",
    "    print(str(i) + \" neighbors accuracy : \" + str(accuracy_score(y_test_norm, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 neighbors accuracy : 0.4493464052287582\n"
     ]
    }
   ],
   "source": [
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = KNeighborsClassifier(n_neighbors=6)\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(dataset_train_norm, y_train_norm)\n",
    "y_pred = nbtrain.predict(dataset_test_norm)\n",
    "print(str(i) + \" neighbors accuracy : \" + str(accuracy_score(y_test_norm, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: 0.5491 - accuracy: 0.0810\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -0.3235 - accuracy: 0.0806\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -2.5270 - accuracy: 0.0806\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -4.9322 - accuracy: 0.0806\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -6.7917 - accuracy: 0.0806\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -8.3230 - accuracy: 0.0806\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -9.6980 - accuracy: 0.0806\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -10.9776 - accuracy: 0.0806\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -12.2096 - accuracy: 0.0806\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -13.4026 - accuracy: 0.0806\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -14.5694 - accuracy: 0.0806\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -15.7165 - accuracy: 0.0806\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -16.8506 - accuracy: 0.0806\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -17.9709 - accuracy: 0.0806\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -19.0850 - accuracy: 0.0806\n",
      "Epoch 16/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -20.1889 - accuracy: 0.0806\n",
      "Epoch 17/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -21.2897 - accuracy: 0.0806\n",
      "Epoch 18/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -22.3792 - accuracy: 0.0806\n",
      "Epoch 19/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -23.4701 - accuracy: 0.0806\n",
      "Epoch 20/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -24.5526 - accuracy: 0.0806\n",
      "Epoch 21/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -25.6343 - accuracy: 0.0806\n",
      "Epoch 22/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -26.7097 - accuracy: 0.0806\n",
      "Epoch 23/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -27.7854 - accuracy: 0.0806\n",
      "Epoch 24/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -28.8591 - accuracy: 0.0806\n",
      "Epoch 25/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -29.9263 - accuracy: 0.0806\n",
      "Epoch 26/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -30.9948 - accuracy: 0.0806\n",
      "Epoch 27/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -32.0629 - accuracy: 0.0806\n",
      "Epoch 28/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -33.1252 - accuracy: 0.0806\n",
      "Epoch 29/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -34.1863 - accuracy: 0.0806\n",
      "Epoch 30/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -35.2493 - accuracy: 0.0806\n",
      "Epoch 31/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -36.3095 - accuracy: 0.0806\n",
      "Epoch 32/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -37.3678 - accuracy: 0.0806\n",
      "Epoch 33/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -38.4259 - accuracy: 0.0806\n",
      "Epoch 34/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -39.4799 - accuracy: 0.0806\n",
      "Epoch 35/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -40.5405 - accuracy: 0.0806\n",
      "Epoch 36/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -41.5917 - accuracy: 0.0806\n",
      "Epoch 37/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -42.6475 - accuracy: 0.0806\n",
      "Epoch 38/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -43.7021 - accuracy: 0.0806\n",
      "Epoch 39/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -44.7521 - accuracy: 0.0806\n",
      "Epoch 40/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -45.8058 - accuracy: 0.0806\n",
      "Epoch 41/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -46.8570 - accuracy: 0.0806\n",
      "Epoch 42/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -47.9094 - accuracy: 0.0806\n",
      "Epoch 43/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -48.9600 - accuracy: 0.0806\n",
      "Epoch 44/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -50.0090 - accuracy: 0.0806\n",
      "Epoch 45/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -51.0594 - accuracy: 0.0806\n",
      "Epoch 46/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -52.1104 - accuracy: 0.0806\n",
      "Epoch 47/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -53.1566 - accuracy: 0.0806\n",
      "Epoch 48/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -54.2058 - accuracy: 0.0806\n",
      "Epoch 49/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -55.2562 - accuracy: 0.0806\n",
      "Epoch 50/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -56.3014 - accuracy: 0.0806\n",
      "Epoch 51/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -57.3497 - accuracy: 0.0806\n",
      "Epoch 52/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -58.3962 - accuracy: 0.0806\n",
      "Epoch 53/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -59.4438 - accuracy: 0.0806\n",
      "Epoch 54/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -60.4890 - accuracy: 0.0806\n",
      "Epoch 55/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -61.5347 - accuracy: 0.0806\n",
      "Epoch 56/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -62.5814 - accuracy: 0.0806\n",
      "Epoch 57/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -63.6250 - accuracy: 0.0806\n",
      "Epoch 58/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -64.6722 - accuracy: 0.0806\n",
      "Epoch 59/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -65.7162 - accuracy: 0.0806\n",
      "Epoch 60/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -66.7618 - accuracy: 0.0806\n",
      "Epoch 61/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -67.8075 - accuracy: 0.0806\n",
      "Epoch 62/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -68.8483 - accuracy: 0.0806\n",
      "Epoch 63/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -69.8977 - accuracy: 0.0806\n",
      "Epoch 64/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -70.9389 - accuracy: 0.0806\n",
      "Epoch 65/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -71.9842 - accuracy: 0.0806\n",
      "Epoch 66/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -73.0294 - accuracy: 0.0806\n",
      "Epoch 67/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -74.0727 - accuracy: 0.0806\n",
      "Epoch 68/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -75.1168 - accuracy: 0.0806\n",
      "Epoch 69/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -76.1589 - accuracy: 0.0806\n",
      "Epoch 70/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -77.2041 - accuracy: 0.0806\n",
      "Epoch 71/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -78.2449 - accuracy: 0.0806\n",
      "Epoch 72/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -79.2898 - accuracy: 0.0806\n",
      "Epoch 73/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -80.3345 - accuracy: 0.0806\n",
      "Epoch 74/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -81.3735 - accuracy: 0.0806\n",
      "Epoch 75/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -82.4210 - accuracy: 0.0806\n",
      "Epoch 76/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -83.4618 - accuracy: 0.0806\n",
      "Epoch 77/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -84.5053 - accuracy: 0.0806\n",
      "Epoch 78/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -85.5477 - accuracy: 0.0806\n",
      "Epoch 79/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -86.5902 - accuracy: 0.0806\n",
      "Epoch 80/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -87.6337 - accuracy: 0.0806\n",
      "Epoch 81/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -88.6749 - accuracy: 0.0806\n",
      "Epoch 82/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -89.7184 - accuracy: 0.0806\n",
      "Epoch 83/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -90.7602 - accuracy: 0.0806\n",
      "Epoch 84/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -91.8045 - accuracy: 0.0806\n",
      "Epoch 85/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -92.8442 - accuracy: 0.0806\n",
      "Epoch 86/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -93.8866 - accuracy: 0.0806\n",
      "Epoch 87/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -94.9332 - accuracy: 0.0806\n",
      "Epoch 88/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -95.9720 - accuracy: 0.0806\n",
      "Epoch 89/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -97.0165 - accuracy: 0.0806\n",
      "Epoch 90/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -98.0586 - accuracy: 0.0806\n",
      "Epoch 91/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -99.0991 - accuracy: 0.0806\n",
      "Epoch 92/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -100.1432 - accuracy: 0.0806\n",
      "Epoch 93/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -101.1864 - accuracy: 0.0806\n",
      "Epoch 94/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -102.2272 - accuracy: 0.0806\n",
      "Epoch 95/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -103.2709 - accuracy: 0.0806\n",
      "Epoch 96/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -104.3116 - accuracy: 0.0806\n",
      "Epoch 97/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -105.3536 - accuracy: 0.0806\n",
      "Epoch 98/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -106.3939 - accuracy: 0.0806\n",
      "Epoch 99/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -107.4395 - accuracy: 0.0806\n",
      "Epoch 100/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -108.4809 - accuracy: 0.0806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a405e2e820>"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units=10,kernel_initializer='uniform' , activation='tanh' , input_dim=8))\n",
    "classifier.add(Dense(units=6,kernel_initializer='uniform' , activation='tanh'))\n",
    "classifier.add(Dense(units=1 , kernel_initializer='uniform' , activation='sigmoid'))\n",
    "classifier.compile(optimizer='adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "classifier.fit(dataset_train_norm, y_train_norm, batch_size = 50, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        84\n",
      "           1       0.07      1.00      0.13        41\n",
      "           2       0.00      0.00      0.00        46\n",
      "           3       0.00      0.00      0.00        40\n",
      "           4       0.00      0.00      0.00        68\n",
      "           5       0.00      0.00      0.00       117\n",
      "           6       0.00      0.00      0.00       115\n",
      "           7       0.00      0.00      0.00       101\n",
      "\n",
      "    accuracy                           0.07       612\n",
      "   macro avg       0.01      0.12      0.02       612\n",
      "weighted avg       0.00      0.07      0.01       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = classifier.predict(dataset_test_norm)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "print(classification_report(y_test_norm, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adaboost + Normalisasi + Imbalanced Overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over Fiting ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After oversampling:  Counter({6: 457, 7: 457, 0: 457, 5: 457, 3: 457, 4: 457, 1: 457, 2: 457})\n",
      "Input Training: (3057, 26)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "# define oversampling strategy\n",
    "\n",
    "SMOTE = SMOTE()\n",
    "\n",
    "# fit and apply the transform\n",
    "X_train_SMOTE, y_train_SMOTE = SMOTE.fit_resample(x_train_norm, y_train_norm)\n",
    "\n",
    "# summarize class distribution\n",
    "print(\"After oversampling: \",Counter(y_train_SMOTE))\n",
    "print(\"Input Training:\",x_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14007112, 0.13661089, 0.1469025 , ..., 0.13095772, 0.14136333,\n",
       "        0.14255947],\n",
       "       [0.13810592, 0.15969913, 0.13686226, ..., 0.12883488, 0.14324963,\n",
       "        0.1417257 ],\n",
       "       [0.17435657, 0.11912419, 0.14468888, ..., 0.15940229, 0.14206805,\n",
       "        0.14906941],\n",
       "       ...,\n",
       "       [0.12058854, 0.11650908, 0.10203184, ..., 0.14242942, 0.14058166,\n",
       "        0.14157636],\n",
       "       [0.12505224, 0.10260685, 0.06375501, ..., 0.14007587, 0.14040742,\n",
       "        0.14037984],\n",
       "       [0.14952025, 0.04504042, 0.00182313, ..., 0.14497247, 0.14215005,\n",
       "        0.14399739]])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_mod = AdaBoostClassifier(n_estimators=300, learning_rate=0.2)\n",
    "\n",
    "ada_mod.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "\n",
    "x_adaptiveresult_smote = ada_mod.predict_proba(x_test_norm)\n",
    "x_adaptiveresult_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.140071</td>\n",
       "      <td>0.136611</td>\n",
       "      <td>0.146903</td>\n",
       "      <td>0.061254</td>\n",
       "      <td>0.100281</td>\n",
       "      <td>0.130958</td>\n",
       "      <td>0.141363</td>\n",
       "      <td>0.142559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.138106</td>\n",
       "      <td>0.159699</td>\n",
       "      <td>0.136862</td>\n",
       "      <td>0.142588</td>\n",
       "      <td>0.008935</td>\n",
       "      <td>0.128835</td>\n",
       "      <td>0.143250</td>\n",
       "      <td>0.141726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.174357</td>\n",
       "      <td>0.119124</td>\n",
       "      <td>0.144689</td>\n",
       "      <td>0.055369</td>\n",
       "      <td>0.055921</td>\n",
       "      <td>0.159402</td>\n",
       "      <td>0.142068</td>\n",
       "      <td>0.149069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.137818</td>\n",
       "      <td>0.143449</td>\n",
       "      <td>0.145527</td>\n",
       "      <td>0.060836</td>\n",
       "      <td>0.100522</td>\n",
       "      <td>0.131366</td>\n",
       "      <td>0.139046</td>\n",
       "      <td>0.141435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.170021</td>\n",
       "      <td>0.112541</td>\n",
       "      <td>0.150429</td>\n",
       "      <td>0.052444</td>\n",
       "      <td>0.065615</td>\n",
       "      <td>0.159120</td>\n",
       "      <td>0.142366</td>\n",
       "      <td>0.147464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>0.147518</td>\n",
       "      <td>0.050238</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>0.202854</td>\n",
       "      <td>0.169518</td>\n",
       "      <td>0.145154</td>\n",
       "      <td>0.140126</td>\n",
       "      <td>0.142547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>0.148852</td>\n",
       "      <td>0.044228</td>\n",
       "      <td>0.001879</td>\n",
       "      <td>0.203964</td>\n",
       "      <td>0.168509</td>\n",
       "      <td>0.145844</td>\n",
       "      <td>0.142507</td>\n",
       "      <td>0.144218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>0.120589</td>\n",
       "      <td>0.116509</td>\n",
       "      <td>0.102032</td>\n",
       "      <td>0.092224</td>\n",
       "      <td>0.144059</td>\n",
       "      <td>0.142429</td>\n",
       "      <td>0.140582</td>\n",
       "      <td>0.141576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>0.125052</td>\n",
       "      <td>0.102607</td>\n",
       "      <td>0.063755</td>\n",
       "      <td>0.143240</td>\n",
       "      <td>0.144483</td>\n",
       "      <td>0.140076</td>\n",
       "      <td>0.140407</td>\n",
       "      <td>0.140380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>0.149520</td>\n",
       "      <td>0.045040</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.203984</td>\n",
       "      <td>0.168512</td>\n",
       "      <td>0.144972</td>\n",
       "      <td>0.142150</td>\n",
       "      <td>0.143997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat_0     cat_1     cat_2     cat_3     cat_4     cat_5     cat_6  \\\n",
       "0    0.140071  0.136611  0.146903  0.061254  0.100281  0.130958  0.141363   \n",
       "1    0.138106  0.159699  0.136862  0.142588  0.008935  0.128835  0.143250   \n",
       "2    0.174357  0.119124  0.144689  0.055369  0.055921  0.159402  0.142068   \n",
       "3    0.137818  0.143449  0.145527  0.060836  0.100522  0.131366  0.139046   \n",
       "4    0.170021  0.112541  0.150429  0.052444  0.065615  0.159120  0.142366   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "607  0.147518  0.050238  0.002045  0.202854  0.169518  0.145154  0.140126   \n",
       "608  0.148852  0.044228  0.001879  0.203964  0.168509  0.145844  0.142507   \n",
       "609  0.120589  0.116509  0.102032  0.092224  0.144059  0.142429  0.140582   \n",
       "610  0.125052  0.102607  0.063755  0.143240  0.144483  0.140076  0.140407   \n",
       "611  0.149520  0.045040  0.001823  0.203984  0.168512  0.144972  0.142150   \n",
       "\n",
       "        cat_7  \n",
       "0    0.142559  \n",
       "1    0.141726  \n",
       "2    0.149069  \n",
       "3    0.141435  \n",
       "4    0.147464  \n",
       "..        ...  \n",
       "607  0.142547  \n",
       "608  0.144218  \n",
       "609  0.141576  \n",
       "610  0.140380  \n",
       "611  0.143997  \n",
       "\n",
       "[612 rows x 8 columns]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_adaptivetest_smote = np.array(ada_mod.predict_proba(x_test_norm))\n",
    "dataset_test_smote = pd.DataFrame(x_adaptivetest_smote, columns=x_norm.columns[:8])\n",
    "dataset_test_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.139960</td>\n",
       "      <td>0.139766</td>\n",
       "      <td>0.005966</td>\n",
       "      <td>0.133509</td>\n",
       "      <td>0.164651</td>\n",
       "      <td>0.140062</td>\n",
       "      <td>0.137625</td>\n",
       "      <td>0.138462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.125643</td>\n",
       "      <td>0.100861</td>\n",
       "      <td>0.063706</td>\n",
       "      <td>0.143727</td>\n",
       "      <td>0.144957</td>\n",
       "      <td>0.140027</td>\n",
       "      <td>0.140824</td>\n",
       "      <td>0.140255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.135338</td>\n",
       "      <td>0.159200</td>\n",
       "      <td>0.135089</td>\n",
       "      <td>0.144879</td>\n",
       "      <td>0.009109</td>\n",
       "      <td>0.130117</td>\n",
       "      <td>0.143994</td>\n",
       "      <td>0.142275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.138452</td>\n",
       "      <td>0.136893</td>\n",
       "      <td>0.006543</td>\n",
       "      <td>0.135634</td>\n",
       "      <td>0.165169</td>\n",
       "      <td>0.141831</td>\n",
       "      <td>0.137625</td>\n",
       "      <td>0.137854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.138100</td>\n",
       "      <td>0.132440</td>\n",
       "      <td>0.005766</td>\n",
       "      <td>0.140118</td>\n",
       "      <td>0.164129</td>\n",
       "      <td>0.142844</td>\n",
       "      <td>0.138501</td>\n",
       "      <td>0.138101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>0.124677</td>\n",
       "      <td>0.103114</td>\n",
       "      <td>0.069169</td>\n",
       "      <td>0.141136</td>\n",
       "      <td>0.140315</td>\n",
       "      <td>0.140639</td>\n",
       "      <td>0.141045</td>\n",
       "      <td>0.139904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3652</th>\n",
       "      <td>0.133204</td>\n",
       "      <td>0.124674</td>\n",
       "      <td>0.109573</td>\n",
       "      <td>0.156582</td>\n",
       "      <td>0.019866</td>\n",
       "      <td>0.157376</td>\n",
       "      <td>0.150261</td>\n",
       "      <td>0.148464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3653</th>\n",
       "      <td>0.149243</td>\n",
       "      <td>0.152187</td>\n",
       "      <td>0.101201</td>\n",
       "      <td>0.146628</td>\n",
       "      <td>0.018196</td>\n",
       "      <td>0.134128</td>\n",
       "      <td>0.150242</td>\n",
       "      <td>0.148175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3654</th>\n",
       "      <td>0.141645</td>\n",
       "      <td>0.111326</td>\n",
       "      <td>0.149386</td>\n",
       "      <td>0.064512</td>\n",
       "      <td>0.103629</td>\n",
       "      <td>0.138300</td>\n",
       "      <td>0.145809</td>\n",
       "      <td>0.145393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3655</th>\n",
       "      <td>0.125727</td>\n",
       "      <td>0.126379</td>\n",
       "      <td>0.126781</td>\n",
       "      <td>0.073570</td>\n",
       "      <td>0.126038</td>\n",
       "      <td>0.138850</td>\n",
       "      <td>0.141207</td>\n",
       "      <td>0.141448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3656 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cat_0     cat_1     cat_2     cat_3     cat_4     cat_5     cat_6  \\\n",
       "0     0.139960  0.139766  0.005966  0.133509  0.164651  0.140062  0.137625   \n",
       "1     0.125643  0.100861  0.063706  0.143727  0.144957  0.140027  0.140824   \n",
       "2     0.135338  0.159200  0.135089  0.144879  0.009109  0.130117  0.143994   \n",
       "3     0.138452  0.136893  0.006543  0.135634  0.165169  0.141831  0.137625   \n",
       "4     0.138100  0.132440  0.005766  0.140118  0.164129  0.142844  0.138501   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3651  0.124677  0.103114  0.069169  0.141136  0.140315  0.140639  0.141045   \n",
       "3652  0.133204  0.124674  0.109573  0.156582  0.019866  0.157376  0.150261   \n",
       "3653  0.149243  0.152187  0.101201  0.146628  0.018196  0.134128  0.150242   \n",
       "3654  0.141645  0.111326  0.149386  0.064512  0.103629  0.138300  0.145809   \n",
       "3655  0.125727  0.126379  0.126781  0.073570  0.126038  0.138850  0.141207   \n",
       "\n",
       "         cat_7  \n",
       "0     0.138462  \n",
       "1     0.140255  \n",
       "2     0.142275  \n",
       "3     0.137854  \n",
       "4     0.138101  \n",
       "...        ...  \n",
       "3651  0.139904  \n",
       "3652  0.148464  \n",
       "3653  0.148175  \n",
       "3654  0.145393  \n",
       "3655  0.141448  \n",
       "\n",
       "[3656 rows x 8 columns]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_adaptivetrain_smote = np.array(ada_mod.predict_proba(X_train_SMOTE))\n",
    "dataset_train_smote = pd.DataFrame(x_adaptivetrain_smote, columns=x_norm.columns[:8])\n",
    "dataset_train_smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNB ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.20      0.29        84\n",
      "           1       0.17      0.56      0.26        41\n",
      "           2       0.20      0.70      0.30        46\n",
      "           3       0.16      0.42      0.23        40\n",
      "           4       0.21      0.40      0.28        68\n",
      "           5       0.11      0.01      0.02       117\n",
      "           6       0.25      0.01      0.02       115\n",
      "           7       0.24      0.08      0.12       101\n",
      "\n",
      "    accuracy                           0.21       612\n",
      "   macro avg       0.23      0.30      0.19       612\n",
      "weighted avg       0.24      0.21      0.15       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = GaussianNB()\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "y_pred = nbtrain.predict(x_test_norm)\n",
    "print(classification_report(y_test_norm, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.61      0.73        84\n",
      "           1       0.66      0.85      0.74        41\n",
      "           2       0.38      0.78      0.51        46\n",
      "           3       0.29      0.65      0.40        40\n",
      "           4       0.36      0.62      0.46        68\n",
      "           5       0.87      0.66      0.75       117\n",
      "           6       0.61      0.43      0.51       115\n",
      "           7       0.62      0.21      0.31       101\n",
      "\n",
      "    accuracy                           0.55       612\n",
      "   macro avg       0.59      0.60      0.55       612\n",
      "weighted avg       0.64      0.55      0.56       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# overfit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = svm.SVC()\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "y_pred = nbtrain.predict(x_test_norm)\n",
    "print(classification_report(y_test_norm, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.63      0.71        84\n",
      "           1       0.71      0.78      0.74        41\n",
      "           2       0.57      0.61      0.59        46\n",
      "           3       0.28      0.38      0.32        40\n",
      "           4       0.40      0.41      0.41        68\n",
      "           5       0.69      0.62      0.65       117\n",
      "           6       0.62      0.60      0.61       115\n",
      "           7       0.39      0.44      0.41       101\n",
      "\n",
      "    accuracy                           0.56       612\n",
      "   macro avg       0.56      0.56      0.56       612\n",
      "weighted avg       0.58      0.56      0.57       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# overfit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = DecisionTreeClassifier(random_state=0)\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "y_pred = nbtrain.predict(x_test_norm)\n",
    "print(classification_report(y_test_norm, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knn ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.71      0.76        84\n",
      "           1       0.70      0.85      0.77        41\n",
      "           2       0.55      0.80      0.65        46\n",
      "           3       0.26      0.55      0.35        40\n",
      "           4       0.35      0.51      0.42        68\n",
      "           5       0.84      0.66      0.74       117\n",
      "           6       0.61      0.50      0.55       115\n",
      "           7       0.55      0.27      0.36       101\n",
      "\n",
      "    accuracy                           0.57       612\n",
      "   macro avg       0.58      0.61      0.58       612\n",
      "weighted avg       0.62      0.57      0.58       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# overfit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = KNeighborsClassifier(n_neighbors=6)\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "y_pred = nbtrain.predict(x_test_norm)\n",
    "print(classification_report(y_test_norm, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "74/74 [==============================] - 1s 1ms/step - loss: 0.4866 - accuracy: 0.1362\n",
      "Epoch 2/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -0.7849 - accuracy: 0.1518\n",
      "Epoch 3/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -3.5769 - accuracy: 0.1255\n",
      "Epoch 4/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -6.2560 - accuracy: 0.1250\n",
      "Epoch 5/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -8.2755 - accuracy: 0.1250\n",
      "Epoch 6/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -10.0009 - accuracy: 0.1250\n",
      "Epoch 7/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -11.5862 - accuracy: 0.1250\n",
      "Epoch 8/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -13.0948 - accuracy: 0.1250\n",
      "Epoch 9/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -14.5506 - accuracy: 0.1250\n",
      "Epoch 10/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -16.0040 - accuracy: 0.1275\n",
      "Epoch 11/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -17.4909 - accuracy: 0.1384\n",
      "Epoch 12/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -18.9219 - accuracy: 0.1436\n",
      "Epoch 13/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -20.3296 - accuracy: 0.1463\n",
      "Epoch 14/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -21.7331 - accuracy: 0.1504\n",
      "Epoch 15/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -23.1025 - accuracy: 0.1504\n",
      "Epoch 16/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -24.4655 - accuracy: 0.1482\n",
      "Epoch 17/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -25.8379 - accuracy: 0.1515\n",
      "Epoch 18/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -27.2062 - accuracy: 0.1518\n",
      "Epoch 19/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -28.5866 - accuracy: 0.1575\n",
      "Epoch 20/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -29.9257 - accuracy: 0.1562\n",
      "Epoch 21/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -31.2506 - accuracy: 0.1521\n",
      "Epoch 22/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -32.6141 - accuracy: 0.1562\n",
      "Epoch 23/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -33.9492 - accuracy: 0.1570\n",
      "Epoch 24/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -35.3111 - accuracy: 0.1595\n",
      "Epoch 25/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -36.6006 - accuracy: 0.1551\n",
      "Epoch 26/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -37.9981 - accuracy: 0.1606\n",
      "Epoch 27/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -39.3248 - accuracy: 0.1608\n",
      "Epoch 28/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -40.5645 - accuracy: 0.1540\n",
      "Epoch 29/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -41.9399 - accuracy: 0.1573\n",
      "Epoch 30/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -43.2924 - accuracy: 0.1608\n",
      "Epoch 31/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -44.5773 - accuracy: 0.1586\n",
      "Epoch 32/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -45.9603 - accuracy: 0.1627\n",
      "Epoch 33/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -47.3067 - accuracy: 0.1627\n",
      "Epoch 34/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -48.6195 - accuracy: 0.1630\n",
      "Epoch 35/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -49.8772 - accuracy: 0.1592\n",
      "Epoch 36/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -51.2582 - accuracy: 0.1630\n",
      "Epoch 37/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -52.6125 - accuracy: 0.1638\n",
      "Epoch 38/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -53.8649 - accuracy: 0.1625\n",
      "Epoch 39/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -55.2529 - accuracy: 0.1641\n",
      "Epoch 40/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -56.5651 - accuracy: 0.1649\n",
      "Epoch 41/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -57.8857 - accuracy: 0.1644\n",
      "Epoch 42/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -59.2021 - accuracy: 0.1649\n",
      "Epoch 43/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -60.4843 - accuracy: 0.1627\n",
      "Epoch 44/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -61.8338 - accuracy: 0.1658\n",
      "Epoch 45/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -63.0735 - accuracy: 0.1622\n",
      "Epoch 46/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -64.3916 - accuracy: 0.1611\n",
      "Epoch 47/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -65.8005 - accuracy: 0.1666\n",
      "Epoch 48/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -67.0481 - accuracy: 0.1633\n",
      "Epoch 49/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -68.2845 - accuracy: 0.1619\n",
      "Epoch 50/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -69.6573 - accuracy: 0.1666\n",
      "Epoch 51/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -71.0468 - accuracy: 0.1668\n",
      "Epoch 52/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -72.1985 - accuracy: 0.1633\n",
      "Epoch 53/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -73.5551 - accuracy: 0.1641\n",
      "Epoch 54/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -74.8737 - accuracy: 0.1627\n",
      "Epoch 55/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -76.2951 - accuracy: 0.1666\n",
      "Epoch 56/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -77.6172 - accuracy: 0.1679\n",
      "Epoch 57/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -78.6016 - accuracy: 0.1627\n",
      "Epoch 58/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -79.9461 - accuracy: 0.1573\n",
      "Epoch 59/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -81.4258 - accuracy: 0.1638\n",
      "Epoch 60/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -82.8497 - accuracy: 0.1671\n",
      "Epoch 61/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -84.0346 - accuracy: 0.1647\n",
      "Epoch 62/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -85.2825 - accuracy: 0.1668\n",
      "Epoch 63/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -86.7323 - accuracy: 0.1679\n",
      "Epoch 64/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -87.9842 - accuracy: 0.1655\n",
      "Epoch 65/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -89.4074 - accuracy: 0.1677\n",
      "Epoch 66/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -90.6861 - accuracy: 0.1674\n",
      "Epoch 67/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -91.9527 - accuracy: 0.1666\n",
      "Epoch 68/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -93.2746 - accuracy: 0.1671\n",
      "Epoch 69/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -94.5735 - accuracy: 0.1666\n",
      "Epoch 70/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -95.6612 - accuracy: 0.1622\n",
      "Epoch 71/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -96.8996 - accuracy: 0.1567\n",
      "Epoch 72/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -98.2410 - accuracy: 0.1589\n",
      "Epoch 73/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -99.8659 - accuracy: 0.1688\n",
      "Epoch 74/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -100.9581 - accuracy: 0.1644\n",
      "Epoch 75/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -102.3858 - accuracy: 0.1666\n",
      "Epoch 76/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -103.6951 - accuracy: 0.1652\n",
      "Epoch 77/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -105.0222 - accuracy: 0.1677\n",
      "Epoch 78/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -106.4074 - accuracy: 0.1677\n",
      "Epoch 79/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -107.4664 - accuracy: 0.1630\n",
      "Epoch 80/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -108.6194 - accuracy: 0.1567\n",
      "Epoch 81/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -110.1279 - accuracy: 0.1625\n",
      "Epoch 82/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -111.6464 - accuracy: 0.1685\n",
      "Epoch 83/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -112.6737 - accuracy: 0.1641\n",
      "Epoch 84/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -113.8829 - accuracy: 0.1592\n",
      "Epoch 85/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -115.5733 - accuracy: 0.1682\n",
      "Epoch 86/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -116.9035 - accuracy: 0.1696\n",
      "Epoch 87/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -117.6535 - accuracy: 0.1630\n",
      "Epoch 88/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -119.1237 - accuracy: 0.1592\n",
      "Epoch 89/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -120.8241 - accuracy: 0.1693\n",
      "Epoch 90/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -122.1062 - accuracy: 0.1690\n",
      "Epoch 91/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -123.2832 - accuracy: 0.1658\n",
      "Epoch 92/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -124.2849 - accuracy: 0.1586\n",
      "Epoch 93/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -125.8578 - accuracy: 0.1641\n",
      "Epoch 94/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -127.4066 - accuracy: 0.1704\n",
      "Epoch 95/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -128.2980 - accuracy: 0.1608\n",
      "Epoch 96/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -129.8934 - accuracy: 0.1685\n",
      "Epoch 97/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -131.2841 - accuracy: 0.1693\n",
      "Epoch 98/100\n",
      "74/74 [==============================] - 0s 1ms/step - loss: -132.6191 - accuracy: 0.1690\n",
      "Epoch 99/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -133.8149 - accuracy: 0.1685\n",
      "Epoch 100/100\n",
      "74/74 [==============================] - 0s 2ms/step - loss: -135.2642 - accuracy: 0.1696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a406065490>"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overfit\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units=10,kernel_initializer='uniform' , activation='tanh' , input_dim=26))\n",
    "classifier.add(Dense(units=6,kernel_initializer='uniform' , activation='tanh'))\n",
    "classifier.add(Dense(units=1 , kernel_initializer='uniform' , activation='sigmoid'))\n",
    "classifier.compile(optimizer='adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "classifier.fit(X_train_SMOTE, y_train_SMOTE, batch_size = 50, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.32      0.49        84\n",
      "           1       0.07      1.00      0.13        41\n",
      "           2       0.00      0.00      0.00        46\n",
      "           3       0.00      0.00      0.00        40\n",
      "           4       0.00      0.00      0.00        68\n",
      "           5       0.00      0.00      0.00       117\n",
      "           6       0.00      0.00      0.00       115\n",
      "           7       0.00      0.00      0.00       101\n",
      "\n",
      "    accuracy                           0.11       612\n",
      "   macro avg       0.13      0.17      0.08       612\n",
      "weighted avg       0.14      0.11      0.08       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = classifier.predict(x_test_norm)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "print(classification_report(y_test_norm, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | Adaboost | Adaboost + Normalisasi | Adaboost + Normalisasi + imbalanced |\n",
    "| --- | --- | --- | --- |\n",
    "| GaussianNB | 0.28 | 0.27 | 0.21 |\n",
    "| SVM | 0.36 | 0.34 | 0.55 |\n",
    "| Decision Tree | 0.42 | 0.39 | 0.56 |\n",
    "| KNN | 0.45076923076923076 | 0.4493464052287582 | 0.57 |\n",
    "| ANN | 0.08 | 0.07 | 0.11\n",
    "\n",
    "\n",
    "Berdasarkan hasil diatas, menunjukkan nilai akurasi yang dihasilkan oleh metode ensemble menggunakan adaptive boosting dan overfit pada imbalanced dataset menghasilkan nilai evaluasi yang lebih baik dibandingkan metode tanpa menggunakan overfit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7288e82646d3164eca24130947288f8779d11454649f2c02a5dfc42af7f324c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penerapan Metode Ensemble untuk meningkatkan kinerja Algoritma Klasifikasi pada Imbalanced Dataset\n",
    "\n",
    "| | Nama | NRP |\n",
    "| --- | --- | --- |\n",
    "| 1. | Helsa Nesta Dhaifullah | 5025201005 |\n",
    "| 2. | Danial Farros Maulana | 5025201004 |\n",
    "| 3. | Aiffah Kiysa Waafi | 5025201202 |\n",
    "| 4. | Muhammad Afif Dwi Ardhiansyah | 5025201212 |\n",
    "| 5. | Gloria Dyah Pramesti | 5025201033 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "      <th>cat_8</th>\n",
       "      <th>cat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_17</th>\n",
       "      <th>cat_18</th>\n",
       "      <th>cat_19</th>\n",
       "      <th>cat_20</th>\n",
       "      <th>cat_21</th>\n",
       "      <th>cat_22</th>\n",
       "      <th>cat_23</th>\n",
       "      <th>cat_24</th>\n",
       "      <th>cat_25</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9323</td>\n",
       "      <td>7670</td>\n",
       "      <td>3810</td>\n",
       "      <td>2313</td>\n",
       "      <td>1690</td>\n",
       "      <td>1450</td>\n",
       "      <td>1390</td>\n",
       "      <td>1744</td>\n",
       "      <td>1818</td>\n",
       "      <td>2246</td>\n",
       "      <td>...</td>\n",
       "      <td>2088</td>\n",
       "      <td>1870</td>\n",
       "      <td>2251</td>\n",
       "      <td>2286</td>\n",
       "      <td>3249</td>\n",
       "      <td>4540</td>\n",
       "      <td>6821</td>\n",
       "      <td>12954</td>\n",
       "      <td>104300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8820</td>\n",
       "      <td>7629</td>\n",
       "      <td>3669</td>\n",
       "      <td>2388</td>\n",
       "      <td>1623</td>\n",
       "      <td>1531</td>\n",
       "      <td>1343</td>\n",
       "      <td>1646</td>\n",
       "      <td>1785</td>\n",
       "      <td>2211</td>\n",
       "      <td>...</td>\n",
       "      <td>1909</td>\n",
       "      <td>1810</td>\n",
       "      <td>2111</td>\n",
       "      <td>2218</td>\n",
       "      <td>3125</td>\n",
       "      <td>4585</td>\n",
       "      <td>6945</td>\n",
       "      <td>12943</td>\n",
       "      <td>105986</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13101</td>\n",
       "      <td>6905</td>\n",
       "      <td>5009</td>\n",
       "      <td>3260</td>\n",
       "      <td>2465</td>\n",
       "      <td>1954</td>\n",
       "      <td>1690</td>\n",
       "      <td>1602</td>\n",
       "      <td>1610</td>\n",
       "      <td>1676</td>\n",
       "      <td>...</td>\n",
       "      <td>1692</td>\n",
       "      <td>1725</td>\n",
       "      <td>2019</td>\n",
       "      <td>2310</td>\n",
       "      <td>3281</td>\n",
       "      <td>4908</td>\n",
       "      <td>6899</td>\n",
       "      <td>13322</td>\n",
       "      <td>111351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12866</td>\n",
       "      <td>6717</td>\n",
       "      <td>5054</td>\n",
       "      <td>3392</td>\n",
       "      <td>2559</td>\n",
       "      <td>1972</td>\n",
       "      <td>1826</td>\n",
       "      <td>1736</td>\n",
       "      <td>1708</td>\n",
       "      <td>1882</td>\n",
       "      <td>...</td>\n",
       "      <td>1769</td>\n",
       "      <td>1856</td>\n",
       "      <td>2091</td>\n",
       "      <td>2523</td>\n",
       "      <td>3399</td>\n",
       "      <td>4871</td>\n",
       "      <td>6825</td>\n",
       "      <td>13230</td>\n",
       "      <td>109851</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8157</td>\n",
       "      <td>6292</td>\n",
       "      <td>2910</td>\n",
       "      <td>1899</td>\n",
       "      <td>1423</td>\n",
       "      <td>1301</td>\n",
       "      <td>1084</td>\n",
       "      <td>1453</td>\n",
       "      <td>1767</td>\n",
       "      <td>2447</td>\n",
       "      <td>...</td>\n",
       "      <td>2172</td>\n",
       "      <td>1586</td>\n",
       "      <td>1908</td>\n",
       "      <td>2458</td>\n",
       "      <td>3705</td>\n",
       "      <td>5406</td>\n",
       "      <td>5382</td>\n",
       "      <td>13690</td>\n",
       "      <td>95304</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat_0  cat_1  cat_2  cat_3  cat_4  cat_5  cat_6  cat_7  cat_8  cat_9  ...  \\\n",
       "0   9323   7670   3810   2313   1690   1450   1390   1744   1818   2246  ...   \n",
       "1   8820   7629   3669   2388   1623   1531   1343   1646   1785   2211  ...   \n",
       "2  13101   6905   5009   3260   2465   1954   1690   1602   1610   1676  ...   \n",
       "3  12866   6717   5054   3392   2559   1972   1826   1736   1708   1882  ...   \n",
       "4   8157   6292   2910   1899   1423   1301   1084   1453   1767   2447  ...   \n",
       "\n",
       "   cat_17  cat_18  cat_19  cat_20  cat_21  cat_22  cat_23  cat_24  cat_25  \\\n",
       "0    2088    1870    2251    2286    3249    4540    6821   12954  104300   \n",
       "1    1909    1810    2111    2218    3125    4585    6945   12943  105986   \n",
       "2    1692    1725    2019    2310    3281    4908    6899   13322  111351   \n",
       "3    1769    1856    2091    2523    3399    4871    6825   13230  109851   \n",
       "4    2172    1586    1908    2458    3705    5406    5382   13690   95304   \n",
       "\n",
       "   category  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('Fitur_LBPuniform_Ikan.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cat_0', 'cat_1', 'cat_2', 'cat_3', 'cat_4', 'cat_5', 'cat_6', 'cat_7',\n",
       "       'cat_8', 'cat_9', 'cat_10', 'cat_11', 'cat_12', 'cat_13', 'cat_14',\n",
       "       'cat_15', 'cat_16', 'cat_17', 'cat_18', 'cat_19', 'cat_20', 'cat_21',\n",
       "       'cat_22', 'cat_23', 'cat_24', 'cat_25', 'category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    577\n",
       "5    564\n",
       "6    544\n",
       "0    500\n",
       "4    331\n",
       "3    252\n",
       "1    240\n",
       "2    240\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Adaboost\n",
    "*   Adaboost + Normalisasi\n",
    "*   Adaboost + Normalisasi + imbalanced Overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x y partition ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "3243    7\n",
       "3244    7\n",
       "3245    7\n",
       "3246    7\n",
       "3247    7\n",
       "Name: category, Length: 3248, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df.drop(['category'],axis=1)\n",
    "y = df['category']\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting DataSet ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Training: (3248, 26)\n",
      "Input Training: (2598, 26)\n",
      "Input Test: (650, 26)\n",
      "Output Training: (2598,)\n",
      "Output Test: (650,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_ada, x_test_ada, y_train_ada, y_test_ada = train_test_split(x, y, train_size=0.8,random_state=1)\n",
    "print(\"Input Training:\",x.shape)\n",
    "print(\"Input Training:\",x_train_ada.shape)\n",
    "print(\"Input Test:\",x_test_ada.shape)\n",
    "print(\"Output Training:\",y_train_ada.shape)\n",
    "print(\"Output Test:\",y_test_ada.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15975355, 0.07241733, 0.00958981, ..., 0.14675932, 0.14670291,\n",
       "        0.15010377],\n",
       "       [0.15691719, 0.14460951, 0.13513871, ..., 0.14074908, 0.14242612,\n",
       "        0.14362121],\n",
       "       [0.13442079, 0.146122  , 0.13631884, ..., 0.12884309, 0.132377  ,\n",
       "        0.13785988],\n",
       "       ...,\n",
       "       [0.14636597, 0.06027012, 0.00744168, ..., 0.14717055, 0.14481282,\n",
       "        0.14544028],\n",
       "       [0.12323081, 0.09096467, 0.0875205 , ..., 0.13785853, 0.13790885,\n",
       "        0.13935031],\n",
       "       [0.13003815, 0.13996243, 0.14634144, ..., 0.13315268, 0.13518736,\n",
       "        0.13548248]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_mod = AdaBoostClassifier(n_estimators=300, learning_rate=0.2)\n",
    "\n",
    "ada_mod.fit(x_train_ada, y_train_ada)\n",
    "\n",
    "x_adaptiveresult = ada_mod.predict_proba(x_test_ada)\n",
    "x_adaptiveresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.159754</td>\n",
       "      <td>0.072417</td>\n",
       "      <td>0.009590</td>\n",
       "      <td>0.144885</td>\n",
       "      <td>0.169788</td>\n",
       "      <td>0.146759</td>\n",
       "      <td>0.146703</td>\n",
       "      <td>0.150104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.156917</td>\n",
       "      <td>0.144610</td>\n",
       "      <td>0.135139</td>\n",
       "      <td>0.118532</td>\n",
       "      <td>0.018006</td>\n",
       "      <td>0.140749</td>\n",
       "      <td>0.142426</td>\n",
       "      <td>0.143621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.134421</td>\n",
       "      <td>0.146122</td>\n",
       "      <td>0.136319</td>\n",
       "      <td>0.071697</td>\n",
       "      <td>0.112361</td>\n",
       "      <td>0.128843</td>\n",
       "      <td>0.132377</td>\n",
       "      <td>0.137860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.163001</td>\n",
       "      <td>0.054822</td>\n",
       "      <td>0.052293</td>\n",
       "      <td>0.175421</td>\n",
       "      <td>0.141631</td>\n",
       "      <td>0.082407</td>\n",
       "      <td>0.164777</td>\n",
       "      <td>0.165648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.130812</td>\n",
       "      <td>0.090072</td>\n",
       "      <td>0.110969</td>\n",
       "      <td>0.124330</td>\n",
       "      <td>0.125790</td>\n",
       "      <td>0.137270</td>\n",
       "      <td>0.140710</td>\n",
       "      <td>0.140047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>0.136384</td>\n",
       "      <td>0.179467</td>\n",
       "      <td>0.163702</td>\n",
       "      <td>0.092131</td>\n",
       "      <td>0.025626</td>\n",
       "      <td>0.132467</td>\n",
       "      <td>0.133694</td>\n",
       "      <td>0.136527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>0.144047</td>\n",
       "      <td>0.138025</td>\n",
       "      <td>0.141977</td>\n",
       "      <td>0.068065</td>\n",
       "      <td>0.096695</td>\n",
       "      <td>0.130520</td>\n",
       "      <td>0.139547</td>\n",
       "      <td>0.141124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>0.146366</td>\n",
       "      <td>0.060270</td>\n",
       "      <td>0.007442</td>\n",
       "      <td>0.176805</td>\n",
       "      <td>0.171694</td>\n",
       "      <td>0.147171</td>\n",
       "      <td>0.144813</td>\n",
       "      <td>0.145440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>0.123231</td>\n",
       "      <td>0.090965</td>\n",
       "      <td>0.087520</td>\n",
       "      <td>0.144399</td>\n",
       "      <td>0.138768</td>\n",
       "      <td>0.137859</td>\n",
       "      <td>0.137909</td>\n",
       "      <td>0.139350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>0.130038</td>\n",
       "      <td>0.139962</td>\n",
       "      <td>0.146341</td>\n",
       "      <td>0.085228</td>\n",
       "      <td>0.094607</td>\n",
       "      <td>0.133153</td>\n",
       "      <td>0.135187</td>\n",
       "      <td>0.135482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>650 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat_0     cat_1     cat_2     cat_3     cat_4     cat_5     cat_6  \\\n",
       "0    0.159754  0.072417  0.009590  0.144885  0.169788  0.146759  0.146703   \n",
       "1    0.156917  0.144610  0.135139  0.118532  0.018006  0.140749  0.142426   \n",
       "2    0.134421  0.146122  0.136319  0.071697  0.112361  0.128843  0.132377   \n",
       "3    0.163001  0.054822  0.052293  0.175421  0.141631  0.082407  0.164777   \n",
       "4    0.130812  0.090072  0.110969  0.124330  0.125790  0.137270  0.140710   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "645  0.136384  0.179467  0.163702  0.092131  0.025626  0.132467  0.133694   \n",
       "646  0.144047  0.138025  0.141977  0.068065  0.096695  0.130520  0.139547   \n",
       "647  0.146366  0.060270  0.007442  0.176805  0.171694  0.147171  0.144813   \n",
       "648  0.123231  0.090965  0.087520  0.144399  0.138768  0.137859  0.137909   \n",
       "649  0.130038  0.139962  0.146341  0.085228  0.094607  0.133153  0.135187   \n",
       "\n",
       "        cat_7  \n",
       "0    0.150104  \n",
       "1    0.143621  \n",
       "2    0.137860  \n",
       "3    0.165648  \n",
       "4    0.140047  \n",
       "..        ...  \n",
       "645  0.136527  \n",
       "646  0.141124  \n",
       "647  0.145440  \n",
       "648  0.139350  \n",
       "649  0.135482  \n",
       "\n",
       "[650 rows x 8 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_adaptivetest = np.array(ada_mod.predict_proba(x_test_ada))\n",
    "dataset_test = pd.DataFrame(x_adaptivetest, columns=x.columns[:8])\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.109725</td>\n",
       "      <td>0.090432</td>\n",
       "      <td>0.114522</td>\n",
       "      <td>0.138600</td>\n",
       "      <td>0.129700</td>\n",
       "      <td>0.136681</td>\n",
       "      <td>0.141701</td>\n",
       "      <td>0.138639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.139010</td>\n",
       "      <td>0.110770</td>\n",
       "      <td>0.133712</td>\n",
       "      <td>0.073902</td>\n",
       "      <td>0.106780</td>\n",
       "      <td>0.142641</td>\n",
       "      <td>0.146225</td>\n",
       "      <td>0.146960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.153692</td>\n",
       "      <td>0.068315</td>\n",
       "      <td>0.007921</td>\n",
       "      <td>0.157719</td>\n",
       "      <td>0.176270</td>\n",
       "      <td>0.146247</td>\n",
       "      <td>0.143749</td>\n",
       "      <td>0.146089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.130683</td>\n",
       "      <td>0.146666</td>\n",
       "      <td>0.168009</td>\n",
       "      <td>0.143703</td>\n",
       "      <td>0.018940</td>\n",
       "      <td>0.096666</td>\n",
       "      <td>0.146555</td>\n",
       "      <td>0.148778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.138894</td>\n",
       "      <td>0.148492</td>\n",
       "      <td>0.010369</td>\n",
       "      <td>0.128042</td>\n",
       "      <td>0.156890</td>\n",
       "      <td>0.138639</td>\n",
       "      <td>0.137410</td>\n",
       "      <td>0.141263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>0.132811</td>\n",
       "      <td>0.150818</td>\n",
       "      <td>0.171065</td>\n",
       "      <td>0.142008</td>\n",
       "      <td>0.021867</td>\n",
       "      <td>0.078832</td>\n",
       "      <td>0.150211</td>\n",
       "      <td>0.152389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>0.117397</td>\n",
       "      <td>0.100526</td>\n",
       "      <td>0.135026</td>\n",
       "      <td>0.137285</td>\n",
       "      <td>0.093730</td>\n",
       "      <td>0.137095</td>\n",
       "      <td>0.141192</td>\n",
       "      <td>0.137749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>0.146882</td>\n",
       "      <td>0.061214</td>\n",
       "      <td>0.007406</td>\n",
       "      <td>0.176998</td>\n",
       "      <td>0.171839</td>\n",
       "      <td>0.146589</td>\n",
       "      <td>0.144119</td>\n",
       "      <td>0.144954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2596</th>\n",
       "      <td>0.139311</td>\n",
       "      <td>0.050502</td>\n",
       "      <td>0.050160</td>\n",
       "      <td>0.171799</td>\n",
       "      <td>0.149022</td>\n",
       "      <td>0.143797</td>\n",
       "      <td>0.148739</td>\n",
       "      <td>0.146670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2597</th>\n",
       "      <td>0.143401</td>\n",
       "      <td>0.059196</td>\n",
       "      <td>0.006879</td>\n",
       "      <td>0.188559</td>\n",
       "      <td>0.175326</td>\n",
       "      <td>0.144440</td>\n",
       "      <td>0.140556</td>\n",
       "      <td>0.141645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2598 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cat_0     cat_1     cat_2     cat_3     cat_4     cat_5     cat_6  \\\n",
       "0     0.109725  0.090432  0.114522  0.138600  0.129700  0.136681  0.141701   \n",
       "1     0.139010  0.110770  0.133712  0.073902  0.106780  0.142641  0.146225   \n",
       "2     0.153692  0.068315  0.007921  0.157719  0.176270  0.146247  0.143749   \n",
       "3     0.130683  0.146666  0.168009  0.143703  0.018940  0.096666  0.146555   \n",
       "4     0.138894  0.148492  0.010369  0.128042  0.156890  0.138639  0.137410   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2593  0.132811  0.150818  0.171065  0.142008  0.021867  0.078832  0.150211   \n",
       "2594  0.117397  0.100526  0.135026  0.137285  0.093730  0.137095  0.141192   \n",
       "2595  0.146882  0.061214  0.007406  0.176998  0.171839  0.146589  0.144119   \n",
       "2596  0.139311  0.050502  0.050160  0.171799  0.149022  0.143797  0.148739   \n",
       "2597  0.143401  0.059196  0.006879  0.188559  0.175326  0.144440  0.140556   \n",
       "\n",
       "         cat_7  \n",
       "0     0.138639  \n",
       "1     0.146960  \n",
       "2     0.146089  \n",
       "3     0.148778  \n",
       "4     0.141263  \n",
       "...        ...  \n",
       "2593  0.152389  \n",
       "2594  0.137749  \n",
       "2595  0.144954  \n",
       "2596  0.146670  \n",
       "2597  0.141645  \n",
       "\n",
       "[2598 rows x 8 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_adaptivetrain = np.array(ada_mod.predict_proba(x_train_ada))\n",
    "dataset_train = pd.DataFrame(x_adaptivetrain, columns=x.columns[:8])\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.31      0.33       107\n",
      "           1       0.66      0.55      0.60        49\n",
      "           2       0.33      0.79      0.47        53\n",
      "           3       0.11      0.21      0.14        57\n",
      "           4       0.18      0.48      0.26        66\n",
      "           5       0.51      0.17      0.26       109\n",
      "           6       0.26      0.05      0.09        97\n",
      "           7       0.21      0.08      0.12       112\n",
      "\n",
      "    accuracy                           0.28       650\n",
      "   macro avg       0.33      0.33      0.28       650\n",
      "weighted avg       0.32      0.28      0.25       650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = GaussianNB()\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(dataset_train, y_train_ada)\n",
    "y_pred = nbtrain.predict(dataset_test)\n",
    "print(classification_report(y_test_ada, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.56      0.65       107\n",
      "           1       0.48      0.67      0.56        49\n",
      "           2       0.32      0.47      0.38        53\n",
      "           3       0.00      0.00      0.00        57\n",
      "           4       0.27      0.38      0.31        66\n",
      "           5       0.35      0.36      0.35       109\n",
      "           6       0.25      0.49      0.33        97\n",
      "           7       0.20      0.05      0.08       112\n",
      "\n",
      "    accuracy                           0.36       650\n",
      "   macro avg       0.33      0.37      0.33       650\n",
      "weighted avg       0.35      0.36      0.34       650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = svm.SVC()\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(dataset_train, y_train_ada)\n",
    "y_pred = nbtrain.predict(dataset_test)\n",
    "print(classification_report(y_test_ada, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.61      0.58       107\n",
      "           1       0.61      0.61      0.61        49\n",
      "           2       0.56      0.58      0.57        53\n",
      "           3       0.26      0.28      0.27        57\n",
      "           4       0.27      0.20      0.23        66\n",
      "           5       0.38      0.38      0.38       109\n",
      "           6       0.41      0.52      0.46        97\n",
      "           7       0.28      0.22      0.25       112\n",
      "\n",
      "    accuracy                           0.42       650\n",
      "   macro avg       0.42      0.42      0.42       650\n",
      "weighted avg       0.41      0.42      0.41       650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = DecisionTreeClassifier(random_state=0)\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(dataset_train, y_train_ada)\n",
    "y_pred = nbtrain.predict(dataset_test)\n",
    "print(classification_report(y_test_ada, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 neighbors accuracy : 0.4307692307692308\n",
      "2 neighbors accuracy : 0.41384615384615386\n",
      "3 neighbors accuracy : 0.4276923076923077\n",
      "4 neighbors accuracy : 0.45076923076923076\n",
      "5 neighbors accuracy : 0.43846153846153846\n",
      "6 neighbors accuracy : 0.44769230769230767\n",
      "7 neighbors accuracy : 0.4276923076923077\n",
      "8 neighbors accuracy : 0.4307692307692308\n",
      "9 neighbors accuracy : 0.4107692307692308\n",
      "10 neighbors accuracy : 0.42923076923076925\n",
      "11 neighbors accuracy : 0.4169230769230769\n",
      "12 neighbors accuracy : 0.4169230769230769\n",
      "13 neighbors accuracy : 0.4246153846153846\n",
      "14 neighbors accuracy : 0.4169230769230769\n",
      "15 neighbors accuracy : 0.42153846153846153\n",
      "16 neighbors accuracy : 0.42\n",
      "17 neighbors accuracy : 0.42153846153846153\n",
      "18 neighbors accuracy : 0.4153846153846154\n",
      "19 neighbors accuracy : 0.4307692307692308\n",
      "20 neighbors accuracy : 0.43846153846153846\n",
      "21 neighbors accuracy : 0.4323076923076923\n",
      "22 neighbors accuracy : 0.41846153846153844\n",
      "23 neighbors accuracy : 0.4169230769230769\n",
      "24 neighbors accuracy : 0.4169230769230769\n",
      "25 neighbors accuracy : 0.40307692307692305\n",
      "26 neighbors accuracy : 0.39384615384615385\n",
      "27 neighbors accuracy : 0.4076923076923077\n",
      "28 neighbors accuracy : 0.4123076923076923\n",
      "29 neighbors accuracy : 0.40923076923076923\n",
      "30 neighbors accuracy : 0.39384615384615385\n",
      "31 neighbors accuracy : 0.4076923076923077\n",
      "32 neighbors accuracy : 0.4169230769230769\n",
      "33 neighbors accuracy : 0.4046153846153846\n",
      "34 neighbors accuracy : 0.4046153846153846\n",
      "35 neighbors accuracy : 0.4\n",
      "36 neighbors accuracy : 0.3953846153846154\n",
      "37 neighbors accuracy : 0.4015384615384615\n",
      "38 neighbors accuracy : 0.4076923076923077\n",
      "39 neighbors accuracy : 0.3984615384615385\n",
      "40 neighbors accuracy : 0.40615384615384614\n",
      "41 neighbors accuracy : 0.40307692307692305\n",
      "42 neighbors accuracy : 0.40923076923076923\n",
      "43 neighbors accuracy : 0.4169230769230769\n",
      "44 neighbors accuracy : 0.42\n",
      "45 neighbors accuracy : 0.40923076923076923\n",
      "46 neighbors accuracy : 0.4\n",
      "47 neighbors accuracy : 0.39384615384615385\n",
      "48 neighbors accuracy : 0.3923076923076923\n",
      "49 neighbors accuracy : 0.3861538461538462\n",
      "50 neighbors accuracy : 0.3892307692307692\n",
      "51 neighbors accuracy : 0.38461538461538464\n",
      "52 neighbors accuracy : 0.37846153846153846\n",
      "53 neighbors accuracy : 0.38\n",
      "54 neighbors accuracy : 0.38153846153846155\n",
      "55 neighbors accuracy : 0.3769230769230769\n",
      "56 neighbors accuracy : 0.3769230769230769\n",
      "57 neighbors accuracy : 0.37538461538461537\n",
      "58 neighbors accuracy : 0.37846153846153846\n",
      "59 neighbors accuracy : 0.3707692307692308\n",
      "60 neighbors accuracy : 0.37384615384615383\n",
      "61 neighbors accuracy : 0.3723076923076923\n",
      "62 neighbors accuracy : 0.3707692307692308\n",
      "63 neighbors accuracy : 0.37846153846153846\n",
      "64 neighbors accuracy : 0.37384615384615383\n",
      "65 neighbors accuracy : 0.3676923076923077\n",
      "66 neighbors accuracy : 0.3707692307692308\n",
      "67 neighbors accuracy : 0.3707692307692308\n",
      "68 neighbors accuracy : 0.37538461538461537\n",
      "69 neighbors accuracy : 0.36923076923076925\n",
      "70 neighbors accuracy : 0.37384615384615383\n",
      "71 neighbors accuracy : 0.37384615384615383\n",
      "72 neighbors accuracy : 0.38\n",
      "73 neighbors accuracy : 0.38153846153846155\n",
      "74 neighbors accuracy : 0.3723076923076923\n",
      "75 neighbors accuracy : 0.37384615384615383\n",
      "76 neighbors accuracy : 0.3630769230769231\n",
      "77 neighbors accuracy : 0.3723076923076923\n",
      "78 neighbors accuracy : 0.3769230769230769\n",
      "79 neighbors accuracy : 0.3630769230769231\n",
      "80 neighbors accuracy : 0.36\n",
      "81 neighbors accuracy : 0.3553846153846154\n",
      "82 neighbors accuracy : 0.35846153846153844\n",
      "83 neighbors accuracy : 0.3553846153846154\n",
      "84 neighbors accuracy : 0.36\n",
      "85 neighbors accuracy : 0.36615384615384616\n",
      "86 neighbors accuracy : 0.35846153846153844\n",
      "87 neighbors accuracy : 0.34923076923076923\n",
      "88 neighbors accuracy : 0.35384615384615387\n",
      "89 neighbors accuracy : 0.35384615384615387\n",
      "90 neighbors accuracy : 0.34615384615384615\n",
      "91 neighbors accuracy : 0.3446153846153846\n",
      "92 neighbors accuracy : 0.3523076923076923\n",
      "93 neighbors accuracy : 0.35846153846153844\n",
      "94 neighbors accuracy : 0.36153846153846153\n",
      "95 neighbors accuracy : 0.35384615384615387\n",
      "96 neighbors accuracy : 0.36\n",
      "97 neighbors accuracy : 0.35846153846153844\n",
      "98 neighbors accuracy : 0.36\n",
      "99 neighbors accuracy : 0.35384615384615387\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score  \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "for i in range (1,100) :\n",
    "    # Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "    modelnb = KNeighborsClassifier(n_neighbors=i)\n",
    "    # Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "    nbtrain = modelnb.fit(dataset_train, y_train_ada)\n",
    "    y_pred = nbtrain.predict(dataset_test)\n",
    "    print(str(i) + \" neighbors accuracy : \" + str(accuracy_score(y_test_ada, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 neighbors accuracy : 0.45076923076923076\n"
     ]
    }
   ],
   "source": [
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = KNeighborsClassifier(n_neighbors=4)\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(dataset_train, y_train_ada)\n",
    "y_pred = nbtrain.predict(dataset_test)\n",
    "print(str(i) + \" neighbors accuracy : \" + str(accuracy_score(y_test_ada, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "52/52 [==============================] - 1s 2ms/step - loss: 0.5277 - accuracy: 0.0735\n",
      "Epoch 2/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -0.4888 - accuracy: 0.0735\n",
      "Epoch 3/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -2.9012 - accuracy: 0.0735\n",
      "Epoch 4/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -5.3349 - accuracy: 0.0735\n",
      "Epoch 5/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -7.1876 - accuracy: 0.0735\n",
      "Epoch 6/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -8.7378 - accuracy: 0.0735\n",
      "Epoch 7/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -10.1398 - accuracy: 0.0735\n",
      "Epoch 8/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -11.4596 - accuracy: 0.0735\n",
      "Epoch 9/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -12.7269 - accuracy: 0.0735\n",
      "Epoch 10/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -13.9623 - accuracy: 0.0735\n",
      "Epoch 11/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -15.1692 - accuracy: 0.0735\n",
      "Epoch 12/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -16.3595 - accuracy: 0.0735\n",
      "Epoch 13/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -17.5394 - accuracy: 0.0735\n",
      "Epoch 14/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -18.7028 - accuracy: 0.0735\n",
      "Epoch 15/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -19.8600 - accuracy: 0.0735\n",
      "Epoch 16/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -21.0099 - accuracy: 0.0735\n",
      "Epoch 17/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -22.1541 - accuracy: 0.0735\n",
      "Epoch 18/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -23.2908 - accuracy: 0.0735\n",
      "Epoch 19/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -24.4250 - accuracy: 0.0735\n",
      "Epoch 20/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -25.5550 - accuracy: 0.0735\n",
      "Epoch 21/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -26.6823 - accuracy: 0.0735\n",
      "Epoch 22/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -27.8029 - accuracy: 0.0735\n",
      "Epoch 23/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -28.9263 - accuracy: 0.0735\n",
      "Epoch 24/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -30.0442 - accuracy: 0.0735\n",
      "Epoch 25/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -31.1588 - accuracy: 0.0735\n",
      "Epoch 26/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -32.2734 - accuracy: 0.0735\n",
      "Epoch 27/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -33.3873 - accuracy: 0.0735\n",
      "Epoch 28/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -34.4968 - accuracy: 0.0735\n",
      "Epoch 29/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -35.6064 - accuracy: 0.0735\n",
      "Epoch 30/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -36.7167 - accuracy: 0.0735\n",
      "Epoch 31/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -37.8207 - accuracy: 0.0735\n",
      "Epoch 32/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -38.9268 - accuracy: 0.0735\n",
      "Epoch 33/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -40.0337 - accuracy: 0.0735\n",
      "Epoch 34/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -41.1352 - accuracy: 0.0735\n",
      "Epoch 35/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -42.2396 - accuracy: 0.0735\n",
      "Epoch 36/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -43.3419 - accuracy: 0.0735\n",
      "Epoch 37/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -44.4417 - accuracy: 0.0735\n",
      "Epoch 38/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -45.5435 - accuracy: 0.0735\n",
      "Epoch 39/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -46.6433 - accuracy: 0.0735\n",
      "Epoch 40/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -47.7425 - accuracy: 0.0735\n",
      "Epoch 41/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -48.8409 - accuracy: 0.0735\n",
      "Epoch 42/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -49.9393 - accuracy: 0.0735\n",
      "Epoch 43/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -51.0353 - accuracy: 0.0735\n",
      "Epoch 44/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -52.1355 - accuracy: 0.0735\n",
      "Epoch 45/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -53.2290 - accuracy: 0.0735\n",
      "Epoch 46/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -54.3264 - accuracy: 0.0735\n",
      "Epoch 47/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -55.4215 - accuracy: 0.0735\n",
      "Epoch 48/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -56.5176 - accuracy: 0.0735\n",
      "Epoch 49/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -57.6138 - accuracy: 0.0735\n",
      "Epoch 50/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -58.7064 - accuracy: 0.0735\n",
      "Epoch 51/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -59.8025 - accuracy: 0.0735\n",
      "Epoch 52/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: -60.8963 - accuracy: 0.0735\n",
      "Epoch 53/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -61.9906 - accuracy: 0.0735\n",
      "Epoch 54/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -63.0839 - accuracy: 0.0735\n",
      "Epoch 55/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -64.1767 - accuracy: 0.0735\n",
      "Epoch 56/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -65.2706 - accuracy: 0.0735\n",
      "Epoch 57/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -66.3632 - accuracy: 0.0735\n",
      "Epoch 58/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -67.4547 - accuracy: 0.0735\n",
      "Epoch 59/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -68.5480 - accuracy: 0.0735\n",
      "Epoch 60/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -69.6393 - accuracy: 0.0735\n",
      "Epoch 61/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -70.7327 - accuracy: 0.0735\n",
      "Epoch 62/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: -71.8244 - accuracy: 0.0735\n",
      "Epoch 63/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -72.9167 - accuracy: 0.0735\n",
      "Epoch 64/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -74.0092 - accuracy: 0.0735\n",
      "Epoch 65/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -75.1019 - accuracy: 0.0735\n",
      "Epoch 66/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -76.1889 - accuracy: 0.0735\n",
      "Epoch 67/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -77.2828 - accuracy: 0.0735\n",
      "Epoch 68/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -78.3762 - accuracy: 0.0735\n",
      "Epoch 69/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -79.4663 - accuracy: 0.0735\n",
      "Epoch 70/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -80.5581 - accuracy: 0.0735\n",
      "Epoch 71/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -81.6468 - accuracy: 0.0735\n",
      "Epoch 72/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -82.7364 - accuracy: 0.0735\n",
      "Epoch 73/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -83.8326 - accuracy: 0.0735\n",
      "Epoch 74/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -84.9202 - accuracy: 0.0735\n",
      "Epoch 75/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -86.0089 - accuracy: 0.0735\n",
      "Epoch 76/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -87.1029 - accuracy: 0.0735\n",
      "Epoch 77/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -88.1916 - accuracy: 0.0735\n",
      "Epoch 78/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -89.2828 - accuracy: 0.0735\n",
      "Epoch 79/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -90.3742 - accuracy: 0.0735\n",
      "Epoch 80/100\n",
      "52/52 [==============================] - 0s 3ms/step - loss: -91.4640 - accuracy: 0.0735\n",
      "Epoch 81/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -92.5548 - accuracy: 0.0735\n",
      "Epoch 82/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -93.6444 - accuracy: 0.0735\n",
      "Epoch 83/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -94.7359 - accuracy: 0.0735\n",
      "Epoch 84/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -95.8257 - accuracy: 0.0735\n",
      "Epoch 85/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -96.9168 - accuracy: 0.0735\n",
      "Epoch 86/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -98.0069 - accuracy: 0.0735\n",
      "Epoch 87/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -99.0969 - accuracy: 0.0735\n",
      "Epoch 88/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -100.1881 - accuracy: 0.0735\n",
      "Epoch 89/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -101.2762 - accuracy: 0.0735\n",
      "Epoch 90/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -102.3673 - accuracy: 0.0735\n",
      "Epoch 91/100\n",
      "52/52 [==============================] - 0s 1ms/step - loss: -103.4579 - accuracy: 0.0735\n",
      "Epoch 92/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -104.5466 - accuracy: 0.0735\n",
      "Epoch 93/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -105.6389 - accuracy: 0.0735\n",
      "Epoch 94/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -106.7267 - accuracy: 0.0735\n",
      "Epoch 95/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -107.8184 - accuracy: 0.0735\n",
      "Epoch 96/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -108.9067 - accuracy: 0.0735\n",
      "Epoch 97/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -109.9976 - accuracy: 0.0735\n",
      "Epoch 98/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -111.0871 - accuracy: 0.0735\n",
      "Epoch 99/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -112.1760 - accuracy: 0.0735\n",
      "Epoch 100/100\n",
      "52/52 [==============================] - 0s 2ms/step - loss: -113.2684 - accuracy: 0.0735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aab45d22b0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units=10,kernel_initializer='uniform' , activation='tanh' , input_dim=8))\n",
    "classifier.add(Dense(units=6,kernel_initializer='uniform' , activation='tanh'))\n",
    "classifier.add(Dense(units=1 , kernel_initializer='uniform' , activation='sigmoid'))\n",
    "classifier.compile(optimizer='adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "classifier.fit(dataset_train, y_train_ada, batch_size = 50, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for finding best hyperparameters\n",
    "def FunctionFindBestParams(x_train, y_train):\n",
    "    \n",
    "    # Defining the list of hyper parameters to try\n",
    "    TrialNumber=0\n",
    "    batch_size_list=[5, 10, 20, 25]\n",
    "    epoch_list=[25, 50, 100]\n",
    "    \n",
    "    for batch_size_trial in batch_size_list:\n",
    "        for epochs_trial in epoch_list:\n",
    "            TrialNumber+=1\n",
    " \n",
    "            # Creating the classifier ANN model\n",
    "            classifier = Sequential()\n",
    "            classifier.add(Dense(units=10,kernel_initializer='uniform' , activation='tanh' , input_dim=8))\n",
    "            classifier.add(Dense(units=6,kernel_initializer='uniform' , activation='tanh'))\n",
    "            classifier.add(Dense(units=1 , kernel_initializer='uniform' , activation='sigmoid'))\n",
    "            classifier.compile(optimizer='adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "            survivalANN_Model=classifier.fit(x_train,y_train, batch_size=batch_size_trial , epochs=epochs_trial, verbose=0)\n",
    "            Accuracy = survivalANN_Model.history['accuracy'][-1]\n",
    "            \n",
    "            # printing the results of the current iteration\n",
    "            print(TrialNumber, 'Parameters:','batch_size:', batch_size_trial,'-', 'epochs:',epochs_trial, 'Accuracy:', Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the function\n",
    "# ResultsData = FunctionFindBestParams(dataset_train, y_train_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       107\n",
      "           1       0.08      1.00      0.14        49\n",
      "           2       0.00      0.00      0.00        53\n",
      "           3       0.00      0.00      0.00        57\n",
      "           4       0.00      0.00      0.00        66\n",
      "           5       0.00      0.00      0.00       109\n",
      "           6       0.00      0.00      0.00        97\n",
      "           7       0.00      0.00      0.00       112\n",
      "\n",
      "    accuracy                           0.08       650\n",
      "   macro avg       0.01      0.12      0.02       650\n",
      "weighted avg       0.01      0.08      0.01       650\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = classifier.predict(dataset_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "print(classification_report(y_test_ada, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Adaboost + Normalisasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check missing value ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check null =  0\n"
     ]
    }
   ],
   "source": [
    "print(\"check null = \",  df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data visualization ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Pair-wise Scatter Plots\n",
    "# import seaborn as sns\n",
    "# cols = [\"cat_0\",\"cat_1\",\"cat_2\",\"cat_3\",\"cat_4\",\"cat_5\",\"cat_6\",\"cat_7\",\"cat_8\",\"cat_9\",\"cat_10\",\"cat_11\",\"cat_12\",\"cat_13\",\"cat_14\",\"cat_15\",\"cat_16\",\"cat_17\",\"cat_18\",\"cat_19\",\"cat_20\",\"cat_21\",\"cat_22\",\"cat_23\",\"cat_24\",\"cat_25\",\"category\"]\n",
    "# pp = sns.pairplot(df[cols], size=1.8, aspect=1.8,\n",
    "#                   plot_kws=dict(edgecolor=\"k\", linewidth=0.5),\n",
    "#                   diag_kind=\"kde\", diag_kws=dict(shade=True))\n",
    "\n",
    "# fig = pp.fig \n",
    "# fig.subplots_adjust(top=0.93, wspace=0.3)\n",
    "# t = fig.suptitle('Wine Attributes Pairwise Plots', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fixing outlier ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x2160 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = [\"cat_0\",\"cat_1\",\"cat_2\",\"cat_3\",\"cat_4\",\"cat_5\",\"cat_6\",\"cat_7\",\"cat_8\",\"cat_9\",\"cat_10\",\"cat_11\",\"cat_12\",\"cat_13\",\"cat_14\",\"cat_15\",\"cat_16\",\"cat_17\",\"cat_18\",\"cat_19\",\"cat_20\",\"cat_21\",\"cat_22\",\"cat_23\",\"cat_24\",\"cat_25\",\"category\"]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (10, 30))\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "for x in range (20,26):\n",
    "    index = x%10\n",
    "    ax=plt.subplot(5,2,index+1) \n",
    "    plt.boxplot(df[cols[x]])\n",
    "    ax.set_title(cols[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFixOutliers = df\n",
    "\n",
    "outlierframe =['cat_9','cat_10','cat_11','cat_12','cat_12','cat_13','cat_17','cat_18','cat_19','cat_22','cat_24','cat_25']\n",
    "for cols in outlierframe :\n",
    "    Q1 = dfFixOutliers[cols].quantile(0.25)\n",
    "    Q3 = dfFixOutliers[cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1     \n",
    "\n",
    "    filter = (dfFixOutliers[cols] >= Q1 - 1.5 * IQR) & (dfFixOutliers[cols] <= Q3 + 1.5 *IQR)\n",
    "    dfFixOutliers=dfFixOutliers.loc[filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x2160 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (10, 30))\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "# for x in range (0,10): \n",
    "for x in range (10,12): \n",
    "    index = x%10\n",
    "    ax=plt.subplot(5,2,index+1)\n",
    "    plt.boxplot(dfFixOutliers[outlierframe[x]])\n",
    "    ax.set_title(outlierframe[x])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "      <th>cat_8</th>\n",
       "      <th>cat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_17</th>\n",
       "      <th>cat_18</th>\n",
       "      <th>cat_19</th>\n",
       "      <th>cat_20</th>\n",
       "      <th>cat_21</th>\n",
       "      <th>cat_22</th>\n",
       "      <th>cat_23</th>\n",
       "      <th>cat_24</th>\n",
       "      <th>cat_25</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.980142</td>\n",
       "      <td>1.557762</td>\n",
       "      <td>-1.134152</td>\n",
       "      <td>-1.482144</td>\n",
       "      <td>-1.273575</td>\n",
       "      <td>-1.194822</td>\n",
       "      <td>-0.996246</td>\n",
       "      <td>-0.673919</td>\n",
       "      <td>-0.730094</td>\n",
       "      <td>-0.576136</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.558060</td>\n",
       "      <td>-0.601064</td>\n",
       "      <td>-0.431706</td>\n",
       "      <td>-0.982956</td>\n",
       "      <td>-1.157560</td>\n",
       "      <td>-1.799805</td>\n",
       "      <td>1.076145</td>\n",
       "      <td>0.039481</td>\n",
       "      <td>0.679935</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.248937</td>\n",
       "      <td>1.522809</td>\n",
       "      <td>-1.345525</td>\n",
       "      <td>-1.393379</td>\n",
       "      <td>-1.349129</td>\n",
       "      <td>-1.095365</td>\n",
       "      <td>-1.055516</td>\n",
       "      <td>-0.800330</td>\n",
       "      <td>-0.770404</td>\n",
       "      <td>-0.613393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.824485</td>\n",
       "      <td>-0.690527</td>\n",
       "      <td>-0.640220</td>\n",
       "      <td>-1.080771</td>\n",
       "      <td>-1.363526</td>\n",
       "      <td>-1.673845</td>\n",
       "      <td>1.192847</td>\n",
       "      <td>0.032540</td>\n",
       "      <td>0.817749</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.038764</td>\n",
       "      <td>0.905593</td>\n",
       "      <td>0.663269</td>\n",
       "      <td>-0.361342</td>\n",
       "      <td>-0.399633</td>\n",
       "      <td>-0.575981</td>\n",
       "      <td>-0.617926</td>\n",
       "      <td>-0.857086</td>\n",
       "      <td>-0.984167</td>\n",
       "      <td>-1.182897</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.147469</td>\n",
       "      <td>-0.817265</td>\n",
       "      <td>-0.777244</td>\n",
       "      <td>-0.948433</td>\n",
       "      <td>-1.104407</td>\n",
       "      <td>-0.769732</td>\n",
       "      <td>1.149554</td>\n",
       "      <td>0.271705</td>\n",
       "      <td>1.256284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.913184</td>\n",
       "      <td>0.745321</td>\n",
       "      <td>0.730728</td>\n",
       "      <td>-0.205116</td>\n",
       "      <td>-0.293632</td>\n",
       "      <td>-0.553879</td>\n",
       "      <td>-0.446421</td>\n",
       "      <td>-0.684238</td>\n",
       "      <td>-0.864460</td>\n",
       "      <td>-0.963612</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.032862</td>\n",
       "      <td>-0.621939</td>\n",
       "      <td>-0.670008</td>\n",
       "      <td>-0.642041</td>\n",
       "      <td>-0.908407</td>\n",
       "      <td>-0.873299</td>\n",
       "      <td>1.079910</td>\n",
       "      <td>0.213649</td>\n",
       "      <td>1.133674</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.603234</td>\n",
       "      <td>0.383004</td>\n",
       "      <td>-2.483341</td>\n",
       "      <td>-1.972125</td>\n",
       "      <td>-1.574663</td>\n",
       "      <td>-1.377773</td>\n",
       "      <td>-1.382133</td>\n",
       "      <td>-1.049283</td>\n",
       "      <td>-0.792391</td>\n",
       "      <td>-0.362173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.433034</td>\n",
       "      <td>-1.024520</td>\n",
       "      <td>-0.942566</td>\n",
       "      <td>-0.735541</td>\n",
       "      <td>-0.400136</td>\n",
       "      <td>0.624224</td>\n",
       "      <td>-0.278157</td>\n",
       "      <td>0.503928</td>\n",
       "      <td>-0.055398</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3243</th>\n",
       "      <td>-0.114973</td>\n",
       "      <td>0.770044</td>\n",
       "      <td>-0.506029</td>\n",
       "      <td>-0.414601</td>\n",
       "      <td>-0.551868</td>\n",
       "      <td>-0.616500</td>\n",
       "      <td>-0.728900</td>\n",
       "      <td>-0.808070</td>\n",
       "      <td>-0.901105</td>\n",
       "      <td>-0.924225</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.663737</td>\n",
       "      <td>-0.902255</td>\n",
       "      <td>-0.716179</td>\n",
       "      <td>-0.876510</td>\n",
       "      <td>-0.278882</td>\n",
       "      <td>0.923729</td>\n",
       "      <td>0.838037</td>\n",
       "      <td>1.911783</td>\n",
       "      <td>0.786115</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3244</th>\n",
       "      <td>-0.046037</td>\n",
       "      <td>0.910708</td>\n",
       "      <td>0.170065</td>\n",
       "      <td>-0.154225</td>\n",
       "      <td>-0.506761</td>\n",
       "      <td>-0.741742</td>\n",
       "      <td>-0.836090</td>\n",
       "      <td>-0.920292</td>\n",
       "      <td>-0.925535</td>\n",
       "      <td>-0.960418</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.802159</td>\n",
       "      <td>-0.841122</td>\n",
       "      <td>-0.826394</td>\n",
       "      <td>-0.585941</td>\n",
       "      <td>-0.212441</td>\n",
       "      <td>1.575922</td>\n",
       "      <td>1.038500</td>\n",
       "      <td>1.403794</td>\n",
       "      <td>0.785380</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3245</th>\n",
       "      <td>-0.769594</td>\n",
       "      <td>-0.278543</td>\n",
       "      <td>-0.798353</td>\n",
       "      <td>-0.383829</td>\n",
       "      <td>-0.144780</td>\n",
       "      <td>-0.238319</td>\n",
       "      <td>-0.134937</td>\n",
       "      <td>-0.045733</td>\n",
       "      <td>0.043118</td>\n",
       "      <td>0.031690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117677</td>\n",
       "      <td>-0.358024</td>\n",
       "      <td>-0.239575</td>\n",
       "      <td>-0.102619</td>\n",
       "      <td>-0.117763</td>\n",
       "      <td>0.002822</td>\n",
       "      <td>-0.263098</td>\n",
       "      <td>-0.129638</td>\n",
       "      <td>0.175027</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246</th>\n",
       "      <td>-0.145967</td>\n",
       "      <td>0.429040</td>\n",
       "      <td>-0.612465</td>\n",
       "      <td>-0.461942</td>\n",
       "      <td>-0.613890</td>\n",
       "      <td>-0.573525</td>\n",
       "      <td>-0.543523</td>\n",
       "      <td>-0.676499</td>\n",
       "      <td>-0.610387</td>\n",
       "      <td>-0.698553</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.628015</td>\n",
       "      <td>-0.875416</td>\n",
       "      <td>-0.877033</td>\n",
       "      <td>-0.591695</td>\n",
       "      <td>-0.278882</td>\n",
       "      <td>0.680207</td>\n",
       "      <td>0.573576</td>\n",
       "      <td>1.216375</td>\n",
       "      <td>0.544737</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>-0.423313</td>\n",
       "      <td>0.163057</td>\n",
       "      <td>-0.434072</td>\n",
       "      <td>-0.334121</td>\n",
       "      <td>-0.342122</td>\n",
       "      <td>-0.519499</td>\n",
       "      <td>-0.194207</td>\n",
       "      <td>-0.250829</td>\n",
       "      <td>-0.135222</td>\n",
       "      <td>-0.124791</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.279728</td>\n",
       "      <td>-0.709910</td>\n",
       "      <td>-0.558304</td>\n",
       "      <td>-0.676564</td>\n",
       "      <td>-0.511424</td>\n",
       "      <td>-0.456232</td>\n",
       "      <td>-0.016519</td>\n",
       "      <td>0.673048</td>\n",
       "      <td>0.231019</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3057 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cat_0     cat_1     cat_2     cat_3     cat_4     cat_5     cat_6  \\\n",
       "0    -0.980142  1.557762 -1.134152 -1.482144 -1.273575 -1.194822 -0.996246   \n",
       "1    -1.248937  1.522809 -1.345525 -1.393379 -1.349129 -1.095365 -1.055516   \n",
       "2     1.038764  0.905593  0.663269 -0.361342 -0.399633 -0.575981 -0.617926   \n",
       "3     0.913184  0.745321  0.730728 -0.205116 -0.293632 -0.553879 -0.446421   \n",
       "4    -1.603234  0.383004 -2.483341 -1.972125 -1.574663 -1.377773 -1.382133   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3243 -0.114973  0.770044 -0.506029 -0.414601 -0.551868 -0.616500 -0.728900   \n",
       "3244 -0.046037  0.910708  0.170065 -0.154225 -0.506761 -0.741742 -0.836090   \n",
       "3245 -0.769594 -0.278543 -0.798353 -0.383829 -0.144780 -0.238319 -0.134937   \n",
       "3246 -0.145967  0.429040 -0.612465 -0.461942 -0.613890 -0.573525 -0.543523   \n",
       "3247 -0.423313  0.163057 -0.434072 -0.334121 -0.342122 -0.519499 -0.194207   \n",
       "\n",
       "         cat_7     cat_8     cat_9  ...    cat_17    cat_18    cat_19  \\\n",
       "0    -0.673919 -0.730094 -0.576136  ... -0.558060 -0.601064 -0.431706   \n",
       "1    -0.800330 -0.770404 -0.613393  ... -0.824485 -0.690527 -0.640220   \n",
       "2    -0.857086 -0.984167 -1.182897  ... -1.147469 -0.817265 -0.777244   \n",
       "3    -0.684238 -0.864460 -0.963612  ... -1.032862 -0.621939 -0.670008   \n",
       "4    -1.049283 -0.792391 -0.362173  ... -0.433034 -1.024520 -0.942566   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3243 -0.808070 -0.901105 -0.924225  ... -0.663737 -0.902255 -0.716179   \n",
       "3244 -0.920292 -0.925535 -0.960418  ... -0.802159 -0.841122 -0.826394   \n",
       "3245 -0.045733  0.043118  0.031690  ...  0.117677 -0.358024 -0.239575   \n",
       "3246 -0.676499 -0.610387 -0.698553  ... -0.628015 -0.875416 -0.877033   \n",
       "3247 -0.250829 -0.135222 -0.124791  ... -0.279728 -0.709910 -0.558304   \n",
       "\n",
       "        cat_20    cat_21    cat_22    cat_23    cat_24    cat_25  category  \n",
       "0    -0.982956 -1.157560 -1.799805  1.076145  0.039481  0.679935         0  \n",
       "1    -1.080771 -1.363526 -1.673845  1.192847  0.032540  0.817749         0  \n",
       "2    -0.948433 -1.104407 -0.769732  1.149554  0.271705  1.256284         0  \n",
       "3    -0.642041 -0.908407 -0.873299  1.079910  0.213649  1.133674         0  \n",
       "4    -0.735541 -0.400136  0.624224 -0.278157  0.503928 -0.055398         0  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3243 -0.876510 -0.278882  0.923729  0.838037  1.911783  0.786115         7  \n",
       "3244 -0.585941 -0.212441  1.575922  1.038500  1.403794  0.785380         7  \n",
       "3245 -0.102619 -0.117763  0.002822 -0.263098 -0.129638  0.175027         7  \n",
       "3246 -0.591695 -0.278882  0.680207  0.573576  1.216375  0.544737         7  \n",
       "3247 -0.676564 -0.511424 -0.456232 -0.016519  0.673048  0.231019         7  \n",
       "\n",
       "[3057 rows x 27 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "d_pre = dfFixOutliers\n",
    "variable_cat2 = [\"cat_0\",\"cat_1\",\"cat_2\",\"cat_3\",\"cat_4\",\"cat_5\",\"cat_6\",\"cat_7\",\"cat_8\",\"cat_9\",\"cat_10\",\"cat_11\",\"cat_12\",\"cat_13\",\"cat_14\",\"cat_15\",\"cat_16\",\"cat_17\",\"cat_18\",\"cat_19\",\"cat_20\",\"cat_21\",\"cat_22\",\"cat_23\",\"cat_24\",\"cat_25\"]\n",
    "d_pre[variable_cat2] = StandardScaler().fit_transform(d_pre[variable_cat2])\n",
    "d_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X Y Partition ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "3243    7\n",
       "3244    7\n",
       "3245    7\n",
       "3246    7\n",
       "3247    7\n",
       "Name: category, Length: 3057, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_norm=d_pre.drop(['category'],axis=1)\n",
    "\n",
    "y_norm=d_pre['category']\n",
    "y_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting DataSet ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Training: (3057, 26)\n",
      "Input Training: (2445, 26)\n",
      "Input Test: (612, 26)\n",
      "Output Training: (2445,)\n",
      "Output Test: (612,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_norm, x_test_norm, y_train_norm, y_test_norm = train_test_split(x_norm, y_norm,train_size=0.8,random_state=1)\n",
    "print(\"Input Training:\",x_norm.shape)\n",
    "print(\"Input Training:\",x_train_norm.shape)\n",
    "print(\"Input Test:\",x_test_norm.shape)\n",
    "print(\"Output Training:\",y_train_norm.shape)\n",
    "print(\"Output Test:\",y_test_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13894881, 0.13554041, 0.14451803, ..., 0.1257676 , 0.14052584,\n",
       "        0.1420485 ],\n",
       "       [0.13970541, 0.15870533, 0.13896371, ..., 0.12100018, 0.14606553,\n",
       "        0.14626277],\n",
       "       [0.1866264 , 0.09315282, 0.14212265, ..., 0.17632458, 0.13669617,\n",
       "        0.14564282],\n",
       "       ...,\n",
       "       [0.12406234, 0.11590304, 0.09810103, ..., 0.14030798, 0.1387998 ,\n",
       "        0.14043417],\n",
       "       [0.12953965, 0.10415269, 0.06502889, ..., 0.13878207, 0.13954496,\n",
       "        0.13969166],\n",
       "       [0.14063617, 0.11885668, 0.00667858, ..., 0.14991912, 0.14177795,\n",
       "        0.14387573]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_mod = AdaBoostClassifier(n_estimators=300, learning_rate=0.2)\n",
    "\n",
    "ada_mod.fit(x_train_norm, y_train_norm)\n",
    "\n",
    "x_adaptiveresult_norm = ada_mod.predict_proba(x_test_norm)\n",
    "x_adaptiveresult_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.138949</td>\n",
       "      <td>0.135540</td>\n",
       "      <td>0.144518</td>\n",
       "      <td>0.065245</td>\n",
       "      <td>0.107406</td>\n",
       "      <td>0.125768</td>\n",
       "      <td>0.140526</td>\n",
       "      <td>0.142049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.139705</td>\n",
       "      <td>0.158705</td>\n",
       "      <td>0.138964</td>\n",
       "      <td>0.138982</td>\n",
       "      <td>0.010315</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>0.146066</td>\n",
       "      <td>0.146263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.186626</td>\n",
       "      <td>0.093153</td>\n",
       "      <td>0.142123</td>\n",
       "      <td>0.057279</td>\n",
       "      <td>0.062155</td>\n",
       "      <td>0.176325</td>\n",
       "      <td>0.136696</td>\n",
       "      <td>0.145643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.138909</td>\n",
       "      <td>0.142066</td>\n",
       "      <td>0.142803</td>\n",
       "      <td>0.064979</td>\n",
       "      <td>0.105890</td>\n",
       "      <td>0.126273</td>\n",
       "      <td>0.137946</td>\n",
       "      <td>0.141135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.180807</td>\n",
       "      <td>0.089096</td>\n",
       "      <td>0.143604</td>\n",
       "      <td>0.054959</td>\n",
       "      <td>0.081390</td>\n",
       "      <td>0.171820</td>\n",
       "      <td>0.135867</td>\n",
       "      <td>0.142457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>0.136943</td>\n",
       "      <td>0.131604</td>\n",
       "      <td>0.007320</td>\n",
       "      <td>0.144416</td>\n",
       "      <td>0.152650</td>\n",
       "      <td>0.148261</td>\n",
       "      <td>0.137977</td>\n",
       "      <td>0.140828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>0.140794</td>\n",
       "      <td>0.119536</td>\n",
       "      <td>0.006724</td>\n",
       "      <td>0.145989</td>\n",
       "      <td>0.153062</td>\n",
       "      <td>0.150111</td>\n",
       "      <td>0.140604</td>\n",
       "      <td>0.143179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>0.124062</td>\n",
       "      <td>0.115903</td>\n",
       "      <td>0.098101</td>\n",
       "      <td>0.099789</td>\n",
       "      <td>0.142603</td>\n",
       "      <td>0.140308</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>0.140434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>0.129540</td>\n",
       "      <td>0.104153</td>\n",
       "      <td>0.065029</td>\n",
       "      <td>0.140609</td>\n",
       "      <td>0.142652</td>\n",
       "      <td>0.138782</td>\n",
       "      <td>0.139545</td>\n",
       "      <td>0.139692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>0.140636</td>\n",
       "      <td>0.118857</td>\n",
       "      <td>0.006679</td>\n",
       "      <td>0.145768</td>\n",
       "      <td>0.152488</td>\n",
       "      <td>0.149919</td>\n",
       "      <td>0.141778</td>\n",
       "      <td>0.143876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat_0     cat_1     cat_2     cat_3     cat_4     cat_5     cat_6  \\\n",
       "0    0.138949  0.135540  0.144518  0.065245  0.107406  0.125768  0.140526   \n",
       "1    0.139705  0.158705  0.138964  0.138982  0.010315  0.121000  0.146066   \n",
       "2    0.186626  0.093153  0.142123  0.057279  0.062155  0.176325  0.136696   \n",
       "3    0.138909  0.142066  0.142803  0.064979  0.105890  0.126273  0.137946   \n",
       "4    0.180807  0.089096  0.143604  0.054959  0.081390  0.171820  0.135867   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "607  0.136943  0.131604  0.007320  0.144416  0.152650  0.148261  0.137977   \n",
       "608  0.140794  0.119536  0.006724  0.145989  0.153062  0.150111  0.140604   \n",
       "609  0.124062  0.115903  0.098101  0.099789  0.142603  0.140308  0.138800   \n",
       "610  0.129540  0.104153  0.065029  0.140609  0.142652  0.138782  0.139545   \n",
       "611  0.140636  0.118857  0.006679  0.145768  0.152488  0.149919  0.141778   \n",
       "\n",
       "        cat_7  \n",
       "0    0.142049  \n",
       "1    0.146263  \n",
       "2    0.145643  \n",
       "3    0.141135  \n",
       "4    0.142457  \n",
       "..        ...  \n",
       "607  0.140828  \n",
       "608  0.143179  \n",
       "609  0.140434  \n",
       "610  0.139692  \n",
       "611  0.143876  \n",
       "\n",
       "[612 rows x 8 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_adaptivetest = np.array(ada_mod.predict_proba(x_test_norm))\n",
    "dataset_test_norm = pd.DataFrame(x_adaptivetest, columns=x_norm.columns[:8])\n",
    "dataset_test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.139984</td>\n",
       "      <td>0.139079</td>\n",
       "      <td>0.007996</td>\n",
       "      <td>0.133946</td>\n",
       "      <td>0.156154</td>\n",
       "      <td>0.143755</td>\n",
       "      <td>0.137996</td>\n",
       "      <td>0.141089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.128749</td>\n",
       "      <td>0.104035</td>\n",
       "      <td>0.066691</td>\n",
       "      <td>0.140033</td>\n",
       "      <td>0.142260</td>\n",
       "      <td>0.138970</td>\n",
       "      <td>0.139720</td>\n",
       "      <td>0.139544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.140292</td>\n",
       "      <td>0.157408</td>\n",
       "      <td>0.123776</td>\n",
       "      <td>0.145038</td>\n",
       "      <td>0.010716</td>\n",
       "      <td>0.123567</td>\n",
       "      <td>0.149583</td>\n",
       "      <td>0.149620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.138251</td>\n",
       "      <td>0.140309</td>\n",
       "      <td>0.008849</td>\n",
       "      <td>0.134354</td>\n",
       "      <td>0.155794</td>\n",
       "      <td>0.144642</td>\n",
       "      <td>0.137699</td>\n",
       "      <td>0.140102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.138173</td>\n",
       "      <td>0.130556</td>\n",
       "      <td>0.007671</td>\n",
       "      <td>0.142072</td>\n",
       "      <td>0.154883</td>\n",
       "      <td>0.146613</td>\n",
       "      <td>0.139057</td>\n",
       "      <td>0.140975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>0.137072</td>\n",
       "      <td>0.127302</td>\n",
       "      <td>0.007566</td>\n",
       "      <td>0.144543</td>\n",
       "      <td>0.153423</td>\n",
       "      <td>0.148356</td>\n",
       "      <td>0.140132</td>\n",
       "      <td>0.141606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>0.137689</td>\n",
       "      <td>0.129132</td>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.142704</td>\n",
       "      <td>0.155394</td>\n",
       "      <td>0.146922</td>\n",
       "      <td>0.139421</td>\n",
       "      <td>0.141056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>0.143431</td>\n",
       "      <td>0.149271</td>\n",
       "      <td>0.009686</td>\n",
       "      <td>0.124668</td>\n",
       "      <td>0.147560</td>\n",
       "      <td>0.139515</td>\n",
       "      <td>0.141249</td>\n",
       "      <td>0.144619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>0.136255</td>\n",
       "      <td>0.132062</td>\n",
       "      <td>0.008559</td>\n",
       "      <td>0.142017</td>\n",
       "      <td>0.154552</td>\n",
       "      <td>0.146966</td>\n",
       "      <td>0.139180</td>\n",
       "      <td>0.140409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>0.139784</td>\n",
       "      <td>0.118020</td>\n",
       "      <td>0.006522</td>\n",
       "      <td>0.147130</td>\n",
       "      <td>0.154383</td>\n",
       "      <td>0.149867</td>\n",
       "      <td>0.141058</td>\n",
       "      <td>0.143236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2445 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cat_0     cat_1     cat_2     cat_3     cat_4     cat_5     cat_6  \\\n",
       "0     0.139984  0.139079  0.007996  0.133946  0.156154  0.143755  0.137996   \n",
       "1     0.128749  0.104035  0.066691  0.140033  0.142260  0.138970  0.139720   \n",
       "2     0.140292  0.157408  0.123776  0.145038  0.010716  0.123567  0.149583   \n",
       "3     0.138251  0.140309  0.008849  0.134354  0.155794  0.144642  0.137699   \n",
       "4     0.138173  0.130556  0.007671  0.142072  0.154883  0.146613  0.139057   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2440  0.137072  0.127302  0.007566  0.144543  0.153423  0.148356  0.140132   \n",
       "2441  0.137689  0.129132  0.007682  0.142704  0.155394  0.146922  0.139421   \n",
       "2442  0.143431  0.149271  0.009686  0.124668  0.147560  0.139515  0.141249   \n",
       "2443  0.136255  0.132062  0.008559  0.142017  0.154552  0.146966  0.139180   \n",
       "2444  0.139784  0.118020  0.006522  0.147130  0.154383  0.149867  0.141058   \n",
       "\n",
       "         cat_7  \n",
       "0     0.141089  \n",
       "1     0.139544  \n",
       "2     0.149620  \n",
       "3     0.140102  \n",
       "4     0.140975  \n",
       "...        ...  \n",
       "2440  0.141606  \n",
       "2441  0.141056  \n",
       "2442  0.144619  \n",
       "2443  0.140409  \n",
       "2444  0.143236  \n",
       "\n",
       "[2445 rows x 8 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_adaptivetrain = np.array(ada_mod.predict_proba(x_train_norm))\n",
    "dataset_train_norm = pd.DataFrame(x_adaptivetrain, columns=x_norm.columns[:8])\n",
    "dataset_train_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.30      0.36        84\n",
      "           1       0.62      0.37      0.46        41\n",
      "           2       0.28      0.76      0.41        46\n",
      "           3       0.13      0.23      0.17        40\n",
      "           4       0.17      0.51      0.26        68\n",
      "           5       0.60      0.21      0.31       117\n",
      "           6       0.36      0.07      0.12       115\n",
      "           7       0.21      0.15      0.17       101\n",
      "\n",
      "    accuracy                           0.27       612\n",
      "   macro avg       0.36      0.32      0.28       612\n",
      "weighted avg       0.37      0.27      0.26       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = GaussianNB()\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(dataset_train_norm, y_train_norm)\n",
    "y_pred = nbtrain.predict(dataset_test_norm)\n",
    "print(classification_report(y_test_norm, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.40      0.55        84\n",
      "           1       0.68      0.56      0.61        41\n",
      "           2       0.32      0.76      0.45        46\n",
      "           3       0.00      0.00      0.00        40\n",
      "           4       0.22      0.16      0.19        68\n",
      "           5       0.59      0.29      0.39       117\n",
      "           6       0.26      0.13      0.17       115\n",
      "           7       0.21      0.55      0.31       101\n",
      "\n",
      "    accuracy                           0.34       612\n",
      "   macro avg       0.39      0.36      0.33       612\n",
      "weighted avg       0.41      0.34      0.33       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = svm.SVC()\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(dataset_train_norm, y_train_norm)\n",
    "y_pred = nbtrain.predict(dataset_test_norm)\n",
    "print(classification_report(y_test_norm, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.44      0.45        84\n",
      "           1       0.55      0.59      0.56        41\n",
      "           2       0.37      0.48      0.42        46\n",
      "           3       0.14      0.15      0.14        40\n",
      "           4       0.37      0.31      0.34        68\n",
      "           5       0.41      0.39      0.40       117\n",
      "           6       0.40      0.41      0.41       115\n",
      "           7       0.32      0.33      0.33       101\n",
      "\n",
      "    accuracy                           0.39       612\n",
      "   macro avg       0.38      0.39      0.38       612\n",
      "weighted avg       0.39      0.39      0.39       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = DecisionTreeClassifier(random_state=0)\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(dataset_train_norm, y_train_norm)\n",
    "y_pred = nbtrain.predict(dataset_test_norm)\n",
    "print(classification_report(y_test_norm, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 neighbors accuracy : 0.4199346405228758\n",
      "2 neighbors accuracy : 0.39869281045751637\n",
      "3 neighbors accuracy : 0.4264705882352941\n",
      "4 neighbors accuracy : 0.43137254901960786\n",
      "5 neighbors accuracy : 0.44281045751633985\n",
      "6 neighbors accuracy : 0.4493464052287582\n",
      "7 neighbors accuracy : 0.4444444444444444\n",
      "8 neighbors accuracy : 0.4477124183006536\n",
      "9 neighbors accuracy : 0.42320261437908496\n",
      "10 neighbors accuracy : 0.43137254901960786\n",
      "11 neighbors accuracy : 0.4297385620915033\n",
      "12 neighbors accuracy : 0.4297385620915033\n",
      "13 neighbors accuracy : 0.4526143790849673\n",
      "14 neighbors accuracy : 0.4411764705882353\n",
      "15 neighbors accuracy : 0.434640522875817\n",
      "16 neighbors accuracy : 0.43790849673202614\n",
      "17 neighbors accuracy : 0.4215686274509804\n",
      "18 neighbors accuracy : 0.44281045751633985\n",
      "19 neighbors accuracy : 0.4444444444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score  \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "for i in range (1,20) :\n",
    "    # Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "    modelnb = KNeighborsClassifier(n_neighbors=i)\n",
    "    # Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "    nbtrain = modelnb.fit(dataset_train_norm, y_train_norm)\n",
    "    y_pred = nbtrain.predict(dataset_test_norm)\n",
    "    print(str(i) + \" neighbors accuracy : \" + str(accuracy_score(y_test_norm, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 neighbors accuracy : 0.4493464052287582\n"
     ]
    }
   ],
   "source": [
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = KNeighborsClassifier(n_neighbors=6)\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(dataset_train_norm, y_train_norm)\n",
    "y_pred = nbtrain.predict(dataset_test_norm)\n",
    "print(str(i) + \" neighbors accuracy : \" + str(accuracy_score(y_test_norm, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "49/49 [==============================] - 1s 2ms/step - loss: 0.5462 - accuracy: 0.0789\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -0.3446 - accuracy: 0.0806\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -2.5904 - accuracy: 0.0806\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -5.0460 - accuracy: 0.0806\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -6.9355 - accuracy: 0.0806\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -8.4846 - accuracy: 0.0806\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -9.8665 - accuracy: 0.0806\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -11.1582 - accuracy: 0.0806\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -12.3961 - accuracy: 0.0806\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -13.5937 - accuracy: 0.0806\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -14.7641 - accuracy: 0.0806\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -15.9178 - accuracy: 0.0806\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -17.0513 - accuracy: 0.0806\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - 0s 3ms/step - loss: -18.1779 - accuracy: 0.0806\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -19.2929 - accuracy: 0.0806\n",
      "Epoch 16/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -20.3969 - accuracy: 0.0806\n",
      "Epoch 17/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -21.4981 - accuracy: 0.0806\n",
      "Epoch 18/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -22.5946 - accuracy: 0.0806\n",
      "Epoch 19/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -23.6827 - accuracy: 0.0806\n",
      "Epoch 20/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -24.7665 - accuracy: 0.0806\n",
      "Epoch 21/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -25.8506 - accuracy: 0.0806\n",
      "Epoch 22/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -26.9292 - accuracy: 0.0806\n",
      "Epoch 23/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -28.0058 - accuracy: 0.0806\n",
      "Epoch 24/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -29.0765 - accuracy: 0.0806\n",
      "Epoch 25/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -30.1508 - accuracy: 0.0806\n",
      "Epoch 26/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -31.2163 - accuracy: 0.0806\n",
      "Epoch 27/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -32.2850 - accuracy: 0.0806\n",
      "Epoch 28/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -33.3472 - accuracy: 0.0806\n",
      "Epoch 29/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -34.4128 - accuracy: 0.0806\n",
      "Epoch 30/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -35.4736 - accuracy: 0.0806\n",
      "Epoch 31/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -36.5345 - accuracy: 0.0806\n",
      "Epoch 32/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -37.5939 - accuracy: 0.0806\n",
      "Epoch 33/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -38.6523 - accuracy: 0.0806\n",
      "Epoch 34/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -39.7093 - accuracy: 0.0806\n",
      "Epoch 35/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -40.7655 - accuracy: 0.0806\n",
      "Epoch 36/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -41.8223 - accuracy: 0.0806\n",
      "Epoch 37/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -42.8776 - accuracy: 0.0806\n",
      "Epoch 38/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -43.9329 - accuracy: 0.0806\n",
      "Epoch 39/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -44.9841 - accuracy: 0.0806\n",
      "Epoch 40/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -46.0405 - accuracy: 0.0806\n",
      "Epoch 41/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -47.0896 - accuracy: 0.0806\n",
      "Epoch 42/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -48.1439 - accuracy: 0.0806\n",
      "Epoch 43/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -49.1954 - accuracy: 0.0806\n",
      "Epoch 44/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -50.2440 - accuracy: 0.0806\n",
      "Epoch 45/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -51.2966 - accuracy: 0.0806\n",
      "Epoch 46/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -52.3460 - accuracy: 0.0806\n",
      "Epoch 47/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -53.3947 - accuracy: 0.0806\n",
      "Epoch 48/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -54.4428 - accuracy: 0.0806\n",
      "Epoch 49/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -55.4927 - accuracy: 0.0806\n",
      "Epoch 50/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -56.5413 - accuracy: 0.0806\n",
      "Epoch 51/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -57.5895 - accuracy: 0.0806\n",
      "Epoch 52/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -58.6337 - accuracy: 0.0806\n",
      "Epoch 53/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -59.6868 - accuracy: 0.0806\n",
      "Epoch 54/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -60.7288 - accuracy: 0.0806\n",
      "Epoch 55/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -61.7756 - accuracy: 0.0806\n",
      "Epoch 56/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -62.8245 - accuracy: 0.0806\n",
      "Epoch 57/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -63.8686 - accuracy: 0.0806\n",
      "Epoch 58/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -64.9132 - accuracy: 0.0806\n",
      "Epoch 59/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -65.9616 - accuracy: 0.0806\n",
      "Epoch 60/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -67.0048 - accuracy: 0.0806\n",
      "Epoch 61/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -68.0497 - accuracy: 0.0806\n",
      "Epoch 62/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -69.0969 - accuracy: 0.0806\n",
      "Epoch 63/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -70.1391 - accuracy: 0.0806\n",
      "Epoch 64/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -71.1841 - accuracy: 0.0806\n",
      "Epoch 65/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -72.2274 - accuracy: 0.0806\n",
      "Epoch 66/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -73.2731 - accuracy: 0.0806\n",
      "Epoch 67/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -74.3180 - accuracy: 0.0806\n",
      "Epoch 68/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -75.3591 - accuracy: 0.0806\n",
      "Epoch 69/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -76.4038 - accuracy: 0.0806\n",
      "Epoch 70/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -77.4474 - accuracy: 0.0806\n",
      "Epoch 71/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -78.4925 - accuracy: 0.0806\n",
      "Epoch 72/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -79.5347 - accuracy: 0.0806\n",
      "Epoch 73/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -80.5784 - accuracy: 0.0806\n",
      "Epoch 74/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -81.6216 - accuracy: 0.0806\n",
      "Epoch 75/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -82.6671 - accuracy: 0.0806\n",
      "Epoch 76/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -83.7079 - accuracy: 0.0806\n",
      "Epoch 77/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -84.7529 - accuracy: 0.0806\n",
      "Epoch 78/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -85.7967 - accuracy: 0.0806\n",
      "Epoch 79/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -86.8378 - accuracy: 0.0806\n",
      "Epoch 80/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -87.8820 - accuracy: 0.0806\n",
      "Epoch 81/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -88.9254 - accuracy: 0.0806\n",
      "Epoch 82/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -89.9691 - accuracy: 0.0806\n",
      "Epoch 83/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -91.0106 - accuracy: 0.0806\n",
      "Epoch 84/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -92.0574 - accuracy: 0.0806\n",
      "Epoch 85/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -93.0955 - accuracy: 0.0806\n",
      "Epoch 86/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -94.1428 - accuracy: 0.0806\n",
      "Epoch 87/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -95.1829 - accuracy: 0.0806\n",
      "Epoch 88/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -96.2286 - accuracy: 0.0806\n",
      "Epoch 89/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -97.2695 - accuracy: 0.0806\n",
      "Epoch 90/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -98.3128 - accuracy: 0.0806\n",
      "Epoch 91/100\n",
      "49/49 [==============================] - 0s 1ms/step - loss: -99.3556 - accuracy: 0.0806\n",
      "Epoch 92/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -100.3986 - accuracy: 0.0806\n",
      "Epoch 93/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -101.4405 - accuracy: 0.0806\n",
      "Epoch 94/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -102.4838 - accuracy: 0.0806\n",
      "Epoch 95/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -103.5259 - accuracy: 0.0806\n",
      "Epoch 96/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -104.5700 - accuracy: 0.0806\n",
      "Epoch 97/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -105.6122 - accuracy: 0.0806\n",
      "Epoch 98/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -106.6517 - accuracy: 0.0806\n",
      "Epoch 99/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -107.6971 - accuracy: 0.0806\n",
      "Epoch 100/100\n",
      "49/49 [==============================] - 0s 2ms/step - loss: -108.7383 - accuracy: 0.0806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aab4526460>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units=10,kernel_initializer='uniform' , activation='tanh' , input_dim=8))\n",
    "classifier.add(Dense(units=6,kernel_initializer='uniform' , activation='tanh'))\n",
    "classifier.add(Dense(units=1 , kernel_initializer='uniform' , activation='sigmoid'))\n",
    "classifier.compile(optimizer='adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "classifier.fit(dataset_train_norm, y_train_norm, batch_size = 50, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the function\n",
    "# ResultsData = FunctionFindBestParams(dataset_train_norm, y_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        84\n",
      "           1       0.07      1.00      0.13        41\n",
      "           2       0.00      0.00      0.00        46\n",
      "           3       0.00      0.00      0.00        40\n",
      "           4       0.00      0.00      0.00        68\n",
      "           5       0.00      0.00      0.00       117\n",
      "           6       0.00      0.00      0.00       115\n",
      "           7       0.00      0.00      0.00       101\n",
      "\n",
      "    accuracy                           0.07       612\n",
      "   macro avg       0.01      0.12      0.02       612\n",
      "weighted avg       0.00      0.07      0.01       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = classifier.predict(dataset_test_norm)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "print(classification_report(y_test_norm, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adaboost + Normalisasi + Imbalanced Overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over Fiting ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After oversampling:  Counter({6: 457, 7: 457, 0: 457, 5: 457, 3: 457, 4: 457, 1: 457, 2: 457})\n",
      "Input Training: (3057, 26)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "# define oversampling strategy\n",
    "\n",
    "SMOTE = SMOTE()\n",
    "\n",
    "# fit and apply the transform\n",
    "X_train_SMOTE, y_train_SMOTE = SMOTE.fit_resample(x_train_norm, y_train_norm)\n",
    "\n",
    "# summarize class distribution\n",
    "print(\"After oversampling: \",Counter(y_train_SMOTE))\n",
    "print(\"Input Training:\",x_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13960339, 0.13403877, 0.14670116, ..., 0.13074066, 0.14151108,\n",
       "        0.14176488],\n",
       "       [0.14001237, 0.16015971, 0.13823728, ..., 0.12017432, 0.14843374,\n",
       "        0.14619446],\n",
       "       [0.16459927, 0.12276871, 0.15700108, ..., 0.15408614, 0.1384815 ,\n",
       "        0.1448402 ],\n",
       "       ...,\n",
       "       [0.12489518, 0.1178771 , 0.10120554, ..., 0.14105492, 0.1392559 ,\n",
       "        0.14086724],\n",
       "       [0.1290428 , 0.10417833, 0.06127265, ..., 0.13853951, 0.13914975,\n",
       "        0.13945209],\n",
       "       [0.13716796, 0.10689234, 0.00172571, ..., 0.13713411, 0.13187657,\n",
       "        0.1314149 ]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_mod = AdaBoostClassifier(n_estimators=300, learning_rate=0.2)\n",
    "\n",
    "ada_mod.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "\n",
    "x_adaptiveresult_smote = ada_mod.predict_proba(x_test_norm)\n",
    "x_adaptiveresult_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.139603</td>\n",
       "      <td>0.134039</td>\n",
       "      <td>0.146701</td>\n",
       "      <td>0.060896</td>\n",
       "      <td>0.104744</td>\n",
       "      <td>0.130741</td>\n",
       "      <td>0.141511</td>\n",
       "      <td>0.141765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.140012</td>\n",
       "      <td>0.160160</td>\n",
       "      <td>0.138237</td>\n",
       "      <td>0.139243</td>\n",
       "      <td>0.007545</td>\n",
       "      <td>0.120174</td>\n",
       "      <td>0.148434</td>\n",
       "      <td>0.146194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.164599</td>\n",
       "      <td>0.122769</td>\n",
       "      <td>0.157001</td>\n",
       "      <td>0.058148</td>\n",
       "      <td>0.060075</td>\n",
       "      <td>0.154086</td>\n",
       "      <td>0.138482</td>\n",
       "      <td>0.144840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.136849</td>\n",
       "      <td>0.143855</td>\n",
       "      <td>0.142621</td>\n",
       "      <td>0.061656</td>\n",
       "      <td>0.106433</td>\n",
       "      <td>0.130219</td>\n",
       "      <td>0.138080</td>\n",
       "      <td>0.140287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.159845</td>\n",
       "      <td>0.116194</td>\n",
       "      <td>0.155559</td>\n",
       "      <td>0.056185</td>\n",
       "      <td>0.079961</td>\n",
       "      <td>0.153417</td>\n",
       "      <td>0.137072</td>\n",
       "      <td>0.141768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>0.134318</td>\n",
       "      <td>0.119378</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.192401</td>\n",
       "      <td>0.155788</td>\n",
       "      <td>0.136710</td>\n",
       "      <td>0.129789</td>\n",
       "      <td>0.129444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>0.138022</td>\n",
       "      <td>0.107769</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.194037</td>\n",
       "      <td>0.156855</td>\n",
       "      <td>0.138352</td>\n",
       "      <td>0.131794</td>\n",
       "      <td>0.131404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>0.124895</td>\n",
       "      <td>0.117877</td>\n",
       "      <td>0.101206</td>\n",
       "      <td>0.089787</td>\n",
       "      <td>0.145058</td>\n",
       "      <td>0.141055</td>\n",
       "      <td>0.139256</td>\n",
       "      <td>0.140867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>0.129043</td>\n",
       "      <td>0.104178</td>\n",
       "      <td>0.061273</td>\n",
       "      <td>0.143220</td>\n",
       "      <td>0.145145</td>\n",
       "      <td>0.138540</td>\n",
       "      <td>0.139150</td>\n",
       "      <td>0.139452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>0.137168</td>\n",
       "      <td>0.106892</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.195960</td>\n",
       "      <td>0.157828</td>\n",
       "      <td>0.137134</td>\n",
       "      <td>0.131877</td>\n",
       "      <td>0.131415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cat_0     cat_1     cat_2     cat_3     cat_4     cat_5     cat_6  \\\n",
       "0    0.139603  0.134039  0.146701  0.060896  0.104744  0.130741  0.141511   \n",
       "1    0.140012  0.160160  0.138237  0.139243  0.007545  0.120174  0.148434   \n",
       "2    0.164599  0.122769  0.157001  0.058148  0.060075  0.154086  0.138482   \n",
       "3    0.136849  0.143855  0.142621  0.061656  0.106433  0.130219  0.138080   \n",
       "4    0.159845  0.116194  0.155559  0.056185  0.079961  0.153417  0.137072   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "607  0.134318  0.119378  0.002171  0.192401  0.155788  0.136710  0.129789   \n",
       "608  0.138022  0.107769  0.001767  0.194037  0.156855  0.138352  0.131794   \n",
       "609  0.124895  0.117877  0.101206  0.089787  0.145058  0.141055  0.139256   \n",
       "610  0.129043  0.104178  0.061273  0.143220  0.145145  0.138540  0.139150   \n",
       "611  0.137168  0.106892  0.001726  0.195960  0.157828  0.137134  0.131877   \n",
       "\n",
       "        cat_7  \n",
       "0    0.141765  \n",
       "1    0.146194  \n",
       "2    0.144840  \n",
       "3    0.140287  \n",
       "4    0.141768  \n",
       "..        ...  \n",
       "607  0.129444  \n",
       "608  0.131404  \n",
       "609  0.140867  \n",
       "610  0.139452  \n",
       "611  0.131415  \n",
       "\n",
       "[612 rows x 8 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_adaptivetest_smote = np.array(ada_mod.predict_proba(x_test_norm))\n",
    "dataset_test_smote = pd.DataFrame(x_adaptivetest_smote, columns=x_norm.columns[:8])\n",
    "dataset_test_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_0</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.138771</td>\n",
       "      <td>0.126459</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.176214</td>\n",
       "      <td>0.161619</td>\n",
       "      <td>0.133874</td>\n",
       "      <td>0.130462</td>\n",
       "      <td>0.130477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.129433</td>\n",
       "      <td>0.102938</td>\n",
       "      <td>0.060967</td>\n",
       "      <td>0.143879</td>\n",
       "      <td>0.145385</td>\n",
       "      <td>0.138524</td>\n",
       "      <td>0.139486</td>\n",
       "      <td>0.139387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.138423</td>\n",
       "      <td>0.156035</td>\n",
       "      <td>0.133549</td>\n",
       "      <td>0.145243</td>\n",
       "      <td>0.007860</td>\n",
       "      <td>0.121588</td>\n",
       "      <td>0.150047</td>\n",
       "      <td>0.147256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.138147</td>\n",
       "      <td>0.123150</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>0.181482</td>\n",
       "      <td>0.160862</td>\n",
       "      <td>0.134835</td>\n",
       "      <td>0.129993</td>\n",
       "      <td>0.129454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.136943</td>\n",
       "      <td>0.119661</td>\n",
       "      <td>0.002047</td>\n",
       "      <td>0.185379</td>\n",
       "      <td>0.160252</td>\n",
       "      <td>0.135923</td>\n",
       "      <td>0.130340</td>\n",
       "      <td>0.129457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>0.148291</td>\n",
       "      <td>0.134897</td>\n",
       "      <td>0.103511</td>\n",
       "      <td>0.149639</td>\n",
       "      <td>0.016385</td>\n",
       "      <td>0.139262</td>\n",
       "      <td>0.155732</td>\n",
       "      <td>0.152283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3652</th>\n",
       "      <td>0.129250</td>\n",
       "      <td>0.124916</td>\n",
       "      <td>0.131617</td>\n",
       "      <td>0.072460</td>\n",
       "      <td>0.123213</td>\n",
       "      <td>0.137087</td>\n",
       "      <td>0.140686</td>\n",
       "      <td>0.140771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3653</th>\n",
       "      <td>0.130732</td>\n",
       "      <td>0.125521</td>\n",
       "      <td>0.132635</td>\n",
       "      <td>0.071352</td>\n",
       "      <td>0.121532</td>\n",
       "      <td>0.137455</td>\n",
       "      <td>0.140156</td>\n",
       "      <td>0.140617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3654</th>\n",
       "      <td>0.134996</td>\n",
       "      <td>0.133844</td>\n",
       "      <td>0.143395</td>\n",
       "      <td>0.063170</td>\n",
       "      <td>0.109264</td>\n",
       "      <td>0.132044</td>\n",
       "      <td>0.141808</td>\n",
       "      <td>0.141478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3655</th>\n",
       "      <td>0.129355</td>\n",
       "      <td>0.124482</td>\n",
       "      <td>0.131986</td>\n",
       "      <td>0.071941</td>\n",
       "      <td>0.122634</td>\n",
       "      <td>0.138603</td>\n",
       "      <td>0.140558</td>\n",
       "      <td>0.140442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3656 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         cat_0     cat_1     cat_2     cat_3     cat_4     cat_5     cat_6  \\\n",
       "0     0.138771  0.126459  0.002124  0.176214  0.161619  0.133874  0.130462   \n",
       "1     0.129433  0.102938  0.060967  0.143879  0.145385  0.138524  0.139486   \n",
       "2     0.138423  0.156035  0.133549  0.145243  0.007860  0.121588  0.150047   \n",
       "3     0.138147  0.123150  0.002076  0.181482  0.160862  0.134835  0.129993   \n",
       "4     0.136943  0.119661  0.002047  0.185379  0.160252  0.135923  0.130340   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3651  0.148291  0.134897  0.103511  0.149639  0.016385  0.139262  0.155732   \n",
       "3652  0.129250  0.124916  0.131617  0.072460  0.123213  0.137087  0.140686   \n",
       "3653  0.130732  0.125521  0.132635  0.071352  0.121532  0.137455  0.140156   \n",
       "3654  0.134996  0.133844  0.143395  0.063170  0.109264  0.132044  0.141808   \n",
       "3655  0.129355  0.124482  0.131986  0.071941  0.122634  0.138603  0.140558   \n",
       "\n",
       "         cat_7  \n",
       "0     0.130477  \n",
       "1     0.139387  \n",
       "2     0.147256  \n",
       "3     0.129454  \n",
       "4     0.129457  \n",
       "...        ...  \n",
       "3651  0.152283  \n",
       "3652  0.140771  \n",
       "3653  0.140617  \n",
       "3654  0.141478  \n",
       "3655  0.140442  \n",
       "\n",
       "[3656 rows x 8 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_adaptivetrain_smote = np.array(ada_mod.predict_proba(X_train_SMOTE))\n",
    "dataset_train_smote = pd.DataFrame(x_adaptivetrain_smote, columns=x_norm.columns[:8])\n",
    "dataset_train_smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianNB ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.19      0.28        84\n",
      "           1       0.14      0.44      0.21        41\n",
      "           2       0.19      0.70      0.29        46\n",
      "           3       0.12      0.57      0.20        40\n",
      "           4       0.25      0.13      0.17        68\n",
      "           5       0.08      0.01      0.02       117\n",
      "           6       0.40      0.02      0.03       115\n",
      "           7       0.20      0.06      0.09       101\n",
      "\n",
      "    accuracy                           0.17       612\n",
      "   macro avg       0.24      0.26      0.16       612\n",
      "weighted avg       0.25      0.17      0.13       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = GaussianNB()\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "y_pred = nbtrain.predict(x_test_norm)\n",
    "print(classification_report(y_test_norm, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.62      0.74        84\n",
      "           1       0.67      0.83      0.74        41\n",
      "           2       0.40      0.78      0.53        46\n",
      "           3       0.30      0.62      0.41        40\n",
      "           4       0.35      0.57      0.43        68\n",
      "           5       0.86      0.65      0.74       117\n",
      "           6       0.57      0.48      0.52       115\n",
      "           7       0.65      0.22      0.33       101\n",
      "\n",
      "    accuracy                           0.55       612\n",
      "   macro avg       0.59      0.60      0.55       612\n",
      "weighted avg       0.64      0.55      0.56       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# overfit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = svm.SVC()\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "y_pred = nbtrain.predict(x_test_norm)\n",
    "print(classification_report(y_test_norm, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.58      0.62        84\n",
      "           1       0.65      0.80      0.72        41\n",
      "           2       0.46      0.50      0.48        46\n",
      "           3       0.37      0.35      0.36        40\n",
      "           4       0.40      0.35      0.38        68\n",
      "           5       0.67      0.60      0.63       117\n",
      "           6       0.47      0.45      0.46       115\n",
      "           7       0.39      0.48      0.43       101\n",
      "\n",
      "    accuracy                           0.51       612\n",
      "   macro avg       0.51      0.51      0.51       612\n",
      "weighted avg       0.52      0.51      0.51       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# overfit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = DecisionTreeClassifier(random_state=0)\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "y_pred = nbtrain.predict(x_test_norm)\n",
    "print(classification_report(y_test_norm, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.73      0.75        84\n",
      "           1       0.70      0.85      0.77        41\n",
      "           2       0.56      0.83      0.67        46\n",
      "           3       0.29      0.62      0.40        40\n",
      "           4       0.40      0.57      0.47        68\n",
      "           5       0.85      0.65      0.74       117\n",
      "           6       0.65      0.54      0.59       115\n",
      "           7       0.54      0.26      0.35       101\n",
      "\n",
      "    accuracy                           0.59       612\n",
      "   macro avg       0.60      0.63      0.59       612\n",
      "weighted avg       0.63      0.59      0.59       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# overfit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Mengaktifkan/memanggil/membuat fungsi klasifikasi Naive Bayes\n",
    "modelnb = KNeighborsClassifier(n_neighbors=6)\n",
    "# Memasukkan data training pada fungsi klasifikasi Naive Bayes\n",
    "nbtrain = modelnb.fit(X_train_SMOTE, y_train_SMOTE)\n",
    "y_pred = nbtrain.predict(x_test_norm)\n",
    "print(classification_report(y_test_norm, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "147/147 [==============================] - 1s 2ms/step - loss: -0.1199 - accuracy: 0.1491\n",
      "Epoch 2/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -4.9108 - accuracy: 0.1250\n",
      "Epoch 3/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -9.2050 - accuracy: 0.1250\n",
      "Epoch 4/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -12.3805 - accuracy: 0.1250\n",
      "Epoch 5/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -15.2778 - accuracy: 0.1250\n",
      "Epoch 6/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -18.0560 - accuracy: 0.1250\n",
      "Epoch 7/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -20.7558 - accuracy: 0.1250\n",
      "Epoch 8/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -23.4377 - accuracy: 0.1313\n",
      "Epoch 9/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -26.3087 - accuracy: 0.1477\n",
      "Epoch 10/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -29.0122 - accuracy: 0.1518\n",
      "Epoch 11/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -31.6882 - accuracy: 0.1532\n",
      "Epoch 12/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -34.2927 - accuracy: 0.1499\n",
      "Epoch 13/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -36.9572 - accuracy: 0.1537\n",
      "Epoch 14/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -39.6110 - accuracy: 0.1570\n",
      "Epoch 15/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -42.2275 - accuracy: 0.1562\n",
      "Epoch 16/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -44.8861 - accuracy: 0.1603\n",
      "Epoch 17/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -47.4578 - accuracy: 0.1597\n",
      "Epoch 18/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -50.0543 - accuracy: 0.1567\n",
      "Epoch 19/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -52.6960 - accuracy: 0.1611\n",
      "Epoch 20/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -55.2838 - accuracy: 0.1606\n",
      "Epoch 21/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -57.8401 - accuracy: 0.1597\n",
      "Epoch 22/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -60.3632 - accuracy: 0.1575\n",
      "Epoch 23/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -63.0903 - accuracy: 0.1630\n",
      "Epoch 24/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -65.6681 - accuracy: 0.1622\n",
      "Epoch 25/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -68.3107 - accuracy: 0.1638\n",
      "Epoch 26/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -70.7987 - accuracy: 0.1627\n",
      "Epoch 27/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -73.4227 - accuracy: 0.1652\n",
      "Epoch 28/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -75.9362 - accuracy: 0.1630\n",
      "Epoch 29/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -78.6535 - accuracy: 0.1660\n",
      "Epoch 30/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -80.8483 - accuracy: 0.1540\n",
      "Epoch 31/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -83.5984 - accuracy: 0.1581\n",
      "Epoch 32/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -86.0496 - accuracy: 0.1575\n",
      "Epoch 33/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -88.7678 - accuracy: 0.1600\n",
      "Epoch 34/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -91.4636 - accuracy: 0.1633\n",
      "Epoch 35/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -93.9226 - accuracy: 0.1619\n",
      "Epoch 36/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -96.5328 - accuracy: 0.1611\n",
      "Epoch 37/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -98.8827 - accuracy: 0.1565\n",
      "Epoch 38/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -101.4850 - accuracy: 0.1567\n",
      "Epoch 39/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -104.0669 - accuracy: 0.1570\n",
      "Epoch 40/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -106.6835 - accuracy: 0.1584\n",
      "Epoch 41/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -109.2021 - accuracy: 0.1570\n",
      "Epoch 42/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -112.0315 - accuracy: 0.1636\n",
      "Epoch 43/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -114.5357 - accuracy: 0.1658\n",
      "Epoch 44/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -116.8937 - accuracy: 0.1603\n",
      "Epoch 45/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -119.5842 - accuracy: 0.1606\n",
      "Epoch 46/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -122.0002 - accuracy: 0.1562\n",
      "Epoch 47/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -124.6256 - accuracy: 0.1570\n",
      "Epoch 48/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -127.1895 - accuracy: 0.1578\n",
      "Epoch 49/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -129.8762 - accuracy: 0.1603\n",
      "Epoch 50/50\n",
      "147/147 [==============================] - 0s 2ms/step - loss: -132.6483 - accuracy: 0.1674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aab8dcdeb0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overfit\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(units=10,kernel_initializer='uniform' , activation='tanh' , input_dim=26))\n",
    "classifier.add(Dense(units=6,kernel_initializer='uniform' , activation='tanh'))\n",
    "classifier.add(Dense(units=1 , kernel_initializer='uniform' , activation='sigmoid'))\n",
    "classifier.compile(optimizer='adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "classifier.fit(X_train_SMOTE, y_train_SMOTE, batch_size = 25, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for finding best hyperparameters\n",
    "def FunctionFindBestParamsSmote(x_train, y_train):\n",
    "    \n",
    "    # Defining the list of hyper parameters to try\n",
    "    TrialNumber=0\n",
    "    batch_size_list=[5, 10, 20, 25]\n",
    "    epoch_list=[25, 50, 100]\n",
    "    \n",
    "    for batch_size_trial in batch_size_list:\n",
    "        for epochs_trial in epoch_list:\n",
    "            TrialNumber+=1\n",
    " \n",
    "            # Creating the classifier ANN model\n",
    "            classifier = Sequential()\n",
    "            classifier.add(Dense(units=10,kernel_initializer='uniform' , activation='tanh' , input_dim=26))\n",
    "            classifier.add(Dense(units=6,kernel_initializer='uniform' , activation='tanh'))\n",
    "            classifier.add(Dense(units=1 , kernel_initializer='uniform' , activation='sigmoid'))\n",
    "            classifier.compile(optimizer='adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "            survivalANN_Model=classifier.fit(x_train,y_train, batch_size=batch_size_trial , epochs=epochs_trial, verbose=0)\n",
    "            Accuracy = survivalANN_Model.history['accuracy'][-1]\n",
    "            \n",
    "            # printing the results of the current iteration\n",
    "            print(TrialNumber, 'Parameters:','batch_size:', batch_size_trial,'-', 'epochs:',epochs_trial, 'Accuracy:', Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the function\n",
    "# ResultsData = FunctionFindBestParamsSmote(X_train_SMOTE, y_train_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.32      0.49        84\n",
      "           1       0.07      1.00      0.13        41\n",
      "           2       0.00      0.00      0.00        46\n",
      "           3       0.00      0.00      0.00        40\n",
      "           4       0.00      0.00      0.00        68\n",
      "           5       0.00      0.00      0.00       117\n",
      "           6       0.00      0.00      0.00       115\n",
      "           7       0.00      0.00      0.00       101\n",
      "\n",
      "    accuracy                           0.11       612\n",
      "   macro avg       0.13      0.17      0.08       612\n",
      "weighted avg       0.14      0.11      0.08       612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = classifier.predict(x_test_norm)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "print(classification_report(y_test_norm, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | Without Adaboost | Adaboost | Adaboost + Normalisasi | Adaboost + Normalisasi + imbalanced |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| GaussianNB | 0.26 | 0.28 | 0.27 | 0.21 |\n",
    "| SVM | 0.23 | 0.36 | 0.34 | 0.55 |\n",
    "| Decision Tree | 0.51 | 0.42 | 0.39 | 0.56 |\n",
    "| KNN | 0.57 | 0.45076923076923076 | 0.4493464052287582 | 0.57 |\n",
    "| ANN | 0.08 | 0.08 | 0.07 | 0.11 |\n",
    "\n",
    "\n",
    "Berdasarkan hasil diatas, menunjukkan nilai akurasi yang dihasilkan oleh metode ensemble menggunakan adaptive boosting dan overfit pada imbalanced dataset menghasilkan nilai evaluasi yang lebih baik dibandingkan metode tanpa menggunakan overfit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7288e82646d3164eca24130947288f8779d11454649f2c02a5dfc42af7f324c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
